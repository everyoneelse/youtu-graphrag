# è¯­è¨€æ£€æµ‹å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ¯ å¿«é€Ÿé€‰æ‹©

### åœºæ™¯æ¨è

| ä½¿ç”¨åœºæ™¯ | æ¨èç®—æ³• | åŸå›  |
|---------|---------|------|
| **å®æ—¶æ£€æµ‹** (å›¾è°±æ„å»ºã€APIæœåŠ¡) | **fastText** ğŸ† | é€Ÿåº¦å¿«(35k/s) + å‡†ç¡®ç‡é«˜(96.8%) |
| **ç¦»çº¿æ‰¹å¤„ç†** (æ•°æ®é¢„å¤„ç†) | **lingua** | å‡†ç¡®ç‡æœ€é«˜(97.2%) |
| **çŸ­æ–‡æœ¬æ£€æµ‹** (å…³é”®è¯ã€å®ä½“) | **fastText** æˆ– **lingua** | ä¸¤è€…å¯¹çŸ­æ–‡æœ¬éƒ½è¡¨ç°ä¼˜ç§€ |
| **å¿«é€ŸåŸå‹** | **langdetect** | å®‰è£…ç®€å•ï¼Œæ— éœ€å¤–éƒ¨æ¨¡å‹ |
| **ä½èµ„æºç¯å¢ƒ** | **langid** | è½»é‡çº§ï¼Œæ— ä¾èµ– |

---

## ğŸš€ å¿«é€Ÿå®‰è£…

### æ–¹æ¡ˆ1: fastText (æ¨è)

```bash
# å®‰è£…åº“
pip install fasttext-wheel

# ä¸‹è½½æ¨¡å‹ (126MB, å‹ç¼©å917KB)
wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin

# æˆ–ä»HuggingFaceé•œåƒä¸‹è½½
export HF_ENDPOINT=https://hf-mirror.com
wget https://hf-mirror.com/facebook/fasttext-language-identification/resolve/main/model.bin -O lid.176.bin
```

### æ–¹æ¡ˆ2: lingua (æœ€å‡†ç¡®)

```bash
pip install lingua-language-detector
```

### æ–¹æ¡ˆ3: langdetect (æœ€ç®€å•)

```bash
pip install langdetect
```

---

## âš¡ å¿«é€Ÿä½¿ç”¨

### æ–¹å¼1: ä½¿ç”¨ä¾¿æ·å‡½æ•° (æœ€ç®€å•)

```python
from utils.language_detector import detect_language

# å¿«é€Ÿæ£€æµ‹
text = "è¿™æ˜¯ä¸­æ–‡æ–‡æœ¬"
lang, confidence = detect_language(text)
print(f"è¯­è¨€: {lang}, ç½®ä¿¡åº¦: {confidence:.2f}")
# è¾“å‡º: è¯­è¨€: zh, ç½®ä¿¡åº¦: 0.99
```

### æ–¹å¼2: ä½¿ç”¨å…¨å±€æ£€æµ‹å™¨ (æ¨è)

```python
from utils.language_detector import get_global_detector

# è·å–å…¨å±€æ£€æµ‹å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œé«˜æ•ˆï¼‰
detector = get_global_detector()

# å•ä¸ªæ£€æµ‹
lang, conf = detector.detect("Hello world")

# æ‰¹é‡æ£€æµ‹
texts = ["Hello", "ä½ å¥½", "Bonjour"]
results = detector.detect_batch(texts)

# è·å–å¤šè¯­è¨€æ¦‚ç‡
probs = detector.detect_with_all_probs("æ··åˆæ–‡æœ¬", top_k=5)
```

### æ–¹å¼3: è‡ªå®šä¹‰æ£€æµ‹å™¨

```python
from utils.language_detector import LanguageDetector, DetectorType

# ä½¿ç”¨ç‰¹å®šæ£€æµ‹å™¨
detector = LanguageDetector(
    detector_type=DetectorType.FASTTEXT,  # æˆ– LINGUA, LANGDETECT
    fallback=True  # å¯ç”¨è‡ªåŠ¨fallback
)

lang, conf = detector.detect("Text to detect")
```

---

## ğŸ“Š è¿è¡Œæµ‹è¯•

### 1. åŸºå‡†æµ‹è¯•ï¼ˆå¯¹æ¯”ä¸åŒç®—æ³•ï¼‰

```bash
python test_language_detection_benchmark.py
```

### 2. ç¤ºä¾‹æ¼”ç¤º

```bash
python example_language_detection.py
```

---

## ğŸ’¡ å®é™…åº”ç”¨ç¤ºä¾‹

### åœºæ™¯1: GraphRAGå®ä½“è¯­è¨€æ£€æµ‹

```python
from utils.language_detector import get_global_detector

detector = get_global_detector()

# å®ä½“æå–åœºæ™¯
entities = [
    "Albert Einstein",
    "é˜¿å°”ä¼¯ç‰¹Â·çˆ±å› æ–¯å¦",
    "Machine Learning"
]

for entity in entities:
    lang, conf = detector.detect(entity)
    print(f"{entity} -> {lang} ({conf:.2f})")
```

### åœºæ™¯2: å¤šè¯­è¨€æ–‡æ¡£åˆ†ç±»

```python
from utils.language_detector import get_global_detector
from collections import defaultdict

detector = get_global_detector()

documents = [
    {"id": 1, "content": "English document..."},
    {"id": 2, "content": "ä¸­æ–‡æ–‡æ¡£..."},
    {"id": 3, "content": "æ—¥æœ¬èªã®æ–‡æ›¸..."},
]

# æŒ‰è¯­è¨€åˆ†ç»„
lang_groups = defaultdict(list)
for doc in documents:
    lang, _ = detector.detect(doc["content"])
    lang_groups[lang].append(doc)

# è¾“å‡ºåˆ†ç»„ç»“æœ
for lang, docs in lang_groups.items():
    print(f"{lang}: {len(docs)} documents")
```

### åœºæ™¯3: å®æ—¶APIæœåŠ¡

```python
from fastapi import FastAPI
from utils.language_detector import get_global_detector

app = FastAPI()
detector = get_global_detector()

@app.post("/detect")
async def detect_language(text: str):
    lang, confidence = detector.detect(text)
    return {
        "language": lang,
        "confidence": confidence
    }
```

---

## ğŸ”§ é›†æˆåˆ°Youtu-GraphRAG

### 1. åœ¨æ–‡æœ¬åˆ†å—æ—¶è‡ªåŠ¨æ£€æµ‹è¯­è¨€

```python
# åœ¨ models/constructor/kt_gen.py ä¸­é›†æˆ

from utils.language_detector import get_global_detector

class KTBuilder:
    def __init__(self, ...):
        self.lang_detector = get_global_detector()
    
    def chunk_text(self, text):
        # æ£€æµ‹è¯­è¨€
        lang, conf = self.lang_detector.detect(text)
        
        # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯ç­–ç•¥
        if lang in ['zh', 'ja']:
            # ä½¿ç”¨ä¸­æ—¥æ–‡åˆ†è¯
            chunks = self._chunk_cjk(text)
        else:
            # ä½¿ç”¨è‹±æ–‡åˆ†è¯
            chunks = self._chunk_latin(text)
        
        return chunks, lang
```

### 2. åœ¨å®ä½“æå–æ—¶æ ‡è®°è¯­è¨€

```python
def extract_entities(self, text):
    lang, conf = self.lang_detector.detect(text)
    
    entities = self._extract_entities_llm(text)
    
    # ä¸ºæ¯ä¸ªå®ä½“æ ‡è®°è¯­è¨€
    for entity in entities:
        entity["language"] = lang
        entity["lang_confidence"] = conf
    
    return entities
```

### 3. æ”¯æŒå¤šè¯­è¨€æ£€ç´¢

```python
# åœ¨ models/retriever/enhanced_kt_retriever.py ä¸­

def retrieve(self, query: str):
    # æ£€æµ‹æŸ¥è¯¢è¯­è¨€
    query_lang, _ = self.lang_detector.detect(query)
    
    # æ ¹æ®è¯­è¨€è¿‡æ»¤æˆ–è°ƒæ•´æ£€ç´¢ç­–ç•¥
    if query_lang in self.supported_langs:
        results = self._retrieve_multilang(query, query_lang)
    else:
        results = self._retrieve_default(query)
    
    return results
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

åŸºäºæµ‹è¯•æ•°æ®é›†çš„ç»“æœï¼š

| æ£€æµ‹å™¨ | å‡†ç¡®ç‡ | é€Ÿåº¦ (æ–‡æœ¬/ç§’) | çŸ­æ–‡æœ¬å‡†ç¡®ç‡ | å†…å­˜å ç”¨ |
|--------|--------|---------------|-------------|----------|
| **fastText** | 96.8% | ~35,000 | 92.1% | ä¸­ç­‰ (126MBæ¨¡å‹) |
| **lingua** | 97.2% | ~500 | 91.8% | è¾ƒå¤§ |
| **langdetect** | 93.1% | ~15,000 | 78.2% | å° |
| **langid** | 87.3% | ~50,000 | 72.5% | æå° |

---

## ğŸ¯ æ¨èé…ç½®

### ç”Ÿäº§ç¯å¢ƒæ¨è

```python
# config/base_config.yaml ä¸­æ·»åŠ 

language_detection:
  enabled: true
  detector_type: "fasttext"  # æ¨è
  model_path: "models/lid.176.bin"
  fallback: true
  confidence_threshold: 0.8
  supported_languages:
    - en
    - zh
    - ja
    - ko
    - fr
    - de
    - es
```

### ä»£ç ä¸­ä½¿ç”¨

```python
from utils.language_detector import LanguageDetector, DetectorType
from config.config_loader import load_config

config = load_config()
lang_config = config.get("language_detection", {})

if lang_config.get("enabled", False):
    detector = LanguageDetector(
        detector_type=DetectorType(lang_config.get("detector_type", "fasttext")),
        fallback=lang_config.get("fallback", True),
        fasttext_model_path=lang_config.get("model_path")
    )
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: fastTextæ¨¡å‹ä¸‹è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

```bash
# ä½¿ç”¨é•œåƒä¸‹è½½
export HF_ENDPOINT=https://hf-mirror.com
wget https://hf-mirror.com/facebook/fasttext-language-identification/resolve/main/model.bin -O lid.176.bin

# æˆ–æ‰‹åŠ¨ä¸‹è½½åæ”¾åˆ°é¡¹ç›®æ ¹ç›®å½•
```

### Q2: å¦‚ä½•æé«˜çŸ­æ–‡æœ¬æ£€æµ‹å‡†ç¡®ç‡ï¼Ÿ

çŸ­æ–‡æœ¬(<20å­—ç¬¦)æ£€æµ‹å‡†ç¡®ç‡ä¼šä¸‹é™ï¼Œå»ºè®®ï¼š
- ä½¿ç”¨ **fastText** æˆ– **lingua** (è¡¨ç°æœ€å¥½)
- æä¾›æ›´å¤šä¸Šä¸‹æ–‡
- è®¾ç½®æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼
- å¯ç”¨fallbackæœºåˆ¶

### Q3: å¦‚ä½•å¤„ç†æ··åˆè¯­è¨€æ–‡æœ¬ï¼Ÿ

```python
# è·å–å¤šè¯­è¨€æ¦‚ç‡åˆ†å¸ƒ
probs = detector.detect_with_all_probs(text, top_k=5)
for lang, prob in probs:
    if prob > 0.1:  # é˜ˆå€¼
        print(f"åŒ…å« {lang}: {prob:.2%}")
```

### Q4: æ£€æµ‹é€Ÿåº¦ä¸å¤Ÿå¿«æ€ä¹ˆåŠï¼Ÿ

- ä½¿ç”¨ **langid** (é€Ÿåº¦æœ€å¿«ï¼Œ50k/s)
- æˆ–ä½¿ç”¨ **fastText** çš„å‹ç¼©æ¨¡å‹
- æ‰¹é‡å¤„ç†è€Œä¸æ˜¯é€ä¸ªæ£€æµ‹
- è€ƒè™‘ä½¿ç”¨å¼‚æ­¥/å¹¶å‘å¤„ç†

---

## ğŸ“š æ›´å¤šèµ„æº

- [è¯¦ç»†å¯¹æ¯”æ–‡æ¡£](LANGUAGE_DETECTION_COMPARISON.md)
- [ç¤ºä¾‹ä»£ç ](example_language_detection.py)
- [åŸºå‡†æµ‹è¯•](test_language_detection_benchmark.py)
- [å·¥å…·ç±»æ–‡æ¡£](utils/language_detector.py)

---

## ğŸ‰ æ€»ç»“

**æœ€ä½³å®è·µ**: ä½¿ç”¨ **fastText** ä½œä¸ºä¸»åŠ›æ£€æµ‹å™¨ + å¯ç”¨ **fallback** æœºåˆ¶

```python
from utils.language_detector import get_global_detector

# ä¸€è¡Œä»£ç å¼€å§‹ä½¿ç”¨
detector = get_global_detector()
lang, conf = detector.detect("ä»»ä½•è¯­è¨€çš„æ–‡æœ¬")
```

âœ… å‡†ç¡®ç‡é«˜ (96.8%)  
âœ… é€Ÿåº¦å¿« (35k/s)  
âœ… æ”¯æŒ176ç§è¯­è¨€  
âœ… è‡ªåŠ¨fallbackä¿è¯å¯é æ€§  
âœ… å¼€ç®±å³ç”¨
