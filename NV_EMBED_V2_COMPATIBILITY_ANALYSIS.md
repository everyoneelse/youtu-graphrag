# NV-Embed-v2 ä¸ sentence-transformers å…¼å®¹æ€§åˆ†æ

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**ç»“è®º**: âŒ **NV-Embed-v2 ä¸ç›´æ¥æ”¯æŒ sentence-transformers æ ‡å‡†æ¥å£**

## ğŸ” è¯¦ç»†åˆ†æ

### 1. NV-Embed-v2 æ¨¡å‹ç‰¹ç‚¹

- **æ¨¡å‹ID**: `nvidia/NV-Embed-v2`
- **å¼€å‘è€…**: NVIDIA
- **å‚æ•°è§„æ¨¡**: ~7B (7 billion parameters)
- **æœ€å¤§åºåˆ—é•¿åº¦**: 32,768 tokens
- **Embedding ç»´åº¦**: 4096
- **æ€§èƒ½**: åœ¨ MTEB æ’è¡Œæ¦œä¸Šè¡¨ç°ä¼˜å¼‚

### 2. æŠ€æœ¯æ¶æ„

NV-Embed-v2 åŸºäºä»¥ä¸‹ç‰¹æ®Šè®¾è®¡ï¼š

```python
# æ¨¡å‹æ¶æ„
- ä½¿ç”¨è‡ªå®šä¹‰çš„ transformer æ¶æ„
- éœ€è¦ trust_remote_code=True å‚æ•°
- ä½¿ç”¨ç‰¹æ®Šçš„ pooling ç­–ç•¥
- éœ€è¦æ·»åŠ ä»»åŠ¡å‰ç¼€ï¼ˆinstruction prefixï¼‰
```

### 3. ä¸ sentence-transformers çš„å…¼å®¹æ€§

#### âŒ **ä¸å…¼å®¹çš„åŸå› **

1. **è‡ªå®šä¹‰æ¨¡å‹æ¶æ„**: NV-Embed-v2 ä½¿ç”¨äº†è‡ªå®šä¹‰çš„æ¨¡å‹ç±»ï¼Œä¸æ˜¯æ ‡å‡†çš„ BERT/RoBERTa æ¶æ„
2. **ç¼ºå°‘é…ç½®æ–‡ä»¶**: æ²¡æœ‰ `sentence_bert_config.json`ï¼Œè¿™æ˜¯ sentence-transformers æ‰€éœ€çš„
3. **ç‰¹æ®ŠåŠ è½½æ–¹å¼**: éœ€è¦ `trust_remote_code=True` å’Œè‡ªå®šä¹‰çš„åˆå§‹åŒ–è¿‡ç¨‹
4. **ä»»åŠ¡å‰ç¼€è¦æ±‚**: éœ€è¦ä¸ºä¸åŒä»»åŠ¡æ·»åŠ ç‰¹å®šå‰ç¼€ï¼Œä¸ç¬¦åˆ sentence-transformers çš„æ ‡å‡†æ¥å£

#### æ­£ç¡®çš„åŠ è½½æ–¹å¼

```python
# âŒ é”™è¯¯ - ä¸èƒ½è¿™æ ·ä½¿ç”¨
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('nvidia/NV-Embed-v2')  # ä¼šå¤±è´¥ï¼

# âœ“ æ­£ç¡® - éœ€è¦ä½¿ç”¨ transformers
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('nvidia/NV-Embed-v2', trust_remote_code=True)
model = AutoModel.from_pretrained('nvidia/NV-Embed-v2', trust_remote_code=True)

# ç¼–ç æ—¶éœ€è¦æ·»åŠ ä»»åŠ¡å‰ç¼€
def encode_text(text, task_prefix="Instruct: Retrieve semantically similar text.\nQuery: "):
    full_text = task_prefix + text
    inputs = tokenizer(full_text, return_tensors='pt', padding=True, truncation=True, max_length=512)
    
    with torch.no_grad():
        outputs = model(**inputs)
        # ä½¿ç”¨ last token çš„ embeddingï¼ˆNV-Embed-v2 çš„ç‰¹æ®Šæ–¹å¼ï¼‰
        embeddings = outputs.last_hidden_state[:, -1, :]
    
    return embeddings
```

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ä½¿ç”¨å…¼å®¹çš„æ›¿ä»£æ¨¡å‹ï¼ˆæ¨èï¼‰

å¦‚æœéœ€è¦ä¿æŒ sentence-transformers æ¥å£çš„ç®€æ´æ€§ï¼Œå»ºè®®ä½¿ç”¨ä»¥ä¸‹é«˜æ€§èƒ½æ›¿ä»£æ¨¡å‹ï¼š

#### è‹±æ–‡æ¨¡å‹
```yaml
# æ¨èæ¨¡å‹ï¼ˆæŒ‰æ€§èƒ½æ’åºï¼‰
1. BAAI/bge-large-en-v1.5  
   - ç»´åº¦: 1024
   - æ€§èƒ½: åœ¨ MTEB ä¸Šè¡¨ç°ä¼˜å¼‚
   - ç›´æ¥æ”¯æŒ sentence-transformers

2. intfloat/e5-large-v2
   - ç»´åº¦: 1024
   - æ€§èƒ½: å¼ºå¤§çš„æ£€ç´¢èƒ½åŠ›
   - ç›´æ¥æ”¯æŒ sentence-transformers

3. sentence-transformers/all-mpnet-base-v2
   - ç»´åº¦: 768
   - æ€§èƒ½: å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦
   - å®˜æ–¹ç»´æŠ¤
```

#### ä¸­æ–‡æ¨¡å‹
```yaml
1. BAAI/bge-large-zh-v1.5
   - ç»´åº¦: 1024
   - ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–
   
2. moka-ai/m3e-large
   - ç»´åº¦: 1024
   - ä¸­æ–‡æ€§èƒ½ä¼˜å¼‚
```

**é…ç½®ä¿®æ”¹**:
```yaml
# config/base_config.yaml
tree_comm:
  embedding_model: BAAI/bge-large-en-v1.5  # æ›¿æ¢è¿™é‡Œ
  
embeddings:
  model_name: BAAI/bge-large-en-v1.5  # æ›¿æ¢è¿™é‡Œ
```

### æ–¹æ¡ˆ 2: è‡ªå®šä¹‰ Embedding é€‚é…å™¨ï¼ˆé€‚ç”¨äºå¿…é¡»ä½¿ç”¨ NV-Embed-v2ï¼‰

å¦‚æœå¿…é¡»ä½¿ç”¨ NV-Embed-v2ï¼Œéœ€è¦ä¿®æ”¹ä»£ç ä»¥æ”¯æŒè‡ªå®šä¹‰åŠ è½½ï¼š

#### æ­¥éª¤ 1: åˆ›å»º NV-Embed-v2 é€‚é…å™¨

```python
# utils/nv_embed_adapter.py
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np

class NVEmbedV2Adapter:
    """NV-Embed-v2 é€‚é…å™¨ï¼Œæä¾›ç±»ä¼¼ SentenceTransformer çš„æ¥å£"""
    
    def __init__(self, model_name='nvidia/NV-Embed-v2', device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
        self.model.to(device)
        self.model.eval()
        self.device = device
        self.task_prefix = "Instruct: Retrieve semantically similar text.\nQuery: "
    
    def encode(self, sentences, batch_size=32, show_progress_bar=False, 
               convert_to_numpy=True, normalize_embeddings=False, **kwargs):
        """
        ç¼–ç æ–‡æœ¬ï¼Œæ¥å£ä¸ SentenceTransformer å…¼å®¹
        """
        if isinstance(sentences, str):
            sentences = [sentences]
        
        all_embeddings = []
        
        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i + batch_size]
            # æ·»åŠ ä»»åŠ¡å‰ç¼€
            batch_with_prefix = [self.task_prefix + text for text in batch]
            
            # Tokenize
            inputs = self.tokenizer(
                batch_with_prefix,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)
            
            # Encode
            with torch.no_grad():
                outputs = self.model(**inputs)
                # NV-Embed-v2 ä½¿ç”¨ last token embedding
                embeddings = outputs.last_hidden_state[:, -1, :]
            
            if normalize_embeddings:
                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
            
            all_embeddings.append(embeddings.cpu())
        
        all_embeddings = torch.cat(all_embeddings, dim=0)
        
        if convert_to_numpy:
            all_embeddings = all_embeddings.numpy()
        
        return all_embeddings
```

#### æ­¥éª¤ 2: ä¿®æ”¹ kt_gen.py

```python
# åœ¨ models/constructor/kt_gen.py çš„ _get_semantic_dedup_embedder æ–¹æ³•ä¸­

def _get_semantic_dedup_embedder(self):
    config = self._get_semantic_dedup_config()
    if not config or not config.use_embeddings:
        return None

    if self._semantic_dedup_embedder is not None:
        return self._semantic_dedup_embedder

    model_name = getattr(config, "embedding_model", "") or getattr(self.config.embeddings, "model_name", "all-MiniLM-L6-v2")
    
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ NV-Embed-v2
        if 'NV-Embed' in model_name or 'nv-embed' in model_name.lower():
            from utils.nv_embed_adapter import NVEmbedV2Adapter
            logger.info(f"Using NV-Embed-v2 adapter for model: {model_name}")
            self._semantic_dedup_embedder = NVEmbedV2Adapter(model_name)
        else:
            # ä½¿ç”¨æ ‡å‡† sentence-transformers
            from sentence_transformers import SentenceTransformer
            self._semantic_dedup_embedder = SentenceTransformer(model_name)
            
    except Exception as e:
        logger.warning(
            "Failed to initialize semantic dedup embedder with model '%s': %s: %s",
            model_name,
            type(e).__name__,
            e,
        )
        self._semantic_dedup_embedder = None

    return self._semantic_dedup_embedder
```

#### æ­¥éª¤ 3: ä¿®æ”¹ tree_comm.py

ç±»ä¼¼åœ°ä¿®æ”¹ `utils/tree_comm.py` çš„åˆå§‹åŒ–éƒ¨åˆ†ã€‚

### æ–¹æ¡ˆ 3: ä½¿ç”¨ sentence-transformers 2.3+ çš„è‡ªå®šä¹‰æ¨¡å‹åŠŸèƒ½

ä» sentence-transformers 2.3 å¼€å§‹ï¼Œå¯ä»¥è‡ªå®šä¹‰æ¨¡å‹ï¼š

```python
from sentence_transformers import SentenceTransformer, models

# åˆ›å»ºè‡ªå®šä¹‰ transformer
word_embedding_model = models.Transformer('nvidia/NV-Embed-v2', trust_remote_code=True)

# æ·»åŠ  pooling å±‚
pooling_model = models.Pooling(
    word_embedding_model.get_word_embedding_dimension(),
    pooling_mode='last_token'  # NV-Embed-v2 ä½¿ç”¨ last token
)

# ç»„åˆæˆ SentenceTransformer
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])
```

ä½†è¿™ç§æ–¹å¼å¯èƒ½ä»éœ€è¦å¤„ç†ä»»åŠ¡å‰ç¼€çš„é—®é¢˜ã€‚

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### NV-Embed-v2 vs æ›¿ä»£æ¨¡å‹

| æ¨¡å‹ | ç»´åº¦ | å‚æ•°é‡ | MTEB åˆ†æ•° | GPU å†…å­˜ | æ¨ç†é€Ÿåº¦ | sentence-transformers |
|------|------|--------|-----------|---------|---------|----------------------|
| NV-Embed-v2 | 4096 | ~7B | **69.3** | 16GB+ | æ…¢ | âŒ ä¸æ”¯æŒ |
| bge-large-en-v1.5 | 1024 | 335M | 64.2 | 2GB | å¿« | âœ… æ”¯æŒ |
| e5-large-v2 | 1024 | 335M | 62.3 | 2GB | å¿« | âœ… æ”¯æŒ |
| all-mpnet-base-v2 | 768 | 110M | 57.8 | 1GB | å¾ˆå¿« | âœ… æ”¯æŒ |

### ç»¼åˆè€ƒè™‘

- **å¦‚æœè¿½æ±‚æè‡´æ€§èƒ½** ä¸”æœ‰å……è¶³çš„ GPU èµ„æº: ä½¿ç”¨æ–¹æ¡ˆ 2ï¼ˆè‡ªå®šä¹‰é€‚é…å™¨ï¼‰
- **å¦‚æœè¿½æ±‚å¹³è¡¡** (æ€§èƒ½ã€æ˜“ç”¨æ€§ã€èµ„æº): ä½¿ç”¨æ–¹æ¡ˆ 1ï¼ˆbge-large-en-v1.5ï¼‰
- **å¦‚æœèµ„æºå—é™**: ä½¿ç”¨ all-mpnet-base-v2

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### å¯¹äº kt_gen.py

**æ¨èä½¿ç”¨æ–¹æ¡ˆ 1**ï¼šé‡‡ç”¨ `BAAI/bge-large-en-v1.5` æˆ– `BAAI/bge-large-zh-v1.5`ï¼ˆä¸­æ–‡ï¼‰

**åŸå› **:
1. âœ… å¼€ç®±å³ç”¨ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
2. âœ… æ€§èƒ½ä¼˜å¼‚ï¼ˆMTEB: 64.2ï¼Œä»…æ¯” NV-Embed-v2 ä½ 5 åˆ†ï¼‰
3. âœ… èµ„æºéœ€æ±‚åˆç†ï¼ˆ2GB GPU å†…å­˜ï¼‰
4. âœ… æ¨ç†é€Ÿåº¦å¿«ï¼ˆæ¯” NV-Embed-v2 å¿« 10x+ï¼‰
5. âœ… ç¤¾åŒºå¹¿æ³›ä½¿ç”¨ï¼Œç¨³å®šå¯é 

**é…ç½®ä¿®æ”¹**:

```yaml
# config/base_config.yaml
construction:
  tree_comm:
    embedding_model: BAAI/bge-large-en-v1.5  # æˆ– BAAI/bge-large-zh-v1.5
    
embeddings:
  model_name: BAAI/bge-large-en-v1.5  # æˆ– BAAI/bge-large-zh-v1.5
```

## ğŸ“ æ€»ç»“

1. **NV-Embed-v2 ä¸èƒ½ç›´æ¥ç”¨äº kt_gen.py**ï¼Œå› ä¸ºä¸å…¼å®¹ sentence-transformers
2. **æ¨èä½¿ç”¨ BAAI/bge-large ç³»åˆ—**ï¼Œæ€§èƒ½å¼ºå¤§ä¸”æ˜“äºä½¿ç”¨
3. å¦‚æœå¿…é¡»ä½¿ç”¨ NV-Embed-v2ï¼Œéœ€è¦å®ç°è‡ªå®šä¹‰é€‚é…å™¨ï¼ˆå·¥ä½œé‡è¾ƒå¤§ï¼‰
4. å¯¹äºå¤§å¤šæ•°åº”ç”¨åœºæ™¯ï¼Œbge-large çš„æ€§èƒ½å·²ç»è¶³å¤Ÿä¼˜å¼‚

## ğŸ”— å‚è€ƒèµ„æº

- [NV-Embed-v2 on HuggingFace](https://huggingface.co/nvidia/NV-Embed-v2)
- [BAAI/bge Models](https://huggingface.co/BAAI)
- [Sentence-Transformers Documentation](https://www.sbert.net/)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
