"""
DSPy Semantic Dedup æ¼”ç¤ºè„šæœ¬

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨DSPyä¼˜åŒ–semantic deduplicationã€‚

è¿è¡Œå‰æï¼š
1. pip install dspy-ai
2. export OPENAI_API_KEY=your_key

è¿è¡Œæ–¹æ³•ï¼š
    python example_dspy_demo.py
"""

import os
import sys
from pathlib import Path

# Add current directory to path
sys.path.insert(0, str(Path(__file__).parent))

try:
    import dspy
    from models.constructor.dspy_semantic_dedup import (
        SemanticClusteringModule,
        SemanticDedupModule,
        DSPySemanticDedupOptimizer,
        clustering_metric,
        dedup_metric
    )
    from scripts.prepare_dspy_training_data import create_synthetic_training_examples
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("\nè¯·ç¡®ä¿:")
    print("1. å·²å®‰è£… dspy-ai: pip install dspy-ai")
    print("2. åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


def setup():
    """è®¾ç½®DSPyç¯å¢ƒ"""
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("\nè¯·è®¾ç½® API key:")
        print("  export OPENAI_API_KEY=your_key")
        sys.exit(1)
    
    print("ğŸ”§ é…ç½® DSPy...")
    lm = dspy.OpenAI(model="gpt-3.5-turbo", api_key=api_key, max_tokens=2000)
    dspy.settings.configure(lm=lm)
    print("âœ“ DSPy é…ç½®å®Œæˆ\n")


def demo_clustering():
    """æ¼”ç¤ºèšç±»åŠŸèƒ½"""
    print("="*80)
    print("ç¤ºä¾‹ 1: LLM-based Clustering")
    print("="*80)
    
    # åˆ›å»ºèšç±»æ¨¡å—
    clustering = SemanticClusteringModule(use_cot=True)
    
    # æµ‹è¯•æ•°æ®
    head_entity = "United States"
    relation = "president"
    tails = [
        "Barack Obama",
        "Obama",
        "Barack H. Obama",
        "Donald Trump",
        "Trump",
        "Donald J. Trump"
    ]
    
    print(f"\nHead Entity: {head_entity}")
    print(f"Relation: {relation}")
    print(f"\nTail Candidates:")
    for i, tail in enumerate(tails, 1):
        print(f"  [{i}] {tail}")
    
    print("\nğŸ¤– è°ƒç”¨ LLM è¿›è¡Œèšç±»...")
    
    try:
        clusters = clustering(
            head_entity=head_entity,
            relation=relation,
            tail_descriptions=tails
        )
        
        print(f"\nâœ“ èšç±»å®Œæˆï¼å…± {len(clusters)} ä¸ªcluster:")
        for i, cluster in enumerate(clusters, 1):
            members = cluster.get('members', [])
            description = cluster.get('description', 'No description')
            
            print(f"\n  Cluster {i}:")
            print(f"    Members: {[tails[m-1] for m in members]}")
            print(f"    Rationale: {description}")
    
    except Exception as e:
        print(f"\nâŒ èšç±»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def demo_dedup():
    """æ¼”ç¤ºå»é‡åŠŸèƒ½"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 2: Semantic Deduplication (Coreference Resolution)")
    print("="*80)
    
    # åˆ›å»ºå»é‡æ¨¡å—
    dedup = SemanticDedupModule(prompt_type="general", use_cot=True)
    
    # æµ‹è¯•æ•°æ®
    head_entity = "Star Wars film series"
    relation = "director"
    tails = [
        "George Lucas",
        "G. Lucas",
        "George Walton Lucas Jr.",
        "J.J. Abrams",
        "Jeffrey Jacob Abrams"
    ]
    
    print(f"\nHead Entity: {head_entity}")
    print(f"Relation: {relation}")
    print(f"\nTail Candidates:")
    for i, tail in enumerate(tails, 1):
        print(f"  [{i}] {tail}")
    
    # å‡†å¤‡batch entries
    batch_entries = [
        {
            "description": tail,
            "context_summaries": [
                f"- {tail} is mentioned as a director of Star Wars films"
            ]
        }
        for tail in tails
    ]
    
    head_contexts = [
        "- Star Wars is a film series with multiple directors",
        "- Different directors worked on different films in the series"
    ]
    
    print("\nğŸ¤– è°ƒç”¨ LLM è¿›è¡Œå»é‡...")
    
    try:
        groups, reasoning = dedup(
            head_entity=head_entity,
            relation=relation,
            head_contexts=head_contexts,
            batch_entries=batch_entries
        )
        
        print(f"\nâœ“ å»é‡å®Œæˆï¼å…± {len(groups)} ä¸ªgroup:")
        
        # æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
        if reasoning:
            print(f"\nğŸ’­ LLM Reasoning:")
            print(f"  {reasoning[:300]}..." if len(reasoning) > 300 else f"  {reasoning}")
        
        # æ˜¾ç¤ºåˆ†ç»„ç»“æœ
        for i, group in enumerate(groups, 1):
            members = group.get('members', [])
            representative = group.get('representative', members[0] if members else 1)
            rationale = group.get('rationale', 'No rationale')
            
            print(f"\n  Group {i}:")
            print(f"    Members: {[tails[m-1] for m in members]}")
            print(f"    Representative: {tails[representative-1]}")
            print(f"    Rationale: {rationale}")
    
    except Exception as e:
        print(f"\nâŒ å»é‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def demo_optimization():
    """æ¼”ç¤ºä¼˜åŒ–è¿‡ç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 3: DSPy Optimization (ä½¿ç”¨åˆæˆæ•°æ®)")
    print("="*80)
    
    print("\nğŸ“Š åˆ›å»ºè®­ç»ƒæ ·æœ¬...")
    examples = create_synthetic_training_examples()
    print(f"âœ“ åˆ›å»ºäº† {len(examples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    split_idx = int(len(examples) * 0.7)
    train_examples = examples[:split_idx]
    val_examples = examples[split_idx:]
    
    print(f"  è®­ç»ƒé›†: {len(train_examples)} ä¸ªæ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_examples)} ä¸ªæ ·æœ¬")
    
    # è¯„ä¼°baseline
    print("\nğŸ“ˆ è¯„ä¼° Baseline æ€§èƒ½...")
    baseline_clustering = SemanticClusteringModule(use_cot=True)
    
    baseline_scores = []
    for ex in val_examples[:2]:  # åªè¯„ä¼°å‰2ä¸ªæ ·æœ¬ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        try:
            pred = baseline_clustering(
                head_entity=ex.head_entity,
                relation=ex.relation,
                tail_descriptions=ex.tail_descriptions
            )
            score = clustering_metric(ex, pred)
            baseline_scores.append(score)
            print(f"  Example: {ex.head_entity} - {ex.relation}")
            print(f"    Score: {score:.2f}")
        except Exception as e:
            print(f"  âŒ è¯„ä¼°å¤±è´¥: {e}")
    
    avg_baseline = sum(baseline_scores) / len(baseline_scores) if baseline_scores else 0
    print(f"\n  Baseline å¹³å‡åˆ†æ•°: {avg_baseline:.2f}")
    
    print("\nğŸ’¡ æç¤º:")
    print("  è¦è¿›è¡Œå®Œæ•´çš„ä¼˜åŒ–è®­ç»ƒï¼Œè¿è¡Œ:")
    print("  python scripts/train_dspy_modules.py --train-all --use-synthetic")
    print("\n  ä¼˜åŒ–è¿‡ç¨‹ä¼š:")
    print("  1. ä½¿ç”¨ GPT-4 ä½œä¸º teacher ç”Ÿæˆé«˜è´¨é‡ç¤ºä¾‹")
    print("  2. è®­ç»ƒ GPT-3.5-turbo å­¦ä¹ è¿™äº›ç¤ºä¾‹")
    print("  3. åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æå‡æ•ˆæœ")
    print("  4. ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å—ä¾›ç”Ÿäº§ä½¿ç”¨")


def demo_cost_comparison():
    """æ¼”ç¤ºæˆæœ¬å¯¹æ¯”"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 4: æˆæœ¬å¯¹æ¯”åˆ†æ")
    print("="*80)
    
    # å‡è®¾çš„æˆæœ¬æ•°æ®ï¼ˆç¾å…ƒ/1k tokensï¼‰
    costs = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002}
    }
    
    # ä¼°ç®—ä¸€æ¬¡å»é‡ä»»åŠ¡çš„tokenä½¿ç”¨
    num_tails = 100
    clustering_calls = num_tails // 30  # æ¯30ä¸ªtailsä¸€æ¬¡clustering call
    dedup_calls = num_tails // 8  # æ¯8ä¸ªtailsä¸€æ¬¡dedup call
    
    avg_tokens_per_call = 1500  # å‡è®¾æ¯æ¬¡è°ƒç”¨å¹³å‡1500 tokens
    
    print(f"\nå‡è®¾åœºæ™¯: å»é‡ {num_tails} ä¸ª tail entities")
    print(f"  Clustering è°ƒç”¨æ¬¡æ•°: ~{clustering_calls}")
    print(f"  Deduplication è°ƒç”¨æ¬¡æ•°: ~{dedup_calls}")
    print(f"  æ¯æ¬¡è°ƒç”¨å¹³å‡ tokens: ~{avg_tokens_per_call}")
    
    total_calls = clustering_calls + dedup_calls
    total_tokens = total_calls * avg_tokens_per_call / 1000  # Convert to k tokens
    
    # è®¡ç®—ä¸åŒæ–¹æ¡ˆçš„æˆæœ¬
    scenarios = {
        "GPT-4 åŸå§‹prompt": {
            "model": "gpt-4",
            "description": "ä½¿ç”¨GPT-4å’Œæ‰‹å·¥ä¼˜åŒ–çš„prompt"
        },
        "GPT-4 DSPyä¼˜åŒ–": {
            "model": "gpt-4", 
            "description": "ä½¿ç”¨GPT-4å’ŒDSPyä¼˜åŒ–çš„promptï¼ˆæ›´é«˜æ•ˆï¼‰",
            "efficiency": 0.9  # DSPyä¼˜åŒ–åtokenså‡å°‘10%
        },
        "GPT-3.5 DSPyä¼˜åŒ–": {
            "model": "gpt-3.5-turbo",
            "description": "ä½¿ç”¨GPT-3.5-turboå’ŒDSPyä¼˜åŒ–ï¼ˆæ¨èï¼‰",
            "efficiency": 1.0
        }
    }
    
    print("\n" + "-"*80)
    print("æˆæœ¬å¯¹æ¯”:")
    print("-"*80)
    
    for scenario_name, config in scenarios.items():
        model = config["model"]
        efficiency = config.get("efficiency", 1.0)
        adjusted_tokens = total_tokens * efficiency
        
        # ç®€åŒ–è®¡ç®—ï¼šå‡è®¾inputå’Œoutputå„å 50%
        cost = (adjusted_tokens * costs[model]["input"] * 0.5 + 
                adjusted_tokens * costs[model]["output"] * 0.5)
        
        print(f"\n{scenario_name}:")
        print(f"  æ¨¡å‹: {model}")
        print(f"  æè¿°: {config['description']}")
        print(f"  Tokenä½¿ç”¨: {adjusted_tokens:.1f}k tokens")
        print(f"  ä¼°ç®—æˆæœ¬: ${cost:.2f}")
    
    # è®¡ç®—èŠ‚çœ
    baseline_cost = (total_tokens * costs["gpt-4"]["input"] * 0.5 + 
                    total_tokens * costs["gpt-4"]["output"] * 0.5)
    optimized_cost = (total_tokens * costs["gpt-3.5-turbo"]["input"] * 0.5 + 
                     total_tokens * costs["gpt-3.5-turbo"]["output"] * 0.5)
    savings = baseline_cost - optimized_cost
    savings_pct = (savings / baseline_cost) * 100
    
    print("\n" + "-"*80)
    print(f"ğŸ’° ä½¿ç”¨ DSPy + GPT-3.5-turbo å¯èŠ‚çœ:")
    print(f"  ç»å¯¹èŠ‚çœ: ${savings:.2f}")
    print(f"  ç›¸å¯¹èŠ‚çœ: {savings_pct:.1f}%")
    print("-"*80)
    
    print("\nğŸ“ æ³¨æ„:")
    print("  - ä»¥ä¸Šæ˜¯ä¼°ç®—å€¼ï¼Œå®é™…æˆæœ¬å–å†³äºå…·ä½“ä½¿ç”¨æƒ…å†µ")
    print("  - DSPyä¼˜åŒ–åï¼ŒGPT-3.5-turbo å¯ä»¥è¾¾åˆ°æ¥è¿‘GPT-4çš„è´¨é‡")
    print("  - é•¿æœŸæ¥çœ‹ï¼Œæˆæœ¬èŠ‚çœéå¸¸å¯è§‚")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("DSPy Semantic Dedup æ¼”ç¤º")
    print("="*80)
    print("\nè¿™ä¸ªè„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨DSPyä¼˜åŒ–semantic deduplication")
    print("åŒ…æ‹¬: clustering, deduplication, optimization, cost analysis\n")
    
    # è®¾ç½®ç¯å¢ƒ
    setup()
    
    # è¿è¡Œæ¼”ç¤º
    try:
        demo_clustering()
        demo_dedup()
        demo_optimization()
        demo_cost_comparison()
        
        print("\n" + "="*80)
        print("âœ… æ¼”ç¤ºå®Œæˆï¼")
        print("="*80)
        
        print("\nä¸‹ä¸€æ­¥:")
        print("1. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: DSPY_QUICKSTART.md")
        print("2. å‡†å¤‡è®­ç»ƒæ•°æ®: python scripts/prepare_dspy_training_data.py")
        print("3. è®­ç»ƒä¼˜åŒ–æ¨¡å—: python scripts/train_dspy_modules.py --train-all --use-synthetic")
        print("4. é›†æˆåˆ°ç”Ÿäº§ç¯å¢ƒ: æ›´æ–° config/base_config.yaml è®¾ç½® use_dspy: true")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
