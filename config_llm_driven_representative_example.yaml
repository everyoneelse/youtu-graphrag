# Configuration Example: LLM-Driven Representative Selection
# Updated version that asks LLM to choose the representative

construction:
  semantic_dedup:
    head_dedup:
      enabled: true
      merge_strategy: "alias"  # Use alias method
      
      # NEW: Use LLM to select representative
      use_llm_for_representative_selection: true  # Let LLM decide, not code heuristics
      
      # Alias configuration
      alias_relation_name: "alias_of"
      
      # Semantic deduplication
      enable_semantic: true
      similarity_threshold: 0.85  # For post-filtering (not used in representative selection)
      use_llm_validation: true     # Must be true for LLM-driven representative selection
      max_candidates: 1000
      candidate_similarity_threshold: 0.75

prompts:
  head_dedup:
    # Original prompt (still available for backward compatibility)
    general: |-
      You are an expert in knowledge graph entity resolution.
      TASK: Determine if the following two entities refer to the SAME real-world object.
      ...
      OUTPUT FORMAT (strict JSON):
      {
        "is_coreferent": true/false,
        "rationale": "..."
      }
    
    # NEW: Improved prompt with representative selection
    with_representative_selection: |-
      You are an expert in knowledge graph entity resolution.

      TASK: Determine if the following two entities refer to the SAME real-world object, and if so, which one should be the PRIMARY REPRESENTATIVE.

      Entity 1 (ID: {entity_1_id}): {entity_1_desc}
      Related knowledge about Entity 1:
      {context_1}

      Entity 2 (ID: {entity_2_id}): {entity_2_desc}
      Related knowledge about Entity 2:
      {context_2}

      CRITICAL RULES:

      1. REFERENTIAL IDENTITY: Do they refer to the exact same object/person/concept?
         - Same entity with different names → YES (e.g., "NYC" = "New York City")
         - Different but related entities → NO (e.g., "Apple Inc." ≠ "Apple Store")

      2. SUBSTITUTION TEST: Can you replace one with the other in all contexts?

      3. CONSERVATIVE PRINCIPLE:
         - When uncertain about coreference → answer NO
         - When uncertain about representative → choose the one with MORE relationships
         - False merge is worse than false split

      CONTEXT VERIFICATION (MANDATORY - Must execute for every comparison):
      
      Use the provided context (relationships and source text) to make informed coreference decisions:
      
      Step A: **Context Consistency Check** (CRITICAL for avoiding false merges)
         ✓ Do the two entities' contexts contain contradictory information?
         ✓ Check for conflicts in:
            - Temporal attributes (founding dates, time periods, ages)
            - Spatial attributes (locations, geographic information)
            - Type/category information (person vs organization, company vs product)
            - Functional roles (founder vs employee, parent vs subsidiary)
            - Quantitative attributes (different sizes, populations, counts)
         ✗ If ANY key contradiction exists → they are DIFFERENT entities, answer NO
         ✓ If contexts are consistent or complementary → proceed to next step
         
      Step B: **Context Completeness Assessment**
         ✓ Is the provided context sufficient to make a reliable coreference decision?
         ✓ Assess information quality:
            - High confidence: Multiple consistent relationships, clear context
            - Medium confidence: Some relationships, partial context
            - Low confidence: Minimal information, unclear context
         ✗ If information is insufficient and uncertainty is high → apply CONSERVATIVE PRINCIPLE (answer NO)
         
      Step C: **Relationship Pattern Analysis**
         ✓ Compare the relationship patterns of both entities:
            - Do they share common relationship types?
            - Are the shared relationships consistent with being the same entity?
            - Do relationship targets align or conflict?
         ✓ Strong evidence for coreference:
            - Multiple overlapping relationships with same or equivalent targets
            - Relationships form consistent and coherent pattern
            - No conflicting relationship information
         ✗ Evidence against coreference:
            - Relationships suggest hierarchical structure (owner-owned, parent-child)
            - Relationships describe different scopes or domains
            - Relationship targets are contradictory
      
      Step D: **Source Text Validation** (when hybrid_context is enabled)
         ✓ If source text chunks are provided, verify:
            - Does the text context support or contradict the coreference hypothesis?
            - Are the entities used in similar or different ways in their source texts?
         ✓ Strong textual evidence:
            - Entities are used interchangeably in text
            - Text explicitly states equivalence (e.g., "also known as", "abbreviated as")
            - Consistent usage patterns across texts
         ✗ Weak or contradictory textual evidence → be more conservative

      DECISION PROCEDURE:
      For Entity 1 and Entity 2, follow these steps IN ORDER:
        
        PHASE 1: CONTEXT VERIFICATION (Execute Steps A-D above)
          → Check context consistency for contradictions
          → Assess context completeness and information quality
          → Analyze relationship patterns
          → Validate source text evidence (if available)
          → Document any issues or concerns found
        
        PHASE 2: COREFERENCE DETERMINATION
          Step 1: Check if names are variations of the same entity (e.g., abbreviations, translations)
          Step 2: Verify relation patterns are consistent (not just similar, but CONSISTENT)
          Step 3: Look for contradictions - if ANY key relations conflict → they are DIFFERENT
          Step 4: Apply substitution test - can they be swapped in ALL contexts?
          Step 5: If uncertain after context verification → answer NO (conservative principle)
        
        PHASE 3: REPRESENTATIVE SELECTION (only if coreferent)
          Choose PRIMARY REPRESENTATIVE based on:
            a) **Formality and Completeness**: Full name > Abbreviation, BUT domain conventions matter
            b) **Domain Convention**: Medical/Academic prefer standard terms, Popular prefers common forms
            c) **Information Richness**: Entity with MORE relationships (visible in context above)
            d) **Naming Quality**: Official name > Colloquial, Standard spelling > Variant
            e) **Cultural Context**: Consider primary language and widely recognized forms

      OUTPUT FORMAT (strict JSON):
      {{
        "is_coreferent": true/false,
        "preferred_representative": "{entity_1_id}" or "{entity_2_id}" or null,
        "rationale": "MUST include: (1) Context verification summary (Steps A-D results), (2) Coreference decision reasoning, (3) If coreferent, representative selection reasoning"
      }}

      IMPORTANT NOTES:
      - Set "preferred_representative" ONLY if "is_coreferent" is true
      - If "is_coreferent" is false, set "preferred_representative" to null
      - The "preferred_representative" must be exactly one of: "{entity_1_id}" or "{entity_2_id}"
      - Always include context verification results in rationale to demonstrate thorough analysis

# Example usage in code:
# 
# config.construction.semantic_dedup.head_dedup.use_llm_for_representative_selection = True
# 
# Then call:
# stats = builder.deduplicate_heads_with_llm_v2()
