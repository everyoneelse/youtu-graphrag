construction:
  chunk_size: 1000
  datasets_no_chunk:
  - hotpot
  - 2wiki
  - musique
  - graphrag-bench
  - annoy_chs
  - annoy_eng
  - demo
  max_workers: 32
  mode: agent
  overlap: 200
  tree_comm:
    embedding_model: all-MiniLM-L6-v2
    enable_fast_mode: true
    struct_weight: 0.3
  semantic_dedup:
    enabled: false
    # Clustering method for initial tail grouping: "embedding" or "llm"
    # - embedding: Fast, uses sentence embeddings + hierarchical clustering
    # - llm: More accurate, uses LLM to understand semantic similarity
    clustering_method: embedding
    embedding_threshold: 0.85
    max_batch_size: 8
    max_candidates: 50
    # Maximum number of tails to send to LLM for clustering at once (only for llm clustering)
    llm_clustering_batch_size: 30
    use_embeddings: true
    prompt_type: general
    # Save intermediate results for debugging and analysis
    save_intermediate_results: false
    # Path to save intermediate results (optional, auto-generated if not specified)
    # intermediate_results_path: "output/dedup_intermediate/"
    
    # Two-step validation for clustering: Enable LLM self-validation of clustering results
    # When enabled, after initial clustering, LLM will check for inconsistencies
    # between cluster descriptions and members, and automatically correct them.
    # Recommended: true when using llm clustering, false for embedding clustering
    enable_clustering_validation: false
    
    # Two-step validation for semantic dedup: Enable LLM self-validation of dedup results
    # When enabled, after semantic deduplication, LLM will check for inconsistencies
    # between group rationales and members, and automatically correct them.
    # This addresses the issue where rationale says "should merge" but members are separate.
    # Recommended: true for critical data, false for cost-sensitive scenarios
    enable_semantic_dedup_validation: false
    
    # Dual LLM configuration: use different models for clustering and deduplication
    # If not specified, both will use the default LLM configuration from environment variables
    # This allows you to use a cheaper/faster model for clustering and a more powerful one for dedup
    clustering_llm:
      # model: "gpt-3.5-turbo"  # Cheaper model for clustering
      # base_url: "https://api.openai.com/v1"
      # api_key: "${CLUSTERING_LLM_API_KEY}"
      # temperature: 0.3
    dedup_llm:
      # model: "gpt-4"  # More powerful model for final deduplication
      # base_url: "https://api.openai.com/v1"
      # api_key: "${DEDUP_LLM_API_KEY}"
      # temperature: 0.0
    
    # Head node deduplication configuration
    # This deduplicates entity nodes globally (across all relations)
    # Should be run AFTER tail deduplication for best results
    head_dedup:
      enabled: false
      # Enable semantic deduplication (exact match always runs first)
      enable_semantic: true
      # Similarity threshold for semantic deduplication (0.0-1.0)
      # Recommended: 0.85-0.90 for production, 0.80-0.85 for exploratory analysis
      similarity_threshold: 0.85
      # Use LLM for validation (slower but more accurate)
      # false: Use embedding-only (fast mode)
      # true: Use LLM to validate candidates (high precision mode)
      use_llm_validation: false
      # Maximum number of candidate pairs to process
      # Recommended: 1000 for medium graphs, 500 for large graphs, 2000 for small graphs
      max_candidates: 1000
      # Embedding similarity threshold for candidate generation (pre-filtering)
      # Lower than similarity_threshold to avoid missing potential matches
      candidate_similarity_threshold: 0.75
      # Maximum number of relations to include in context for each entity
      max_relations_context: 10
      # Export candidates for human review
      export_review: false
      # Confidence range for human review (only export if confidence in this range)
      review_confidence_range: [0.70, 0.90]
      # Output directory for review files
      review_output_dir: "output/review"
      # Use hybrid context (both graph relations and text chunks)
      # false: Use only graph relations (recommended, faster)
      # true: Use both graph relations and text chunks (provides more context, slower and more tokens)
      # When enabled, adds the source chunk text to help LLM make better decisions
      use_hybrid_context: false
      # Save intermediate results for debugging and analysis
      save_intermediate_results: false
      # Path to save intermediate results (optional, auto-generated if not specified)
      # intermediate_results_path: "output/dedup_intermediate/"

datasets:
  hotpot:
    corpus_path: data/hotpotqa/hotpotqa_corpus.json
    qa_path: data/hotpotqa/hotpotqa.json
    schema_path: schemas/hotpot.json
    graph_output: output/graphs/hotpot_new.json
  2wiki:
    corpus_path: data/2wiki/2wikimultihopqa_corpus.json
    qa_path: data/2wiki/2wikimultihopqa.json
    schema_path: schemas/2wiki.json
    graph_output: output/graphs/2wiki_new.json
  musique:
    corpus_path: data/musique/musique_corpus.json
    qa_path: data/musique/musique.json
    schema_path: schemas/musique.json
    graph_output: output/graphs/musique_new.json
  graphrag-bench:
    corpus_path: data/graphrag-bench-reformat/bench_corpus.json
    qa_path: data/graphrag-bench-reformat/graphrag-bench.json
    schema_path: schemas/graphrag-bench.json
    graph_output: output/graphs/graphrag-bench_new.json
  demo:
    corpus_path: data/demo/demo_corpus.json
    qa_path: data/demo/demo.json
    schema_path: schemas/demo.json
    graph_output: output/graphs/demo_new.json
  anony_chs:
    corpus_path: data/anony_chs/final_chunk_corpus.json
    qa_path: data/anony_chs/final_qa_pairs.json
    schema_path: schemas/anony_chs.json
    graph_output: output/graphs/anony_chs_new.json
  anony_eng:
    corpus_path: data/anony_eng/final_chunk_corpus.json
    qa_path: data/anony_eng/final_qa_pairs.json
    schema_path: schemas/anony_eng.json
    graph_output: output/graphs/anony_eng_new.json

embeddings:
  batch_size: 32
  device: cpu
  max_length: 512
  model_name: all-MiniLM-L6-v2

output:
  base_dir: output
  chunks_dir: output/chunks
  graphs_dir: output/graphs
  logs_dir: output/logs
  save_chunk_details: true
  save_intermediate_results: true
performance:
  batch_size: 16
  max_workers: 32
  memory_optimization: true
  parallel_processing: true
prompts:
  construction:
    # NoAgent mode prompts
    general: "You are an expert information extractor and structured data organizer.\
      \ Your task is to analyze the provided text and extract as many valuable entities,\
      \ their attributes, and relations as possible in a structured JSON format. \
      \ \n\nGuidelines:\n1. Schema Compliance: Strictly follow the predefined schema for entity types;\n\
      \   ```{schema}```\n2. Entity Type Mapping Rules:\n\
      \   - Prioritize using the most specific and accurate type from the schema\n\
      \   - If an entity cannot be accurately classified into any specific type, use a generic fallback type if available in the schema (e.g., 'object', 'concept', 'item', 'entity')\n\
      \   - If no suitable type exists in the schema (even generic ones), skip extracting that entity rather than forcing an inaccurate classification\n\
      \   - Do NOT create new entity types not defined in the schema\n3. Flexible Relations and Attributes: You may extract relations and attributes beyond the schema if they capture valuable knowledge\n\
      4. Conciseness: The Attributes and Triples you extract should be complementary with no semantic redundancy\n\
      5. Completeness: Extract all valuable information that can be accurately classified\n6. Output Format: Return\
      \ only JSON as **Example Output** with:  \n   - Attributes: Map each entity\
      \ to its descriptive features  \n   - Triples: List relations between entities\
      \ in `[entity_mention1, relation, entity_mention2]` format\n   - Entity_types:\
      \ Map each entity to its schema type (must use types from provided schema only)\n\n```{chunk}```\n\
      \nExample Output:  \n{{\n  \"attributes\": {{\n    \"Stephen King\": [\"profession:\
      \ author\"]\n  }},\n  \"triples\": [\n    [\"Shawshank Redemption\", \"based\
      \ on\", \"Rita Hayworth and Shawshank Redemption\"],\n    [\"Shawshank Redemption\"\
      , \"directed by\", \"Frank Darabont\"]\n  ],\n  \"entity_types\": {{\n    \"\
      Stephen King\": \"person\",\n    \"Shawshank Redemption\": \"creative_work\"\
      ,\n    \"Rita Hayworth and Shawshank Redemption\": \"creative_work\",\n    \"\
      Frank Darabont\": \"person\"\n  }}\n}}\n"
    # Agent mode prompts (with schema evolution)
    general_agent: "You are an expert information extractor and structured data organizer.\
      \ Your task is to analyze the provided text and extract as many valuable entities,\
      \ their attributes, and relations as possible in a structured JSON format. \
      \ \n\nGuidelines:\n1. Prioritize the following predefined schema for extraction;\n\
      \   ```{schema}```\n2. Flexibility: If the context doesn't fit the predefined\
      \ schema, extract the valuable knowledge as needed;\n3. Conciseness: The Attributes\
      \ and Triples you extract should be complementary and no semantic redundancy.\n\
      4. Do NOT miss any useful information in the context;\n5. Output Format: Return\
      \ only JSON as **Example Output** with:  \n   - Attributes: Map each entity\
      \ to its descriptive features.  \n   - Triples: List relations between entities\
      \ in `[entity_mention1, relation, entity_mention2]` format.\n   - Entity_types:\
      \ Map each entity to its schema type based on the provided schema.\n\n\
      Schema Evolution: If you find new and important entity types, relation types,\
      \ or attribute types that are valuable for knowledge extraction, include them\
      \ in a \"new_schema_types\" field. Notably, the strict threshold of adding\
      \ new schema considering both importance and similarity to the pattern in the\
      \ existing schema is 0.9.\n\n```{chunk}```\n\nExample Output:  \n{{\n  \"attributes\"\
      : {{\n    \"Stephen King\": [\"profession: author\"]\n  }},\n  \"triples\"\
      : [\n    [\"Shawshank Redemption\", \"based on\", \"Rita Hayworth and Shawshank\
      \ Redemption\"],\n    [\"Shawshank Redemption\", \"directed by\", \"Frank Darabont\"\
      ]\n  ],\n  \"entity_types\": {{\n    \"Stephen King\": \"person\",\n    \"\
      Shawshank Redemption\": \"creative_work\",\n    \"Rita Hayworth and Shawshank\
      \ Redemption\": \"creative_work\",\n    \"Frank Darabont\": \"person\"\n  }},\n\
      \  \"new_schema_types\": {{\n      \"nodes\": [\"Movie\"],\n      \"relations\"\
      : [\"starring\"],\n      \"attributes\": [\"genre\"]\n  }}\n}}\n"
    novel: "你是小说信息提取专家，请根据小说内容，参考以下schema，提取小说中的主要人物、人物关系、人物属性、人物事件等关键信息，并严格按照示例返回一个JSON格式。\n\
      \n小说内容:\n```{chunk}```\n\n参考schema:\n```{schema}```\n\n要求：\n1. 实体类型映射规则：\n\
      \   - 优先使用schema中最具体、最准确的类型\n\
      \   - 如果某个实体无法准确分类到任何具体类型，可使用schema中的通用兜底类型（如'object'、'item'、'entity'等，如果有的话）\n\
      \   - 如果schema中没有合适的类型（包括通用类型），则跳过该实体，不要强行分类\n\
      \   - 不要创建schema中没有定义的新类型\n2. 主要人物：包括主角、配角、反派等\n\
      3. 人物关系：包括人物之间的互动、关系等（关系名称可以灵活命名以准确描述关系）\n4. 人物属性：包括人物的职位、称号、绰号、性别、年龄、职业、性格、特长等\n5. 主要事件：包括策略、行动、故事背景、历史事件等\n\
      6. 事件属性：包括事件的地点、时间、事件的参与者、事件的后果等\n7. 实体地点等名称严格使用文本中的名称，不要自己推理和创造\n注意：\n严格抽取triple三元组为[实体,关系,实体]，不要多余元素。\n\
      \n示例输出，包括属性、三元组及实体类型：\n{{\n    \"attributes\": {{\n      \"PERSON#1\": [\"绰号: 智多星\"\
      , \"职位: 智囊团\", \"特长: 策划\"]\n    }},\n    \"triples\": [\n      [\"PERSON#1\"\
      , \"策划\", \"智取生辰纲\"],\n      [\"智取生辰纲\", \"发生地\", \"LOCATION#1\"]\n    ],\n    \"entity_types\": {{\n      \"PERSON#1\": \"person\",\n      \"智取生辰纲\": \"event\",\n      \"LOCATION#1\": \"location\"\n    }}\n}}\n"
    novel_agent: "你是小说信息提取专家，请根据小说内容，参考以下schema，提取小说中的主要人物、人物关系、人物属性、人物事件等关键信息，并严格按照示例返回一个JSON格式。\n\
      \n小说内容:\n```{chunk}```\n\n参考schema:\n```{schema}```\n\n要求：\n1. 主要人物：包括主角、配角、反派等。\n\
      2. 人物关系：包括人物之间的互动、关系等。\n3. 人物属性：包括人物的职位、称号、绰号、性别、年龄、职业、性格、特长等。\n4. 主要事件：包括策略、行动、故事背景、历史事件等。\n\
      5. 事件属性：包括事件的地点、时间、事件的参与者、事件的后果等。\n6. 实体地点等名称严格使用文本中的名称，不要自己推理和创造。\n7. 根据提供的schema为每个实体分配正确的类型。\n注意：\n严格抽取triple三元组为[实体,关系,实体]，不要多余元素。\n\
      \n模式演化：如果你发现新的重要实体类型、关系类型或属性类型对知识提取有价值，请将它们包含在\"new_schema_types\"字段中。注意，添加新模式的严格阈值是0.9，需要考虑重要性和与现有模式的相似性。\n\
      \n示例输出，包括属性、三元组、实体类型及新模式类型：\n{{\n    \"attributes\": {{\n      \"PERSON#1\": [\"绰号: 智多星\"\
      , \"职位: 智囊团\", \"特长: 策划\"]\n    }},\n    \"triples\": [\n      [\"PERSON#1\"\
      , \"策划\", \"智取生辰纲\"],\n      [\"智取生辰纲\", \"发生地\", \"LOCATION#1\"]\n    ],\n    \"entity_types\": {{\n      \"PERSON#1\": \"person\",\n      \"智取生辰纲\": \"event\",\n      \"LOCATION#1\": \"location\"\n    }},\n    \"new_schema_types\": {{\n      \"nodes\": [\"武器\"],\n      \"relations\": [\"使用\"],\n      \"attributes\": [\"材质\"]\n    }}\n}}\n"
    novel_eng: "You are an expert information extractor and structured data organizer.\
      \ Your task is to analyze the provided text and extract as many valuable entities,\
      \ their attributes, and relations as possible in a structured JSON format. \
      \ \n\nGuidelines:\n1. Schema Compliance: Strictly follow the predefined schema for entity types;\n\
      \   ```{schema}```\n2. Entity Type Mapping Rules:\n\
      \   - Prioritize using the most specific and accurate type from the schema\n\
      \   - If an entity cannot be accurately classified into any specific type, use a generic fallback type if available in the schema (e.g., 'object', 'concept', 'item', 'entity')\n\
      \   - If no suitable type exists in the schema (even generic ones), skip extracting that entity rather than forcing an inaccurate classification\n\
      \   - Do NOT create new entity types not defined in the schema\n3. Flexible Relations and Attributes: You may extract relations and attributes beyond the schema if they capture valuable knowledge\n\
      4. Conciseness: The Attributes and Triples you extract should be complementary with no semantic redundancy\n\
      5. Completeness: Extract all valuable information that can be accurately classified\n6. Output Format: Return\
      \ only JSON as **Example Output** with:  \n   - Attributes: Map each entity\
      \ to its descriptive features  \n   - Triples: List relations between entities\
      \ in `[entity_mention1, relation, entity_mention2]` format\n   - Entity_types:\
      \ Map each entity to its schema type (must use types from provided schema only)\n\nChunk:\n\
      ```{chunk}```\n\nExample Output:  \n{{\n  \"attributes\": {{\n    \"PERSON#1\"\
      : [\"profession: writer\"]\n  }},\n  \"triples\": [\n    [\"PERSON#1\", \"\
      located at\", \"LOCATION#1\"],\n    [\"PERSON#1\", \"good at\", \"playing piano\"\
      ]\n  ],\n  \"entity_types\": {{\n    \"PERSON#1\": \"person\",\n    \"LOCATION#1\"\
      : \"location\"\n  }}\n}}\n"
    novel_eng_agent: "You are an expert information extractor and structured data organizer.\
      \ Your task is to analyze the provided text and extract as many valuable entities,\
      \ their attributes, and relations as possible in a structured JSON format. \
      \ \n\nGuidelines:\n1. Prioritize the following predefined schema for extraction;\n\
      \   ```{schema}```\n2. Flexibility: If the context doesn't fit the predefined\
      \ schema, extract the valuable knowledge as needed;\n3. Conciseness: The Attributes\
      \ and Triples you extract should be complementary and no semantic redundancy.\n\
      4. Do NOT miss any useful information in the context;\n5. Output Format: Return\
      \ only JSON as **Example Output** with:  \n   - Attributes: Map each entity\
      \ to its descriptive features.  \n   - Triples: List relations between entities\
      \ in `[entity_mention1, relation, entity_mention2]` format.\n   - Entity_types:\
      \ Map each entity to its schema type based on the provided schema.\n\n\
      Schema Evolution: If you find new and important entity types, relation types,\
      \ or attribute types that are valuable for knowledge extraction, include them\
      \ in a \"new_schema_types\" field. Notably, the strict threshold of adding\
      \ new schema considering both importance and similarity to the pattern in the\
      \ existing schema is 0.9.\n\nChunk:\n```{chunk}```\n\nExample Output:  \n{{\n\
      \  \"attributes\": {{\n    \"PERSON#1\": [\"profession: writer\"]\n  }},\n\
      \  \"triples\": [\n    [\"PERSON#1\", \"located at\", \"LOCATION#1\"],\n  \
      \  [\"PERSON#1\", \"good at\", \"playing piano\"]\n  ],\n  \"entity_types\"\
      : {{\n    \"PERSON#1\": \"person\",\n    \"LOCATION#1\": \"location\"\n  }},\n\
      \  \"new_schema_types\": {{\n      \"nodes\": [\"Instrument\"],\n      \"relations\"\
      : [\"owns\"],\n      \"attributes\": [\"skill_level\"]\n  }}\n}}\n"
  semantic_dedup:
    general: |-
      You are a knowledge graph curation assistant. All listed triples share the same head entity and relation.
      Your task is to group candidate tail descriptions that are semantically equivalent.

      Head entity: {head}
      Relation: {relation}

      Head contexts:
      {head_context}

      Candidate tails:
      {candidates}

      Instructions:
      1. Use the provided contexts when comparing meanings. Group tails that refer to the same real-world entity or express the same fact.
      2. Keep tails separate when their meanings differ or when the contexts describe different situations.
      3. Choose the most informative mention in each group as the representative index.
      4. Every input index must appear in exactly one group.

      Respond with strict JSON using this schema:
      {{
        "groups": [
          {{
            "members": [1, 3],
            "representative": 3,
            "rationale": "Why the members belong together."
          }}
        ]
      }}
    
    attribute: |-
      You are a knowledge graph curation assistant. All listed triples share the same head entity and relation.
      Your task is to identify and merge ONLY duplicate or redundant attribute values.

      Head entity: {head}
      Relation: {relation}

      Head contexts:
      {head_context}

      Candidate attribute values:
      {candidates}

      CRITICAL INSTRUCTIONS for attribute deduplication:
      1. **ONLY merge if attribute values are truly identical or redundant**:
         - Same attribute expressed differently (e.g., "高度: 10cm" and "height: 10 centimeters")
         - Complete redundancy (e.g., "颜色是红色" and "红色")
      
      2. **NEVER merge if**:
         - Attributes describe different properties (even if related to the same entity)
         - Attributes have different values or measurements
         - Attributes describe different aspects or characteristics
         - Attributes come from the same context but convey different information
      
      3. **Example - DO NOT MERGE**:
         - [1] "角度依赖性、组织依赖性、TE依赖性" (three characteristics)
         - [2] "T2弛豫时间最多可延长两倍以上" (specific measurement)
         → These are DIFFERENT attributes, keep separate!
      
      4. **Example - SHOULD MERGE**:
         - [1] "颜色: 红色"
         - [2] "color: red"
         → Same attribute, same value, can merge
      
      5. Choose the most complete and informative description as the representative.
      6. Every input index must appear in exactly one group.
      7. **When in doubt, keep them separate** - it's better to have duplicate attributes than to lose information.

      Respond with strict JSON using this schema:
      {{
        "groups": [
          {{
            "members": [1, 3],
            "representative": 3,
            "rationale": "Why these attribute values are truly identical or redundant."
          }}
        ]
      }}
  
  # Head node deduplication prompt (for entity coreference resolution)
  head_dedup:
    general: |-
      You are an expert in knowledge graph entity resolution.

      TASK: Determine if the following two entities refer to the SAME real-world object.

      Entity 1: {entity_1}
      Related knowledge about Entity 1:
      {context_1}

      Entity 2: {entity_2}
      Related knowledge about Entity 2:
      {context_2}

      CRITICAL RULES:
      1. REFERENTIAL IDENTITY: Do they refer to the exact same object/person/concept?
         - Same entity with different names → YES (e.g., "NYC" = "New York City")
         - Different but related entities → NO (e.g., "Apple Inc." ≠ "Apple Store")

      2. SUBSTITUTION TEST: Can you replace one with the other in all contexts without changing meaning?
         - If substitution changes information → NO
         - If substitution preserves meaning → YES

      3. TYPE CONSISTENCY: Check entity types/categories
         - Same name, different types → carefully verify with context
         - Example: "Apple (company)" ≠ "Apple (fruit)"

      4. CONSERVATIVE PRINCIPLE:
         - When uncertain → answer NO
         - False merge is worse than false split

      PROHIBITED MERGE REASONS (NOT valid reasons to merge):
      ✗ Similar names: "John Smith" vs "John Smith Jr." → different persons
      ✗ Related entities: "Apple Inc." vs "Apple Store" → company vs retail location
      ✗ Same category: Both are cities → might be different cities
      ✗ Shared relations: Both have similar relations → need comprehensive match
      ✗ Partial overlap: Some relations match → need ALL key relations to match
      
      DECISION PROCEDURE:
      For Entity 1 and Entity 2:
        1. Check if names are variations of the same entity (e.g., abbreviations, translations)
        2. Compare their relation patterns - do they have consistent relationships?
        3. Look for contradictions - if any key relations conflict, they are DIFFERENT
        4. Apply substitution test - can they be swapped in all contexts?
        5. If uncertain → answer NO (conservative principle)

      OUTPUT FORMAT (strict JSON):
      {{
        "is_coreferent": true/false,
        "confidence": 0.0-1.0,
        "rationale": "Clear explanation based on referential identity test"
      }}

      EXAMPLES:
      
      Example 1 - SHOULD MERGE:
      Entity 1: "UN", relations: [founded→1945, member→United States]
      Entity 2: "United Nations", relations: [established→1945, member→USA]
      → is_coreferent: true, confidence: 0.95
      → Rationale: "UN" is abbreviation of "United Nations", consistent founding year and members
      
      Example 2 - SHOULD NOT MERGE:
      Entity 1: "Apple Inc.", relations: [founded_by→Steve Jobs, produces→iPhone]
      Entity 2: "Apple Store", relations: [owned_by→Apple Inc., located_in→New York]
      → is_coreferent: false, confidence: 0.95
      → Rationale: Different entities - Apple Inc. is company, Apple Store is retail location, ownership relation indicates hierarchy
      
      Example 3 - UNCERTAIN (should answer NO):
      Entity 1: "张三", relations: [works_at→清华大学, age→45]
      Entity 2: "张三", relations: [studies_at→北京大学, age→22]
      → is_coreferent: false, confidence: 0.80
      → Rationale: Same name but different age and occupation suggest different persons (conservative principle applied)

      

    # Improved: Information Identity Principle (fundamental approach)
    with_representative_selection: |-
      You are an expert in knowledge graph entity deduplication.

      TASK: Determine if two entities contain EXACTLY THE SAME INFORMATION.

      Entity 1 (ID: {entity_1_id}): {entity_1_desc}
      Graph relationships:
      {graph_context_1}
      Source text:
      {chunk_context_1}

      Entity 2 (ID: {entity_2_id}): {entity_2_desc}
      Graph relationships:
      {graph_context_2}
      Source text:
      {chunk_context_2}

      ═══════════════════════════════════════════════════════════

      FUNDAMENTAL PRINCIPLE: Information Identity

      Entities should be merged if and only if they contain EXACTLY THE SAME INFORMATION.

      Two conditions must BOTH be satisfied:

      1. REFERENTIAL IDENTITY (指称相同)
         → They refer to the exact same real-world object
      
      2. INFORMATION EQUIVALENCE (信息等价)
         → Replacing one with the other is lossless in BOTH directions

      ═══════════════════════════════════════════════════════════

      STEP 1: REFERENTIAL IDENTITY CHECK

      Question: Do Entity 1 and Entity 2 refer to the EXACT SAME real-world object?

      Use evidence from:
      - Source text contexts (how they are described and used)
      - Graph relationships (their connections to other entities)
      - Domain knowledge (what you know about this domain)

      Tests:
      ✓ Same object with different names → Potentially yes (go to Step 2)
      ✗ Different objects (even if related) → No, KEEP SEPARATE
      ✗ Part-whole relationship → No, KEEP SEPARATE  
      ✗ Hierarchical relationship → No, KEEP SEPARATE
      ✗ ANY contradicting evidence → No, KEEP SEPARATE

      If referentially different → OUTPUT: is_coreferent = false, STOP

      ═══════════════════════════════════════════════════════════

      STEP 2: INFORMATION EQUIVALENCE CHECK (only if Step 1 = YES)

      Even if they refer to the same object, they might contain DIFFERENT INFORMATION about it.

      Question: Do they contain the exact same information, or does one contain more specific/detailed information?

      CRITICAL: Test substitution in BOTH directions

      Test A - Entity 1 → Entity 2:
        • Take Entity 1's source context
        • Imagine replacing Entity 1's name/description with Entity 2's
        • Ask: "Is there ANY information loss?"
          - Precision loss? (specific term → vague term)
          - Detail loss? (detailed description → simplified)
          - Specificity loss? (unambiguous → ambiguous)
          - Contextual information loss? (bound to specific scenario → generic)
        • Verdict: LOSSLESS (yes) or HAS LOSS (no)

      Test B - Entity 2 → Entity 1:
        • Take Entity 2's source context  
        • Imagine replacing Entity 2's name/description with Entity 1's
        • Ask: "Is there ANY information loss?"
        • Verdict: LOSSLESS (yes) or HAS LOSS (no)

      Symmetry Evaluation:
        IF both Test A = LOSSLESS AND Test B = LOSSLESS
          → SYMMETRIC substitution
          → Information is equivalent
          → Should MERGE
        
        IF Test A = HAS LOSS OR Test B = HAS LOSS
          → ASYMMETRIC substitution
          → Different information content
          → Should KEEP SEPARATE

      ═══════════════════════════════════════════════════════════

      EXAMPLES:

      Example 1 - Symmetric (MERGE):
        Entity A: "United Nations"
        Entity B: "UN"
        
        Test A→B: "The United Nations voted" → "The UN voted"
                  Information loss? No ✓
        Test B→A: "The UN voted" → "The United Nations voted"
                  Information loss? No ✓
        
        Result: Symmetric → Same information → MERGE

      Example 2 - Asymmetric (KEEP SEPARATE):
        Entity A: "增加读出带宽" (increase readout bandwidth - specific)
        Entity B: "加大带宽" (increase bandwidth - vague)
        
        Test A→B: "通过增加读出带宽解决伪影"
               → "通过加大带宽解决伪影"
                  Information loss? Yes, "读出" specificity lost ✗
        
        Test B→A: "解决方法：加大带宽"
               → "解决方法：增加读出带宽"
                  Information loss? No ✓
        
        Result: Asymmetric → Different information → KEEP SEPARATE
        
        Explanation: Entity A contains MORE SPECIFIC information (which bandwidth).
        They refer to the same operation, but A is more precise than B.

      Example 3 - Contextual binding (KEEP SEPARATE):
        Entity A: "提高带宽" 
          Source text: "流动伪影可通过提高带宽来减轻"
        Entity B: "提高带宽"
          Source text: "化学位移伪影的解决方法：提高带宽"
        
        Test A→B: "流动伪影可通过[提高带宽(from B)]来减轻"
                  Information loss? Yes, loses the context that this bandwidth 
                  adjustment is specifically for flow artifacts ✗
        
        Test B→A: "化学位移伪影的解决方法：[提高带宽(from A)]"
                  Information loss? Yes, loses the context that this bandwidth
                  adjustment is specifically for chemical shift artifacts ✗
        
        Result: Both directions have contextual information loss → KEEP SEPARATE
        
        Explanation: Although both describe "提高带宽" operation, each entity 
        contains CONTEXTUAL BINDING INFORMATION to a specific problem. Replacing
        one with the other loses which problem it's bound to.

      ═══════════════════════════════════════════════════════════

      PROHIBITED MERGE REASONS:

      These are NOT valid reasons to merge:
      ✗ Similar names
      ✗ Same category or type
      ✗ Similar graph relationships
      ✗ Same community membership
      ✗ Related/associated entities
      ✗ Co-occurrence in contexts
      ✗ Partial information overlap

      Only merge if information is COMPLETELY identical.

      ═══════════════════════════════════════════════════════════

      CONSERVATIVE PRINCIPLE:

      When uncertain about information loss → KEEP SEPARATE

      Rationale: Preserving information distinctions is more important than graph simplicity.
      False negatives (missing a merge) are better than false positives (losing information).

      ═══════════════════════════════════════════════════════════

      REPRESENTATIVE SELECTION (only if merging):

      If information is equivalent, choose the representative based on:
      • Formality and completeness (more formal/complete preferred)
      • Domain convention (standard terminology preferred)
      • Information richness (more graph relationships preferred)
      • Naming quality (official/standard preferred over colloquial)

      ═══════════════════════════════════════════════════════════

      OUTPUT FORMAT (strict JSON):
      {{
        "is_coreferent": true/false,
        "substitution_lossless_1to2": true/false/null,
        "substitution_lossless_2to1": true/false/null,
        "information_identity": true/false,
        "preferred_representative": "{entity_1_id}" or "{entity_2_id}" or null,
        "rationale": "Provide UNIFIED analysis: (1) Referential identity - do they refer to the same object? Cite specific evidence from source text and graph. (2) Information equivalence - test BOTH substitution directions explicitly. State clearly if there is ANY information loss in either direction (precision, detail, specificity, context). (3) Decision and reasoning. (4) If merging, explain choice of representative."
      }}

      CRITICAL REQUIREMENTS:
      - Set substitution_lossless_* to null if is_coreferent = false
      - Set information_identity = true ONLY if BOTH substitution tests are lossless
      - Set preferred_representative ONLY if information_identity = true
      - In rationale, EXPLICITLY describe what information (if any) would be lost in each substitution direction
      - Do NOT merge if you detect ANY information asymmetry

      EXAMPLES (demonstrating how to apply the principles):
      
      Example 1 - SHOULD MERGE (passes all merge conditions):
      
      Entity 1 (entity_100): "UN"
      Graph relationships:
        • founded → 1945
        • member → United States
        • headquarters → New York
      Source text:
        (Not available)
      
      Entity 2 (entity_150): "United Nations"
      Graph relationships:
        • established → 1945
        • member → USA
      Source text:
        (Not available)
      
      Analysis:
      → REFERENT TEST: "UN" is the standard abbreviation of "United Nations" - SAME object ✓
      → SUBSTITUTION TEST: Can swap in all contexts without changing meaning ✓
      → NO CONTRADICTIONS: Founding year matches (1945), members consistent (USA = United States) ✓
      → EQUIVALENCE CLASS: Both refer to the same international organization ✓
      
      → is_coreferent: true
      → preferred_representative: "entity_100"
      → Rationale: "Names show 'UN' is the standard abbreviation of 'United Nations' - they refer to the same international organization. Graph evidence confirms: founding year 1945 matches, members are consistent (United States = USA). No contradictions found. Substitution test passes: replacing one with the other preserves meaning in all contexts. Entity_100 chosen as representative: has more graph relationships (3 vs 2) providing richer context, and 'UN' is the widely recognized form in international contexts."
      
      Example 2 - SHOULD NOT MERGE (violates merge condition 3: hierarchical relationship):
      
      Entity 1 (entity_200): "Apple Inc."
      Graph relationships:
        • founded_by → Steve Jobs
        • type → 公司
      Source text:
        (Not available)
      
      Entity 2 (entity_250): "Apple Store"
      Graph relationships:
        • owned_by → Apple Inc.
        • type → 零售店
      Source text:
        (Not available)
      
      Analysis:
      → REFERENT TEST: Different entities - company vs retail store ✗
      → CONTRADICTION CHECK: Hierarchical relationship detected (owned_by) ✗
      → Type conflict: 公司 (company) vs 零售店 (retail store) ✗
      → PROHIBITED REASON: "Related entities" - this is company-subsidiary relationship ✗
      
      → is_coreferent: false
      → preferred_representative: null
      → Rationale: "These are different entities with a hierarchical ownership relationship. Graph shows entity_250 is 'owned_by' entity_200, indicating a company-subsidiary relationship, not identity. Types differ: entity_200 is '公司' (company) while entity_250 is '零售店' (retail store). Substitution test fails: replacing 'Apple Inc.' with 'Apple Store' changes meaning. Violates merge condition 3 (hierarchical relationships must be kept separate). This falls under PROHIBITED REASON: related entities."
      
      Example 3 - SHOULD NOT MERGE (violates merge condition 3: contradictions):
      
      Entity 1 (entity_300): "张三"
      Graph relationships:
        • works_at → 清华大学
        • age → 45
        • position → 教授
      Source text:
        (Not available)
      
      Entity 2 (entity_350): "张三"
      Graph relationships:
        • studies_at → 北京大学
        • age → 22
        • status → 学生
      Source text:
        (Not available)
      
      Analysis:
      → REFERENT TEST: Same name but likely different people ✗
      → CONTRADICTION CHECK: Multiple contradictions detected ✗
        - Age: 45 vs 22 (conflicting attributes)
        - Role: 教授 vs 学生 (professor vs student)
        - Institution: 清华大学 vs 北京大学 (different universities)
        - Relationship type: works_at vs studies_at (incompatible)
      → CRITICAL DISTINCTION: Similar patterns (both at universities) but DIFFERENT entities
      
      → is_coreferent: false
      → preferred_representative: null
      → Rationale: "Despite identical names '张三', these are different people. Multiple contradictions in graph evidence: age differs significantly (45 vs 22), professional status conflicts (教授 vs 学生), institutions differ (清华大学 vs 北京大学), and relationship types are incompatible (works_at vs studies_at). This demonstrates CRITICAL DISTINCTION: similar patterns (both at universities with same name) does not equal identity. Violates merge condition 3 (contradictions must keep entities separate). Conservative principle: these are different persons who happen to share a common name."


  
  decomposition:
    general: "You are a professional question decomposition expert specializing in\
      \ multi-hop reasoning.\nGiven the following ontology and the question, decompose\
      \ the complex question into 2-3 focused sub-questions and identify involved schema types.\n\nCRITICAL REQUIREMENTS:\n\
      1. Each sub-question must be:\n   - Specific and focused on a single fact or\
      \ relationship by identifing all entities, relationships, and reasoning steps\
      \ needed\n   - Answerable independently with the given ontology\n   - Explicitly\
      \ reference entities and relations from the original question\n   - Designed\
      \ to retrieve relevant knowledge for the final answer\n\n2. For simple questions\
      \ (1-2 hop), return the original question as a single sub-question\n3. Analyze the question and identify all schema types that might be involved\n4. Return\
      \ a JSON object with sub_questions array and involved_types object.\n\nOntology:\n{ontology}\n\n\
      Question: {question}\n\nExample for complex question:\nOriginal: \"Which film\
      \ has the director died earlier, Ethnic Notions or Gordon Of Ghost City?\"\n\
      Output:\n{{\n  \"sub_questions\": [\n    {{\"sub-question\": \"Who is the director of Ethnic Notions?\"\
      }},\n    {{\"sub-question\": \"Who is the director of Gordon Of Ghost City?\"\
      }},\n    {{\"sub-question\": \"When did the director of Ethnic Notions die?\"\
      }},\n    {{\"sub-question\": \"When did the director of Gordon Of Ghost City\
      \ die?\"}}\n  ],\n  \"involved_types\": {{\n    \"nodes\": [\"creative_work\", \"person\"],\n    \"relations\": [\"directed_by\"],\n    \"attributes\": [\"name\", \"date\"]\n  }}\n}}\n\nExample for simple question:\nOriginal: \"What is the capital\
      \ of France?\"\nOutput:\n{{\n  \"sub_questions\": [\n    {{\"sub-question\": \"What is the capital\
      \ of France?\"}}\n  ],\n  \"involved_types\": {{\n    \"nodes\": [\"location\"],\n    \"relations\": [\"located_in\"],\n    \"attributes\": [\"name\"]\n  }}\n}}\n"
    novel: "你是一个专业的问题分解大师，请根据以下问题和图本体模式，将问题分解为2-3个子问题，并识别涉及的schema类型。\n   要求：\n   1. 每个子问题必须：\n \
      \     - 明确且专注于一个事实或关系，通过识别所有实体、关系和推理步骤\n      - 明确引用原始问题中的实体和关系\n      - 设计为检索最终答案所需的相关知识\n\
      \   2. 对于简单问题（1-2跳），返回原始问题作为单个子问题\n   3. 分析问题并识别所有可能涉及的schema类型\n   4. 返回一个JSON对象，包含sub_questions数组和involved_types对象。\n   \n   问题：{question}\n\
      \   \n   图本体模式：{ontology}\n   \n   示例：\n   原始问题：\\\"智取生辰纲事件中，PERSON#1的策略为什么能够成功\\\"\n\
      \   输出：\n   {{\n       \\\"sub_questions\\\": [\n           {{\\\"sub-question\\\": \\\"智取生辰纲中PERSON#1的策略是什么？\\\"}},\n  \
      \           {{\\\"sub-question\\\": \\\"智取生辰纲中的PERSON、LOCATION有什么特殊属性？\\\"}}\n       ],\n       \\\"involved_types\\\": {{\n           \\\"nodes\\\": [\\\"person\\\", \\\"event\\\", \\\"location\\\"],\n           \\\"relations\\\": [\\\"策划\\\", \\\"发生地\\\"],\n           \\\"attributes\\\": [\\\"绰号\\\", \\\"职位\\\", \\\"特长\\\"]\n       }}\n   }}\n   如果是简单问题，返回原始问题作为单个子问题。\n\
      \   原始问题：\\\"智取生辰纲事件中，PERSON#1是谁\\\"\n   输出：\n   {{\n       \\\"sub_questions\\\": [\n           {{\\\"sub-question\\\": \\\"\
      智取生辰纲事件中，PERSON#1是谁？\\\"}}\n       ],\n       \\\"involved_types\\\": {{\n           \\\"nodes\\\": [\\\"person\\\", \\\"event\\\"],\n           \\\"relations\\\": [\\\"参与\\\"],\n           \\\"attributes\\\": [\\\"name\\\"]\n       }}\n   }}\n"
  retrieval:
    general: 'You are an expert knowledge assistant. Your task is to answer the question
      based on the provided knowledge context.


      1. Use ONLY the information from the provided knowledge context and try your
      best to answer the question.

      2. If the knowledge is insufficient, reject to answer the question.

      3. Be precise and concise in your answer

      4. For factual questions, provide the specific fact or entity name

      5. For temporal questions, provide the specific date, year, or time period


      Question: {question}


      Knowledge Context:

      {context}


      Answer (be specific and direct):

      '
    ircot: 'You are an expert knowledge assistant using iterative retrieval with chain-of-thought
      reasoning.


      Current Question: {current_query}


      Available Knowledge Context:

      {context}


      Previous Thoughts: {previous_thoughts}


      Step {step}: Please think step by step about what additional information you
      need to answer the question completely and accurately.


      Instructions:

      1. Analyze the current knowledge context and the question

      2. Think about what information might be missing or unclear

      3. If you have enough information to answer, in the end of your response, write
      "So the answer is:" followed by your final answer

      4. If you need more information, in the end of your response, write a specific
      query begin with "The new query is:" to retrieve additional relevant information

      5. Be specific and focused in your reasoning


      Your reasoning:

      '
    novel: '你是小说知识助手，你的任务是根据提供的小说知识库回答问题。

      1. 如果知识库中的信息不足以回答问题，请根据你的推理和知识回答。

      2. 回答要简洁明了。

      3. 对于事实性问题，提供具体的事实或人物名称。

      4. 对于时间性问题，提供具体的时间、年份或时间段。

      问题：{question}

      相关知识：{context}

      答案（简洁明了）：

      '
    novel_eng: "You are a novel knowledge assistant. Your task is to answer the question\
      \ based on the provided novel knowledge context.\n1. If the knowledge is insufficient,\
      \ answer the question based on your own knowledge.\n2. Be precise and concise\
      \ in your answer.\n3. For factual questions, provide the specific fact or entity\
      \ name\n4. For temporal questions, provide the specific date, year, or time\
      \ period\n\nQuestion: {question}\n\nKnowledge Context:\n{context}   \n\nAnswer\
      \ (be specific and direct):\n"
retrieval:
  agent:
    enable_ircot: true
    enable_parallel_subquestions: true
    max_steps: 5
  cache_dir: retriever/faiss_cache_new
  enable_caching: true
  enable_high_recall: true
  enable_query_enhancement: true
  enable_reranking: true
  faiss:
    device: cpu
    max_workers: 4
    search_k: 50
  recall_paths: 2
  similarity_threshold: 0.3
  top_k: 20
  top_k_filter: 20
triggers:
  constructor_trigger: true
  mode: agent
  retrieve_trigger: true
