construction:
  chunk_size: 1000
  datasets_no_chunk:
  - hotpot
  - 2wiki
  - musique
  - graphrag-bench
  - annoy_chs
  - annoy_eng
  - demo
  max_workers: 32
  mode: agent
  overlap: 200
  tree_comm:
    embedding_model: all-MiniLM-L6-v2
    enable_fast_mode: true
    struct_weight: 0.3
  semantic_dedup:
    enabled: false
    # Clustering method for initial tail grouping: "embedding" or "llm"
    # - embedding: Fast, uses sentence embeddings + hierarchical clustering
    # - llm: More accurate, uses LLM to understand semantic similarity
    clustering_method: embedding
    embedding_threshold: 0.85
    max_batch_size: 8
    max_candidates: 50
    # Maximum number of tails to send to LLM for clustering at once (only for llm clustering)
    llm_clustering_batch_size: 30
    use_embeddings: true
    prompt_type: general
    # Save intermediate results for debugging and analysis
    save_intermediate_results: false
    # Path to save intermediate results (optional, auto-generated if not specified)
    # intermediate_results_path: "output/dedup_intermediate/"
    
    # Two-step validation for clustering: Enable LLM self-validation of clustering results
    # When enabled, after initial clustering, LLM will check for inconsistencies
    # between cluster descriptions and members, and automatically correct them.
    # Recommended: true when using llm clustering, false for embedding clustering
    enable_clustering_validation: false
    
    # Two-step validation for semantic dedup: Enable LLM self-validation of dedup results
    # When enabled, after semantic deduplication, LLM will check for inconsistencies
    # between group rationales and members, and automatically correct them.
    # This addresses the issue where rationale says "should merge" but members are separate.
    # Recommended: true for critical data, false for cost-sensitive scenarios
    enable_semantic_dedup_validation: false
    
    # Dual LLM configuration: use different models for clustering and deduplication
    # If not specified, both will use the default LLM configuration from environment variables
    # This allows you to use a cheaper/faster model for clustering and a more powerful one for dedup
    clustering_llm:
      # model: "gpt-3.5-turbo"  # Cheaper model for clustering
      # base_url: "https://api.openai.com/v1"
      # api_key: "${CLUSTERING_LLM_API_KEY}"
      # temperature: 0.3
    dedup_llm:
      # model: "gpt-4"  # More powerful model for final deduplication
      # base_url: "https://api.openai.com/v1"
      # api_key: "${DEDUP_LLM_API_KEY}"
      # temperature: 0.0
    
    # Head node deduplication configuration
    # This deduplicates entity nodes globally (across all relations)
    # Should be run AFTER tail deduplication for best results
    head_dedup:
      enabled: false
      # Enable semantic deduplication (exact match always runs first)
      enable_semantic: true
      # Similarity threshold for semantic deduplication (0.0-1.0)
      # Recommended: 0.85-0.90 for production, 0.80-0.85 for exploratory analysis
      similarity_threshold: 0.85
      # Use LLM for validation (slower but more accurate)
      # false: Use embedding-only (fast mode)
      # true: Use LLM to validate candidates (high precision mode)
      use_llm_validation: false
      # Maximum number of candidate pairs to process
      # Recommended: 1000 for medium graphs, 500 for large graphs, 2000 for small graphs
      max_candidates: 1000
      # Embedding similarity threshold for candidate generation (pre-filtering)
      # Lower than similarity_threshold to avoid missing potential matches
      candidate_similarity_threshold: 0.75
      # Maximum number of relations to include in context for each entity
      max_relations_context: 10
      # Export candidates for human review
      export_review: false
      # Confidence range for human review (only export if confidence in this range)
      review_confidence_range: [0.70, 0.90]
      # Output directory for review files
      review_output_dir: "output/review"
      # Use hybrid context (both graph relations and text chunks)
      # false: Use only graph relations (recommended, faster)
      # true: Use both graph relations and text chunks (provides more context, slower and more tokens)
      # When enabled, adds the source chunk text to help LLM make better decisions
      use_hybrid_context: false
      # Save intermediate results for debugging and analysis
      save_intermediate_results: false
      # Path to save intermediate results (optional, auto-generated if not specified)
      # intermediate_results_path: "output/dedup_intermediate/"

datasets:
  hotpot:
    corpus_path: data/hotpotqa/hotpotqa_corpus.json
    qa_path: data/hotpotqa/hotpotqa.json
    schema_path: schemas/hotpot.json
    graph_output: output/graphs/hotpot_new.json
  2wiki:
    corpus_path: data/2wiki/2wikimultihopqa_corpus.json
    qa_path: data/2wiki/2wikimultihopqa.json
    schema_path: schemas/2wiki.json
    graph_output: output/graphs/2wiki_new.json
  musique:
    corpus_path: data/musique/musique_corpus.json
    qa_path: data/musique/musique.json
    schema_path: schemas/musique.json
    graph_output: output/graphs/musique_new.json
  graphrag-bench:
    corpus_path: data/graphrag-bench-reformat/bench_corpus.json
    qa_path: data/graphrag-bench-reformat/graphrag-bench.json
    schema_path: schemas/graphrag-bench.json
    graph_output: output/graphs/graphrag-bench_new.json
  demo:
    corpus_path: data/demo/demo_corpus.json
    qa_path: data/demo/demo.json
    schema_path: schemas/demo.json
    graph_output: output/graphs/demo_new.json
  anony_chs:
    corpus_path: data/anony_chs/final_chunk_corpus.json
    qa_path: data/anony_chs/final_qa_pairs.json
    schema_path: schemas/anony_chs.json
    graph_output: output/graphs/anony_chs_new.json
  anony_eng:
    corpus_path: data/anony_eng/final_chunk_corpus.json
    qa_path: data/anony_eng/final_qa_pairs.json
    schema_path: schemas/anony_eng.json
    graph_output: output/graphs/anony_eng_new.json

embeddings:
  batch_size: 32
  device: cpu
  max_length: 512
  model_name: all-MiniLM-L6-v2

output:
  base_dir: output
  chunks_dir: output/chunks
  graphs_dir: output/graphs
  logs_dir: output/logs
  save_chunk_details: true
  save_intermediate_results: true
performance:
  batch_size: 16
  max_workers: 32
  memory_optimization: true
  parallel_processing: true
prompts:
  construction:
    # NoAgent mode prompts
    general: "You are an expert information extractor and structured data organizer.\
      \ Your task is to analyze the provided text and extract as many valuable entities,\
      \ their attributes, and relations as possible in a structured JSON format. \
      \ \n\nGuidelines:\n1. Prioritize the following predefined schema for extraction;\n\
      \   ```{schema}```\n2. Flexibility: If the context doesn't fit the predefined\
      \ schema, extract the valuable knowledge as needed;\n3. Conciseness: The Attributes\
      \ and Triples you extract should be complementary and no semantic redundancy.\n\
      4. Do NOT miss any useful information in the context;\n5. Output Format: Return\
      \ only JSON as **Example Output** with:  \n   - Attributes: Map each entity\
      \ to its descriptive features.  \n   - Triples: List relations between entities\
      \ in `[entity_mention1, relation, entity_mention2]` format.\n   - Entity_types:\
      \ Map each entity to its schema type based on the provided schema.\n\n```{chunk}```\n\
      \nExample Output:  \n{{\n  \"attributes\": {{\n    \"Stephen King\": [\"profession:\
      \ author\"]\n  }},\n  \"triples\": [\n    [\"Shawshank Redemption\", \"based\
      \ on\", \"Rita Hayworth and Shawshank Redemption\"],\n    [\"Shawshank Redemption\"\
      , \"directed by\", \"Frank Darabont\"]\n  ],\n  \"entity_types\": {{\n    \"\
      Stephen King\": \"person\",\n    \"Shawshank Redemption\": \"creative_work\"\
      ,\n    \"Rita Hayworth and Shawshank Redemption\": \"creative_work\",\n    \"\
      Frank Darabont\": \"person\"\n  }}\n}}\n"
    # Agent mode prompts (with schema evolution)
    general_agent: "You are an expert information extractor and structured data organizer.\
      \ Your task is to analyze the provided text and extract as many valuable entities,\
      \ their attributes, and relations as possible in a structured JSON format. \
      \ \n\nGuidelines:\n1. Prioritize the following predefined schema for extraction;\n\
      \   ```{schema}```\n2. Flexibility: If the context doesn't fit the predefined\
      \ schema, extract the valuable knowledge as needed;\n3. Conciseness: The Attributes\
      \ and Triples you extract should be complementary and no semantic redundancy.\n\
      4. Do NOT miss any useful information in the context;\n5. Output Format: Return\
      \ only JSON as **Example Output** with:  \n   - Attributes: Map each entity\
      \ to its descriptive features.  \n   - Triples: List relations between entities\
      \ in `[entity_mention1, relation, entity_mention2]` format.\n   - Entity_types:\
      \ Map each entity to its schema type based on the provided schema.\n\n\
      Schema Evolution: If you find new and important entity types, relation types,\
      \ or attribute types that are valuable for knowledge extraction, include them\
      \ in a \"new_schema_types\" field. Notably, the strict threshold of adding\
      \ new schema considering both importance and similarity to the pattern in the\
      \ existing schema is 0.9.\n\n```{chunk}```\n\nExample Output:  \n{{\n  \"attributes\"\
      : {{\n    \"Stephen King\": [\"profession: author\"]\n  }},\n  \"triples\"\
      : [\n    [\"Shawshank Redemption\", \"based on\", \"Rita Hayworth and Shawshank\
      \ Redemption\"],\n    [\"Shawshank Redemption\", \"directed by\", \"Frank Darabont\"\
      ]\n  ],\n  \"entity_types\": {{\n    \"Stephen King\": \"person\",\n    \"\
      Shawshank Redemption\": \"creative_work\",\n    \"Rita Hayworth and Shawshank\
      \ Redemption\": \"creative_work\",\n    \"Frank Darabont\": \"person\"\n  }},\n\
      \  \"new_schema_types\": {{\n      \"nodes\": [\"Movie\"],\n      \"relations\"\
      : [\"starring\"],\n      \"attributes\": [\"genre\"]\n  }}\n}}\n"
    novel: "你是小说信息提取专家，请根据小说内容，参考以下schema，提取小说中的主要人物、人物关系、人物属性、人物事件等关键信息，并严格按照示例返回一个JSON格式。\n\
      \n小说内容:\n```{chunk}```\n\n参考schema:\n```{schema}```\n\n要求：\n1. 主要人物：包括主角、配角、反派等。\n\
      2. 人物关系：包括人物之间的互动、关系等。\n3. 人物属性：包括人物的职位、称号、绰号、性别、年龄、职业、性格、特长等。\n4. 主要事件：包括策略、行动、故事背景、历史事件等。\n\
      5. 事件属性：包括事件的地点、时间、事件的参与者、事件的后果等。\n6. 实体地点等名称严格使用文本中的名称，不要自己推理和创造。\n7. 根据提供的schema为每个实体分配正确的类型。\n注意：\n严格抽取triple三元组为[实体,关系,实体]，不要多余元素。\n\
      \n示例输出，包括属性、三元组及实体类型：\n{{\n    \"attributes\": {{\n      \"PERSON#1\": [\"绰号: 智多星\"\
      , \"职位: 智囊团\", \"特长: 策划\"]\n    }},\n    \"triples\": [\n      [\"PERSON#1\"\
      , \"策划\", \"智取生辰纲\"],\n      [\"智取生辰纲\", \"发生地\", \"LOCATION#1\"]\n    ],\n    \"entity_types\": {{\n      \"PERSON#1\": \"person\",\n      \"智取生辰纲\": \"event\",\n      \"LOCATION#1\": \"location\"\n    }}\n}}\n"
    novel_agent: "你是小说信息提取专家，请根据小说内容，参考以下schema，提取小说中的主要人物、人物关系、人物属性、人物事件等关键信息，并严格按照示例返回一个JSON格式。\n\
      \n小说内容:\n```{chunk}```\n\n参考schema:\n```{schema}```\n\n要求：\n1. 主要人物：包括主角、配角、反派等。\n\
      2. 人物关系：包括人物之间的互动、关系等。\n3. 人物属性：包括人物的职位、称号、绰号、性别、年龄、职业、性格、特长等。\n4. 主要事件：包括策略、行动、故事背景、历史事件等。\n\
      5. 事件属性：包括事件的地点、时间、事件的参与者、事件的后果等。\n6. 实体地点等名称严格使用文本中的名称，不要自己推理和创造。\n7. 根据提供的schema为每个实体分配正确的类型。\n注意：\n严格抽取triple三元组为[实体,关系,实体]，不要多余元素。\n\
      \n模式演化：如果你发现新的重要实体类型、关系类型或属性类型对知识提取有价值，请将它们包含在\"new_schema_types\"字段中。注意，添加新模式的严格阈值是0.9，需要考虑重要性和与现有模式的相似性。\n\
      \n示例输出，包括属性、三元组、实体类型及新模式类型：\n{{\n    \"attributes\": {{\n      \"PERSON#1\": [\"绰号: 智多星\"\
      , \"职位: 智囊团\", \"特长: 策划\"]\n    }},\n    \"triples\": [\n      [\"PERSON#1\"\
      , \"策划\", \"智取生辰纲\"],\n      [\"智取生辰纲\", \"发生地\", \"LOCATION#1\"]\n    ],\n    \"entity_types\": {{\n      \"PERSON#1\": \"person\",\n      \"智取生辰纲\": \"event\",\n      \"LOCATION#1\": \"location\"\n    }},\n    \"new_schema_types\": {{\n      \"nodes\": [\"武器\"],\n      \"relations\": [\"使用\"],\n      \"attributes\": [\"材质\"]\n    }}\n}}\n"
    novel_eng: "You are an expert information extractor and structured data organizer.\
      \ Your task is to analyze the provided text and extract as many valuable entities,\
      \ their attributes, and relations as possible in a structured JSON format. \
      \ \n\nGuidelines:\n1. Prioritize the following predefined schema for extraction;\n\
      \   ```{schema}```\n2. Flexibility: If the context doesn't fit the predefined\
      \ schema, extract the valuable knowledge as needed;\n3. Conciseness: The Attributes\
      \ and Triples you extract should be complementary and no semantic redundancy.\n\
      4. Do NOT miss any useful information in the context;\n5. Output Format: Return\
      \ only JSON as **Example Output** with:  \n   - Attributes: Map each entity\
      \ to its descriptive features.  \n   - Triples: List relations between entities\
      \ in `[entity_mention1, relation, entity_mention2]` format.\n   - Entity_types:\
      \ Map each entity to its schema type based on the provided schema.\n\nChunk:\n\
      ```{chunk}```\n\nExample Output:  \n{{\n  \"attributes\": {{\n    \"PERSON#1\"\
      : [\"profession: writer\"]\n  }},\n  \"triples\": [\n    [\"PERSON#1\", \"\
      located at\", \"LOCATION#1\"],\n    [\"PERSON#1\", \"good at\", \"playing piano\"\
      ]\n  ],\n  \"entity_types\": {{\n    \"PERSON#1\": \"person\",\n    \"LOCATION#1\"\
      : \"location\"\n  }}\n}}\n"
    novel_eng_agent: "You are an expert information extractor and structured data organizer.\
      \ Your task is to analyze the provided text and extract as many valuable entities,\
      \ their attributes, and relations as possible in a structured JSON format. \
      \ \n\nGuidelines:\n1. Prioritize the following predefined schema for extraction;\n\
      \   ```{schema}```\n2. Flexibility: If the context doesn't fit the predefined\
      \ schema, extract the valuable knowledge as needed;\n3. Conciseness: The Attributes\
      \ and Triples you extract should be complementary and no semantic redundancy.\n\
      4. Do NOT miss any useful information in the context;\n5. Output Format: Return\
      \ only JSON as **Example Output** with:  \n   - Attributes: Map each entity\
      \ to its descriptive features.  \n   - Triples: List relations between entities\
      \ in `[entity_mention1, relation, entity_mention2]` format.\n   - Entity_types:\
      \ Map each entity to its schema type based on the provided schema.\n\n\
      Schema Evolution: If you find new and important entity types, relation types,\
      \ or attribute types that are valuable for knowledge extraction, include them\
      \ in a \"new_schema_types\" field. Notably, the strict threshold of adding\
      \ new schema considering both importance and similarity to the pattern in the\
      \ existing schema is 0.9.\n\nChunk:\n```{chunk}```\n\nExample Output:  \n{{\n\
      \  \"attributes\": {{\n    \"PERSON#1\": [\"profession: writer\"]\n  }},\n\
      \  \"triples\": [\n    [\"PERSON#1\", \"located at\", \"LOCATION#1\"],\n  \
      \  [\"PERSON#1\", \"good at\", \"playing piano\"]\n  ],\n  \"entity_types\"\
      : {{\n    \"PERSON#1\": \"person\",\n    \"LOCATION#1\": \"location\"\n  }},\n\
      \  \"new_schema_types\": {{\n      \"nodes\": [\"Instrument\"],\n      \"relations\"\
      : [\"owns\"],\n      \"attributes\": [\"skill_level\"]\n  }}\n}}\n"
  semantic_dedup:
    general: |-
      You are a knowledge graph curation assistant. All listed triples share the same head entity and relation.
      Your task is to group candidate tail descriptions that are semantically equivalent.

      Head entity: {head}
      Relation: {relation}

      Head contexts:
      {head_context}

      Candidate tails:
      {candidates}

      Instructions:
      1. Use the provided contexts when comparing meanings. Group tails that refer to the same real-world entity or express the same fact.
      2. Keep tails separate when their meanings differ or when the contexts describe different situations.
      3. Choose the most informative mention in each group as the representative index.
      4. Every input index must appear in exactly one group.

      Respond with strict JSON using this schema:
      {{
        "groups": [
          {{
            "members": [1, 3],
            "representative": 3,
            "rationale": "Why the members belong together."
          }}
        ]
      }}
    
    attribute: |-
      You are a knowledge graph curation assistant. All listed triples share the same head entity and relation.
      Your task is to identify and merge ONLY duplicate or redundant attribute values.

      Head entity: {head}
      Relation: {relation}

      Head contexts:
      {head_context}

      Candidate attribute values:
      {candidates}

      CRITICAL INSTRUCTIONS for attribute deduplication:
      1. **ONLY merge if attribute values are truly identical or redundant**:
         - Same attribute expressed differently (e.g., "高度: 10cm" and "height: 10 centimeters")
         - Complete redundancy (e.g., "颜色是红色" and "红色")
      
      2. **NEVER merge if**:
         - Attributes describe different properties (even if related to the same entity)
         - Attributes have different values or measurements
         - Attributes describe different aspects or characteristics
         - Attributes come from the same context but convey different information
      
      3. **Example - DO NOT MERGE**:
         - [1] "角度依赖性、组织依赖性、TE依赖性" (three characteristics)
         - [2] "T2弛豫时间最多可延长两倍以上" (specific measurement)
         → These are DIFFERENT attributes, keep separate!
      
      4. **Example - SHOULD MERGE**:
         - [1] "颜色: 红色"
         - [2] "color: red"
         → Same attribute, same value, can merge
      
      5. Choose the most complete and informative description as the representative.
      6. Every input index must appear in exactly one group.
      7. **When in doubt, keep them separate** - it's better to have duplicate attributes than to lose information.

      Respond with strict JSON using this schema:
      {{
        "groups": [
          {{
            "members": [1, 3],
            "representative": 3,
            "rationale": "Why these attribute values are truly identical or redundant."
          }}
        ]
      }}
  
  # Head node deduplication prompt (for entity coreference resolution)
  head_dedup:
    general: |-
      You are an expert in knowledge graph entity resolution.

      TASK: Determine if the following two entities refer to the SAME real-world object.

      Entity 1: {entity_1}
      Related knowledge about Entity 1:
      {context_1}

      Entity 2: {entity_2}
      Related knowledge about Entity 2:
      {context_2}

      CRITICAL RULES:
      1. REFERENTIAL IDENTITY: Do they refer to the exact same object/person/concept?
         - Same entity with different names → YES (e.g., "NYC" = "New York City")
         - Different but related entities → NO (e.g., "Apple Inc." ≠ "Apple Store")

      2. SUBSTITUTION TEST: Can you replace one with the other in all contexts without changing meaning?
         - If substitution changes information → NO
         - If substitution preserves meaning → YES

      3. TYPE CONSISTENCY: Check entity types/categories
         - Same name, different types → carefully verify with context
         - Example: "Apple (company)" ≠ "Apple (fruit)"

      4. CONSERVATIVE PRINCIPLE:
         - When uncertain → answer NO
         - False merge is worse than false split

      PROHIBITED MERGE REASONS (NOT valid reasons to merge):
      ✗ Similar names: "John Smith" vs "John Smith Jr." → different persons
      ✗ Related entities: "Apple Inc." vs "Apple Store" → company vs retail location
      ✗ Same category: Both are cities → might be different cities
      ✗ Shared relations: Both have similar relations → need comprehensive match
      ✗ Partial overlap: Some relations match → need ALL key relations to match
      
      DECISION PROCEDURE:
      For Entity 1 and Entity 2:
        1. Check if names are variations of the same entity (e.g., abbreviations, translations)
        2. Compare their relation patterns - do they have consistent relationships?
        3. Look for contradictions - if any key relations conflict, they are DIFFERENT
        4. Apply substitution test - can they be swapped in all contexts?
        5. If uncertain → answer NO (conservative principle)

      OUTPUT FORMAT (strict JSON):
      {{
        "is_coreferent": true/false,
        "confidence": 0.0-1.0,
        "rationale": "Clear explanation based on referential identity test"
      }}

      EXAMPLES:
      
      Example 1 - SHOULD MERGE:
      Entity 1: "UN", relations: [founded→1945, member→United States]
      Entity 2: "United Nations", relations: [established→1945, member→USA]
      → is_coreferent: true, confidence: 0.95
      → Rationale: "UN" is abbreviation of "United Nations", consistent founding year and members
      
      Example 2 - SHOULD NOT MERGE:
      Entity 1: "Apple Inc.", relations: [founded_by→Steve Jobs, produces→iPhone]
      Entity 2: "Apple Store", relations: [owned_by→Apple Inc., located_in→New York]
      → is_coreferent: false, confidence: 0.95
      → Rationale: Different entities - Apple Inc. is company, Apple Store is retail location, ownership relation indicates hierarchy
      
      Example 3 - UNCERTAIN (should answer NO):
      Entity 1: "张三", relations: [works_at→清华大学, age→45]
      Entity 2: "张三", relations: [studies_at→北京大学, age→22]
      → is_coreferent: false, confidence: 0.80
      → Rationale: Same name but different age and occupation suggest different persons (conservative principle applied)

      

    # New: Improved prompt with representative selection (LLM-driven)
    with_representative_selection: |-
      You are an expert in knowledge graph entity resolution.

      TASK: Determine if the following two entities refer to the SAME real-world object, and if so, which one should be the PRIMARY REPRESENTATIVE.

      Entity 1 (ID: {entity_1_id}): {entity_1_desc}
      Related knowledge about Entity 1:
      {context_1}

      Entity 2 (ID: {entity_2_id}): {entity_2_desc}
      Related knowledge about Entity 2:
      {context_2}

      CRITICAL RULES:

      1. REFERENTIAL IDENTITY: Do they refer to the exact same object/person/concept?
         - Same entity with different names → YES (e.g., "NYC" = "New York City")
         - Different but related entities → NO (e.g., "Apple Inc." ≠ "Apple Store")

      2. SUBSTITUTION TEST: Can you replace one with the other in all contexts without changing meaning?
         - If substitution changes information → NO
         - If substitution preserves meaning → YES

      3. TYPE CONSISTENCY: Check entity types/categories
         - Same name, different types → carefully verify with context
         - Example: "Apple (company)" ≠ "Apple (fruit)"

      4. CONSERVATIVE PRINCIPLE:
         - When uncertain about coreference → answer NO
         - When uncertain about representative → choose the one with MORE relationships
         - False merge is worse than false split

      PROHIBITED MERGE REASONS (NOT valid reasons to merge):
      ✗ Similar names: "John Smith" vs "John Smith Jr." → different persons
      ✗ Related entities: "Apple Inc." vs "Apple Store" → company vs retail location
      ✗ Same category: Both are cities → might be different cities
      ✗ Shared relations: Both have similar relations → need comprehensive match
      ✗ Partial overlap: Some relations match → need ALL key relations to match
      
      CONTEXT USAGE GUIDANCE (MANDATORY):
      
      The provided context (relationships and source text) is additional information to help you determine coreference. Use it wisely:
      
      1. **Identify contradictions**: If the contexts reveal contradictory information about the two entities, they are DIFFERENT entities → answer NO
      
      2. **Find supporting evidence**: If the contexts show consistent and complementary information, it supports the coreference hypothesis
      
      3. **Assess information sufficiency**: If context is too limited to make a confident decision, apply CONSERVATIVE PRINCIPLE → answer NO
      
      4. **Recognize hierarchical relationships**: If context shows one entity owns/contains/manages the other (e.g., company vs subsidiary, organization vs department), they are DIFFERENT entities → answer NO
      
      IMPORTANT: Do NOT verify the context itself - trust it and USE it to make better coreference decisions.
      
      DECISION PROCEDURE:
      For Entity 1 and Entity 2, follow these steps IN ORDER:
        
        PHASE 1: USE CONTEXT TO INFORM COREFERENCE DECISION
          → Use the provided context (relationships and source text) to identify contradictions or supporting evidence
          → If context reveals contradictions or hierarchical relationships → they are DIFFERENT
          → If context is insufficient → apply conservative principle
        
        PHASE 2: COREFERENCE DETERMINATION
          Step 1: Check if names are variations of the same entity (e.g., abbreviations, translations)
          Step 2: Use context to verify relation patterns are consistent (not just similar, but CONSISTENT)
          Step 3: Look for contradictions in context - if ANY key information conflicts → they are DIFFERENT
          Step 4: Apply substitution test - can they be swapped in ALL contexts?
          Step 5: If uncertain → answer NO (conservative principle)
        
        PHASE 3: REPRESENTATIVE SELECTION (only if coreferent)
          Choose PRIMARY REPRESENTATIVE based on:
            a) **Formality and Completeness**: Full name > Abbreviation, BUT domain conventions matter
            b) **Domain Convention**: Medical/Academic prefer standard terms, Popular prefers common forms
            c) **Information Richness**: Entity with MORE relationships (visible in context above)
            d) **Naming Quality**: Official name > Colloquial, Standard spelling > Variant
            e) **Cultural Context**: Consider primary language and widely recognized forms

      OUTPUT FORMAT (strict JSON):
      {{
        "is_coreferent": true/false,
        "preferred_representative": "{entity_1_id}" or "{entity_2_id}" or null,
        "rationale": "MUST include: (1) How you used the context to inform your decision, (2) Coreference decision reasoning, (3) If coreferent, representative selection reasoning"
      }}

      IMPORTANT:
      - Set "preferred_representative" ONLY if "is_coreferent" is true
      - If "is_coreferent" is false, set "preferred_representative" to null
      - The "preferred_representative" must be exactly "{entity_1_id}" or "{entity_2_id}"

      EXAMPLES (demonstrating context usage):
      
      Example 1 - SHOULD MERGE (context supports with consistent information):
      Entity 1 (entity_100): "UN", relations: [founded→1945, member→United States, headquarters→New York]
      Entity 2 (entity_150): "United Nations", relations: [established→1945, member→USA]
      → is_coreferent: true
      → preferred_representative: "entity_100"
      → Rationale: "(Context Usage) The contexts show consistent information: founding year 1945 matches (founded/established are equivalent), members align (United States/USA refer to same country). No contradictions found. The contexts support that these are the same entity with different name forms. (Coreference) 'UN' is standard abbreviation of 'United Nations', all relationships consistent, pass substitution test. (Representative) Choose entity_100 (UN) because: (1) more relationships (3 vs 2), (2) 'UN' is the widely recognized standard form in international relations context."
      
      Example 2 - SHOULD NOT MERGE (context reveals hierarchical relationship):
      Entity 1 (entity_200): "Apple Inc.", relations: [founded_by→Steve Jobs, produces→iPhone, type→公司]
      Entity 2 (entity_250): "Apple Store", relations: [owned_by→Apple Inc., located_in→New York, type→零售店]
      → is_coreferent: false
      → preferred_representative: null
      → Rationale: "(Context Usage) Context reveals hierarchical relationship: 'owned_by→Apple Inc.' shows Apple Store is owned by Apple Inc., not the same entity. Type information confirms: 公司 (company) vs 零售店 (retail store) are different categories. This is owner-owned relationship, not identity. (Coreference Decision) Different entities. Substitution test fails - cannot replace 'Apple Inc. founded by Steve Jobs' with 'Apple Store' context."
      
      Example 3 - SHOULD NOT MERGE (context shows contradictions):
      Entity 1 (entity_300): "张三", relations: [works_at→清华大学, age→45, position→教授]
      Entity 2 (entity_350): "张三", relations: [studies_at→北京大学, age→22, status→学生]
      → is_coreferent: false
      → preferred_representative: null
      → Rationale: "(Context Usage) Contexts reveal critical contradictions: age differs (45 vs 22), one is professor at Tsinghua while other is student at Peking University. These contradictions indicate different persons with the same name. (Coreference Decision) Same name but contradictory context information. Conservative principle applied: answer NO."
      
      Example 4 - SHOULD MERGE (context shows complementary information):
      Entity 1 (entity_400): "北京", relations: [population→21M]
      Entity 2 (entity_450): "北京市", relations: [capital_of→中国, area→16410km², governor→..., founded→1045]
      → is_coreferent: true
      → preferred_representative: "entity_450"
      → Rationale: "(Context Usage) Contexts show complementary information about the same geographic location: population data and administrative details are consistent with being the same city. No contradictions found. '北京' is colloquial form, '北京市' is official administrative name. (Coreference) Pass substitution test. (Representative) Choose entity_450 (北京市): (1) official administrative name, (2) significantly more comprehensive relationships (4 vs 1)."
      
      Example 5 - INSUFFICIENT CONTEXT (conservative principle):
      Entity 1 (entity_500): "李明", relations: [age→30]
      Entity 2 (entity_550): "李明", relations: [gender→male]
      → is_coreferent: false
      → preferred_representative: null
      → Rationale: "(Context Usage) Contexts provide very limited information: only generic attributes (age 30, gender male) that many people share. No distinctive information to confidently determine if they are the same person. (Coreference Decision) Insufficient context for reliable decision. Conservative principle: answer NO to avoid false merge."


  
  decomposition:
    general: "You are a professional question decomposition expert specializing in\
      \ multi-hop reasoning.\nGiven the following ontology and the question, decompose\
      \ the complex question into 2-3 focused sub-questions and identify involved schema types.\n\nCRITICAL REQUIREMENTS:\n\
      1. Each sub-question must be:\n   - Specific and focused on a single fact or\
      \ relationship by identifing all entities, relationships, and reasoning steps\
      \ needed\n   - Answerable independently with the given ontology\n   - Explicitly\
      \ reference entities and relations from the original question\n   - Designed\
      \ to retrieve relevant knowledge for the final answer\n\n2. For simple questions\
      \ (1-2 hop), return the original question as a single sub-question\n3. Analyze the question and identify all schema types that might be involved\n4. Return\
      \ a JSON object with sub_questions array and involved_types object.\n\nOntology:\n{ontology}\n\n\
      Question: {question}\n\nExample for complex question:\nOriginal: \"Which film\
      \ has the director died earlier, Ethnic Notions or Gordon Of Ghost City?\"\n\
      Output:\n{{\n  \"sub_questions\": [\n    {{\"sub-question\": \"Who is the director of Ethnic Notions?\"\
      }},\n    {{\"sub-question\": \"Who is the director of Gordon Of Ghost City?\"\
      }},\n    {{\"sub-question\": \"When did the director of Ethnic Notions die?\"\
      }},\n    {{\"sub-question\": \"When did the director of Gordon Of Ghost City\
      \ die?\"}}\n  ],\n  \"involved_types\": {{\n    \"nodes\": [\"creative_work\", \"person\"],\n    \"relations\": [\"directed_by\"],\n    \"attributes\": [\"name\", \"date\"]\n  }}\n}}\n\nExample for simple question:\nOriginal: \"What is the capital\
      \ of France?\"\nOutput:\n{{\n  \"sub_questions\": [\n    {{\"sub-question\": \"What is the capital\
      \ of France?\"}}\n  ],\n  \"involved_types\": {{\n    \"nodes\": [\"location\"],\n    \"relations\": [\"located_in\"],\n    \"attributes\": [\"name\"]\n  }}\n}}\n"
    novel: "你是一个专业的问题分解大师，请根据以下问题和图本体模式，将问题分解为2-3个子问题，并识别涉及的schema类型。\n   要求：\n   1. 每个子问题必须：\n \
      \     - 明确且专注于一个事实或关系，通过识别所有实体、关系和推理步骤\n      - 明确引用原始问题中的实体和关系\n      - 设计为检索最终答案所需的相关知识\n\
      \   2. 对于简单问题（1-2跳），返回原始问题作为单个子问题\n   3. 分析问题并识别所有可能涉及的schema类型\n   4. 返回一个JSON对象，包含sub_questions数组和involved_types对象。\n   \n   问题：{question}\n\
      \   \n   图本体模式：{ontology}\n   \n   示例：\n   原始问题：\\\"智取生辰纲事件中，PERSON#1的策略为什么能够成功\\\"\n\
      \   输出：\n   {{\n       \\\"sub_questions\\\": [\n           {{\\\"sub-question\\\": \\\"智取生辰纲中PERSON#1的策略是什么？\\\"}},\n  \
      \           {{\\\"sub-question\\\": \\\"智取生辰纲中的PERSON、LOCATION有什么特殊属性？\\\"}}\n       ],\n       \\\"involved_types\\\": {{\n           \\\"nodes\\\": [\\\"person\\\", \\\"event\\\", \\\"location\\\"],\n           \\\"relations\\\": [\\\"策划\\\", \\\"发生地\\\"],\n           \\\"attributes\\\": [\\\"绰号\\\", \\\"职位\\\", \\\"特长\\\"]\n       }}\n   }}\n   如果是简单问题，返回原始问题作为单个子问题。\n\
      \   原始问题：\\\"智取生辰纲事件中，PERSON#1是谁\\\"\n   输出：\n   {{\n       \\\"sub_questions\\\": [\n           {{\\\"sub-question\\\": \\\"\
      智取生辰纲事件中，PERSON#1是谁？\\\"}}\n       ],\n       \\\"involved_types\\\": {{\n           \\\"nodes\\\": [\\\"person\\\", \\\"event\\\"],\n           \\\"relations\\\": [\\\"参与\\\"],\n           \\\"attributes\\\": [\\\"name\\\"]\n       }}\n   }}\n"
  retrieval:
    general: 'You are an expert knowledge assistant. Your task is to answer the question
      based on the provided knowledge context.


      1. Use ONLY the information from the provided knowledge context and try your
      best to answer the question.

      2. If the knowledge is insufficient, reject to answer the question.

      3. Be precise and concise in your answer

      4. For factual questions, provide the specific fact or entity name

      5. For temporal questions, provide the specific date, year, or time period


      Question: {question}


      Knowledge Context:

      {context}


      Answer (be specific and direct):

      '
    ircot: 'You are an expert knowledge assistant using iterative retrieval with chain-of-thought
      reasoning.


      Current Question: {current_query}


      Available Knowledge Context:

      {context}


      Previous Thoughts: {previous_thoughts}


      Step {step}: Please think step by step about what additional information you
      need to answer the question completely and accurately.


      Instructions:

      1. Analyze the current knowledge context and the question

      2. Think about what information might be missing or unclear

      3. If you have enough information to answer, in the end of your response, write
      "So the answer is:" followed by your final answer

      4. If you need more information, in the end of your response, write a specific
      query begin with "The new query is:" to retrieve additional relevant information

      5. Be specific and focused in your reasoning


      Your reasoning:

      '
    novel: '你是小说知识助手，你的任务是根据提供的小说知识库回答问题。

      1. 如果知识库中的信息不足以回答问题，请根据你的推理和知识回答。

      2. 回答要简洁明了。

      3. 对于事实性问题，提供具体的事实或人物名称。

      4. 对于时间性问题，提供具体的时间、年份或时间段。

      问题：{question}

      相关知识：{context}

      答案（简洁明了）：

      '
    novel_eng: "You are a novel knowledge assistant. Your task is to answer the question\
      \ based on the provided novel knowledge context.\n1. If the knowledge is insufficient,\
      \ answer the question based on your own knowledge.\n2. Be precise and concise\
      \ in your answer.\n3. For factual questions, provide the specific fact or entity\
      \ name\n4. For temporal questions, provide the specific date, year, or time\
      \ period\n\nQuestion: {question}\n\nKnowledge Context:\n{context}   \n\nAnswer\
      \ (be specific and direct):\n"
retrieval:
  agent:
    enable_ircot: true
    enable_parallel_subquestions: true
    max_steps: 5
  cache_dir: retriever/faiss_cache_new
  enable_caching: true
  enable_high_recall: true
  enable_query_enhancement: true
  enable_reranking: true
  faiss:
    device: cpu
    max_workers: 4
    search_k: 50
  recall_paths: 2
  similarity_threshold: 0.3
  top_k: 20
  top_k_filter: 20
triggers:
  constructor_trigger: true
  mode: agent
  retrieve_trigger: true
