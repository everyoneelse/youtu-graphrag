# Tailå»é‡å·¥å…·ä¸Youtu-GraphRAGå¯¹æ¥æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©ä½ å°† Youtu-GraphRAG ç”Ÿæˆçš„ `graph.json` ä¸ tail å»é‡ç»“æœå¯¹æ¥ï¼Œå®ç°å®Œæ•´çš„å»é‡æµç¨‹ã€‚

## ğŸ”„ å®Œæ•´å·¥ä½œæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŸå§‹æ–‡æ¡£/æ•°æ®   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Youtu-GraphRAG  â”‚ â† æ„å»ºåˆå§‹çŸ¥è¯†å›¾è°±
â”‚  æ„å»ºçŸ¥è¯†å›¾è°±    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   graph.json    â”‚ â† åŸå§‹å›¾è°±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ semantic_dedup_ â”‚ â† ä½ çš„å»é‡å¤„ç†
â”‚     group       â”‚    (è¯†åˆ«é‡å¤çš„tailèŠ‚ç‚¹)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dedup_results   â”‚ â† ä½ å¤„ç†å¾—åˆ°çš„å»é‡ç»“æœ
â”‚     .json       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ ¼å¼è½¬æ¢è„šæœ¬     â”‚ â† convert_dedup_format.py
â”‚ (å¦‚æœéœ€è¦)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚apply_tail_dedup â”‚ â† åº”ç”¨å»é‡ç»“æœåˆ°å›¾è°±
â”‚   _results.py   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚deduped_graph.jsonâ”‚ â† æœ€ç»ˆå»é‡åçš„å›¾è°±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ ä½ çš„æ•°æ®æ ¼å¼

### 1. graph.json æ ¼å¼

Youtu-GraphRAG ç”Ÿæˆçš„ `graph.json` æ ¼å¼å¦‚ä¸‹ï¼š

```json
[
  {
    "start_node": {
      "label": "entity",
      "properties": {
        "name": "é­”è§’æ•ˆåº”",
        "chunk id": "Dwjxk2M8",
        "schema_type": "MRIä¼ªå½±"
      }
    },
    "relation": "è§£å†³æ–¹æ¡ˆä¸º",
    "end_node": {
      "label": "entity",
      "properties": {
        "name": "å»¶é•¿TEå€¼",
        "chunk id": "Dwjxk2M8"
      }
    }
  }
]
```

### 2. ä½ çš„ dedup_results æ ¼å¼

ä½ é€šè¿‡ `semantic_dedup_group` å¤„ç†åå¾—åˆ°çš„æ ¼å¼ï¼š

```json
[
  {
    "head_node": {
      "label": "entity",
      "properties": {
        "name": "é­”è§’æ•ˆåº”",
        "chunk id": "Dwjxk2M8",
        "schema_type": "MRIä¼ªå½±"
      }
    },
    "relation": "è§£å†³æ–¹æ¡ˆä¸º",
    "tail_nodes_to_dedup": [
      "å»¶é•¿TEå€¼ (chunk id: Dwjxk2M8) [entity]",
      "æ”¹å˜æ‰«æä½“ä½ (chunk id: Dwjxk2M8) [entity]",
      "å¢åŠ TEå€¼ (chunk id: PHuCr1nf) [entity]",
      "å»¶é•¿TE (chunk id: IwfMagF6) [entity]",
      "æ”¹å˜ä½“ä½ (chunk id: IwfMagF6) [entity]"
    ],
    "dedup_results": {
      "cluster_0": {
        "member": [
          "å¢åŠ TEå€¼ (chunk id: PHuCr1nf) [entity]",
          "å»¶é•¿TE (chunk id: IwfMagF6) [entity]",
          "å»¶é•¿TEå€¼ (chunk id: Dwjxk2M8) [entity]"
        ],
        "llm_judge_reason": "ä¸‰æ¡å°¾å·´å‡æŒ‡å»¶é•¿å›æ³¢æ—¶é—´..."
      },
      "cluster_1": {
        "member": [
          "æ”¹å˜ä½“ä½ (chunk id: IwfMagF6) [entity]",
          "æ”¹å˜æ‰«æä½“ä½ (chunk id: Dwjxk2M8) [entity]"
        ],
        "llm_judge_reason": "æ”¹å˜æ‰«æä½“ä½ä¸æ”¹å˜ä½“ä½æŒ‡ä»£åŒä¸€æ“ä½œ..."
      }
    },
    "deduped_tails": [
      "æ”¹å˜æ‰«æä½“ä½ (chunk id: Dwjxk2M8) [entity]",
      "å»¶é•¿TEå€¼ (chunk id: Dwjxk2M8) [entity]"
    ]
  }
]
```

## âœ… ä½ çš„æ ¼å¼å·²ç»å…¼å®¹ï¼

**å¥½æ¶ˆæ¯**ï¼šä½ çš„ `dedup_results` æ ¼å¼**å®Œå…¨ç¬¦åˆ**æˆ‘çš„å·¥å…·è¦æ±‚ï¼å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ3æ­¥å®Œæˆï¼‰

### æ­¥éª¤1ï¼šå‡†å¤‡æ–‡ä»¶

ç¡®ä¿ä½ æœ‰ä»¥ä¸‹æ–‡ä»¶ï¼š

```bash
# ä½ çš„æ–‡ä»¶
output/graphs/original_graph.json     # Youtu-GraphRAGç”Ÿæˆçš„å›¾è°±
your_dedup_results.json               # ä½ çš„å»é‡ç»“æœ

# å·¥å…·è„šæœ¬ï¼ˆå·²åˆ›å»ºï¼‰
apply_tail_dedup_results.py           # å»é‡åº”ç”¨è„šæœ¬
```

### æ­¥éª¤2ï¼šè¿è¡Œå»é‡åº”ç”¨

```bash
python3 apply_tail_dedup_results.py \
    --graph output/graphs/original_graph.json \
    --dedup-results your_dedup_results.json \
    --output output/graphs/deduped_graph.json
```

### æ­¥éª¤3ï¼šéªŒè¯ç»“æœ

```bash
# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯ï¼ˆåœ¨æ­¥éª¤2çš„è¾“å‡ºä¸­ï¼‰
# æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
ls -lh output/graphs/deduped_graph.json
```

## ğŸ“ è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šå®Œæ•´çš„Pythonè„šæœ¬

```python
#!/usr/bin/env python3
"""
å®Œæ•´çš„å»é‡æµç¨‹ç¤ºä¾‹
"""
import json
from pathlib import Path
from apply_tail_dedup_results import TailDedupApplicator
from utils import graph_processor

# 1. åŠ è½½åŸå§‹å›¾è°±
print("1. åŠ è½½åŸå§‹å›¾è°±...")
graph = graph_processor.load_graph_from_json('output/graphs/original_graph.json')
print(f"   åŸå§‹å›¾: {graph.number_of_nodes()} èŠ‚ç‚¹, {graph.number_of_edges()} è¾¹")

# 2. åŠ è½½å»é‡ç»“æœ
print("\n2. åŠ è½½å»é‡ç»“æœ...")
with open('your_dedup_results.json', 'r', encoding='utf-8') as f:
    dedup_results = json.load(f)
print(f"   å»é‡ç»„æ•°: {len(dedup_results)}")

# ç»Ÿè®¡clusterä¿¡æ¯
total_clusters = sum(len(g['dedup_results']) for g in dedup_results)
total_members = sum(
    len(cluster['member']) 
    for g in dedup_results 
    for cluster in g['dedup_results'].values()
)
print(f"   æ€»clusteræ•°: {total_clusters}")
print(f"   æ€»æˆå‘˜æ•°: {total_members}")

# 3. åº”ç”¨å»é‡
print("\n3. åº”ç”¨å»é‡...")
applicator = TailDedupApplicator(graph)
stats = applicator.apply_all(dedup_results)

# 4. ä¿å­˜ç»“æœ
print("\n4. ä¿å­˜ç»“æœ...")
output_path = 'output/graphs/deduped_graph.json'
Path(output_path).parent.mkdir(parents=True, exist_ok=True)
graph_processor.save_graph_to_json(graph, output_path)

# 5. æ‰“å°ç»Ÿè®¡
print("\n" + "="*60)
print("å»é‡ç»Ÿè®¡:")
print("="*60)
for key, value in stats.items():
    print(f"  {key}: {value}")
print(f"\n  æœ€ç»ˆå›¾: {graph.number_of_nodes()} èŠ‚ç‚¹, {graph.number_of_edges()} è¾¹")
print("="*60)
print(f"\nâœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
```

### ç¤ºä¾‹2ï¼šéªŒè¯å»é‡ç»“æœ

```python
#!/usr/bin/env python3
"""
éªŒè¯å»é‡ç»“æœçš„è„šæœ¬
"""
import json
from utils import graph_processor

def verify_dedup(original_path, deduped_path, dedup_results_path):
    """éªŒè¯å»é‡æ˜¯å¦æ­£ç¡®åº”ç”¨"""
    
    # åŠ è½½å›¾è°±
    original = graph_processor.load_graph_from_json(original_path)
    deduped = graph_processor.load_graph_from_json(deduped_path)
    
    # åŠ è½½å»é‡ç»“æœ
    with open(dedup_results_path, 'r', encoding='utf-8') as f:
        dedup_results = json.load(f)
    
    print("="*60)
    print("å»é‡éªŒè¯æŠ¥å‘Š")
    print("="*60)
    
    # 1. å›¾å¤§å°å¯¹æ¯”
    print("\n1. å›¾å¤§å°å˜åŒ–:")
    print(f"   èŠ‚ç‚¹: {original.number_of_nodes()} â†’ {deduped.number_of_nodes()}")
    print(f"   è¾¹:   {original.number_of_edges()} â†’ {deduped.number_of_edges()}")
    
    # 2. æ£€æŸ¥ä»£è¡¨èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
    print("\n2. æ£€æŸ¥ä»£è¡¨èŠ‚ç‚¹:")
    representatives = []
    for group in dedup_results:
        for cluster_name, cluster_data in group['dedup_results'].items():
            members = cluster_data['member']
            rep = members[-1]  # æœ€åä¸€ä¸ªæ˜¯ä»£è¡¨
            representatives.append(rep)
    
    found_count = 0
    for rep in representatives:
        # è§£æèŠ‚ç‚¹æ ‡è¯†ç¬¦
        parts = rep.split(' (chunk id: ')
        if len(parts) == 2:
            name = parts[0]
            chunk_label = parts[1].split(') [')
            chunk_id = chunk_label[0]
            label = chunk_label[1].rstrip(']')
            
            # åœ¨å»é‡åçš„å›¾ä¸­æŸ¥æ‰¾
            found = False
            for node_id, data in deduped.nodes(data=True):
                props = data.get('properties', {})
                if (props.get('name') == name and 
                    props.get('chunk id') == chunk_id and
                    data.get('label') == label):
                    found = True
                    found_count += 1
                    break
            
            if found:
                print(f"   âœ… {name} (chunk: {chunk_id})")
            else:
                print(f"   âŒ {name} (chunk: {chunk_id}) - æœªæ‰¾åˆ°ï¼")
    
    print(f"\n   ä»£è¡¨èŠ‚ç‚¹å­˜åœ¨ç‡: {found_count}/{len(representatives)} ({found_count/len(representatives)*100:.1f}%)")
    
    # 3. æ£€æŸ¥æ˜¯å¦æœ‰åº”è¯¥è¢«åˆ é™¤çš„èŠ‚ç‚¹
    print("\n3. æ£€æŸ¥è¢«åˆ é™¤çš„èŠ‚ç‚¹:")
    should_remove = []
    for group in dedup_results:
        for cluster_name, cluster_data in group['dedup_results'].items():
            members = cluster_data['member']
            rep = members[-1]
            # é™¤äº†ä»£è¡¨ï¼Œå…¶ä»–éƒ½åº”è¯¥è¢«åˆ é™¤
            for member in members[:-1]:
                should_remove.append(member)
    
    still_exists = 0
    for member in should_remove:
        parts = member.split(' (chunk id: ')
        if len(parts) == 2:
            name = parts[0]
            chunk_label = parts[1].split(') [')
            chunk_id = chunk_label[0]
            label = chunk_label[1].rstrip(']')
            
            # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨å›¾ä¸­
            for node_id, data in deduped.nodes(data=True):
                props = data.get('properties', {})
                if (props.get('name') == name and 
                    props.get('chunk id') == chunk_id and
                    data.get('label') == label):
                    still_exists += 1
                    print(f"   âš ï¸  {name} (chunk: {chunk_id}) - åº”è¯¥åˆ é™¤ä½†ä»å­˜åœ¨")
                    break
    
    removed_count = len(should_remove) - still_exists
    print(f"\n   åº”åˆ é™¤èŠ‚ç‚¹: {len(should_remove)}")
    print(f"   å·²åˆ é™¤: {removed_count}")
    print(f"   åˆ é™¤ç‡: {removed_count/len(should_remove)*100:.1f}%")
    
    # 4. æ€»ç»“
    print("\n" + "="*60)
    if found_count == len(representatives) and still_exists == 0:
        print("âœ… éªŒè¯é€šè¿‡ï¼å»é‡æ­£ç¡®åº”ç”¨ã€‚")
    else:
        print("âš ï¸  éªŒè¯å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°è¯¦æƒ…ã€‚")
    print("="*60)

# ä½¿ç”¨ç¤ºä¾‹
verify_dedup(
    'output/graphs/original_graph.json',
    'output/graphs/deduped_graph.json',
    'your_dedup_results.json'
)
```

## ğŸ”§ å¸¸è§åœºæ™¯å¤„ç†

### åœºæ™¯1ï¼šä½ çš„ dedup_results æ ¼å¼ç•¥æœ‰ä¸åŒ

å¦‚æœä½ çš„æ ¼å¼ç¨æœ‰ä¸åŒï¼Œå¯ä»¥ä½¿ç”¨è½¬æ¢è„šæœ¬ï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰ã€‚

### åœºæ™¯2ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªå›¾è°±

```python
#!/usr/bin/env python3
"""æ‰¹é‡å¤„ç†å¤šä¸ªå›¾è°±"""
import glob
import json
from pathlib import Path
from apply_tail_dedup_results import TailDedupApplicator
from utils import graph_processor

# å»é‡ç»“æœï¼ˆå‡è®¾æ‰€æœ‰å›¾è°±ä½¿ç”¨ç›¸åŒçš„å»é‡è§„åˆ™ï¼‰
with open('common_dedup_results.json', 'r', encoding='utf-8') as f:
    dedup_results = json.load(f)

# æ‰¹é‡å¤„ç†
for graph_file in glob.glob('output/graphs/*.json'):
    if 'deduped' in graph_file:
        continue  # è·³è¿‡å·²å¤„ç†çš„
    
    print(f"\nå¤„ç†: {graph_file}")
    
    # åŠ è½½å¹¶å»é‡
    graph = graph_processor.load_graph_from_json(graph_file)
    applicator = TailDedupApplicator(graph)
    stats = applicator.apply_all(dedup_results)
    
    # ä¿å­˜
    output_file = graph_file.replace('.json', '_deduped.json')
    graph_processor.save_graph_to_json(graph, output_file)
    
    print(f"  å®Œæˆ: {stats['edges_updated']} è¾¹æ›´æ–°")
```

### åœºæ™¯3ï¼šåªå¤„ç†ç‰¹å®šç±»å‹çš„èŠ‚ç‚¹

```python
#!/usr/bin/env python3
"""åªå¯¹entityç±»å‹çš„èŠ‚ç‚¹å»é‡"""
import json
from apply_tail_dedup_results import TailDedupApplicator
from utils import graph_processor

# è¿‡æ»¤å»é‡ç»“æœï¼Œåªä¿ç•™entityç±»å‹
with open('all_dedup_results.json', 'r', encoding='utf-8') as f:
    all_results = json.load(f)

# åªä¿ç•™entityèŠ‚ç‚¹çš„å»é‡
entity_results = []
for group in all_results:
    # æ£€æŸ¥æ˜¯å¦æ˜¯entityç±»å‹
    if group['head_node']['label'] == 'entity':
        # è¿›ä¸€æ­¥æ£€æŸ¥clusteræˆå‘˜
        filtered_clusters = {}
        for cluster_name, cluster_data in group['dedup_results'].items():
            # æ£€æŸ¥æˆå‘˜æ˜¯å¦éƒ½æ˜¯entity
            if all('[entity]' in member for member in cluster_data['member']):
                filtered_clusters[cluster_name] = cluster_data
        
        if filtered_clusters:
            group_copy = group.copy()
            group_copy['dedup_results'] = filtered_clusters
            entity_results.append(group_copy)

print(f"è¿‡æ»¤å: {len(entity_results)} ç»„å»é‡è§„åˆ™ï¼ˆåªåŒ…å«entityï¼‰")

# åº”ç”¨å»é‡
graph = graph_processor.load_graph_from_json('output/graphs/original_graph.json')
applicator = TailDedupApplicator(graph)
stats = applicator.apply_all(entity_results)

graph_processor.save_graph_to_json(graph, 'output/graphs/entity_deduped.json')
```

## ğŸ”„ æ ¼å¼è½¬æ¢ï¼ˆå¦‚æœéœ€è¦ï¼‰

å¦‚æœä½ çš„æ ¼å¼éœ€è¦è°ƒæ•´ï¼Œå¯ä»¥åˆ›å»ºè½¬æ¢è„šæœ¬ã€‚æˆ‘ä¼šåœ¨ä¸‹é¢æä¾›é€šç”¨çš„è½¬æ¢å·¥å…·ã€‚

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. èŠ‚ç‚¹æ ‡è¯†ç¬¦æ ¼å¼

ç¡®ä¿ä½ çš„èŠ‚ç‚¹æ ‡è¯†ç¬¦æ ¼å¼ä¸ºï¼š
```
"name (chunk id: xxx) [label]"
```

ä¾‹å¦‚ï¼š
```
"å»¶é•¿TEå€¼ (chunk id: Dwjxk2M8) [entity]"
```

### 2. ä»£è¡¨èŠ‚ç‚¹å¿…é¡»å­˜åœ¨

- clusterçš„æœ€åä¸€ä¸ªæˆå‘˜ä¼šè¢«ç”¨ä½œä»£è¡¨
- ç¡®ä¿è¿™ä¸ªä»£è¡¨èŠ‚ç‚¹åœ¨åŸå›¾ä¸­å­˜åœ¨
- å¦‚æœä¸å­˜åœ¨ï¼Œä¼šè¾“å‡ºè­¦å‘Šä½†ç»§ç»­å¤„ç†

### 3. å¤‡ä»½åŸå§‹æ–‡ä»¶

```bash
# å»ºè®®å…ˆå¤‡ä»½
cp output/graphs/original_graph.json output/graphs/original_graph.json.backup
```

### 4. æ£€æŸ¥æ—¥å¿—è¾“å‡º

å·¥å…·ä¼šè¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼ŒåŒ…æ‹¬ï¼š
- è­¦å‘Šä¿¡æ¯ï¼ˆå¦‚æ‰¾ä¸åˆ°ä»£è¡¨èŠ‚ç‚¹ï¼‰
- å¤„ç†è¿›åº¦
- ç»Ÿè®¡ä¿¡æ¯

## ğŸ“Š é¢„æœŸç»“æœ

è¿è¡Œåä½ ä¼šçœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š

```
[INFO] Loading graph from output/graphs/original_graph.json
[INFO] Original graph: 200 nodes, 350 edges
[INFO] Loading deduplication results from your_dedup_results.json
[INFO] Loaded 10 deduplication groups
[INFO] Building node mapping from deduplication results...
[INFO] Built mapping with 10 clusters and 35 total members
[INFO] Mapping entries: 35
[INFO] Applying deduplication to edges...
[INFO] Updated 25 edges
[INFO] Applying deduplication to communities...
[INFO] Updated 5 communities
[INFO] Deduplicated 8 community members
[INFO] Applying deduplication to keyword_filter_by relations...
[INFO] Updated 2 keyword_filter_by relations
[INFO] Removed 7 isolated nodes
============================================================
Deduplication Statistics:
  Total clusters processed: 10
  Total members in clusters: 35
  Edges updated: 25
  Communities updated: 5
  Community members deduplicated: 8
  Keyword_filter_by relations updated: 2
  Isolated nodes removed: 7
  Graph size: 200 â†’ 193 nodes (7 removed)
  Graph edges: 350 â†’ 325 edges (25 removed)
============================================================
[INFO] Deduplicated graph saved to output/graphs/deduped_graph.json
```

## ğŸ› é—®é¢˜æ’æŸ¥

### é—®é¢˜1ï¼šæ‰¾ä¸åˆ°æ¨¡å—

```bash
# ç¡®ä¿åœ¨workspaceç›®å½•ä¸‹è¿è¡Œ
cd /workspace

# æ£€æŸ¥Pythonè·¯å¾„
python3 -c "import sys; print('\n'.join(sys.path))"
```

### é—®é¢˜2ï¼šæ ¼å¼é”™è¯¯

```bash
# éªŒè¯JSONæ ¼å¼
python3 -c "import json; json.load(open('your_dedup_results.json'))"
```

### é—®é¢˜3ï¼šæ²¡æœ‰æ•ˆæœï¼ˆ0æ›´æ–°ï¼‰

å¯èƒ½åŸå› ï¼š
- èŠ‚ç‚¹æ ‡è¯†ç¬¦æ ¼å¼ä¸åŒ¹é…
- chunk id ä¸ä¸€è‡´
- labelåç§°ä¸åŒ¹é…

è§£å†³ï¼šè¿è¡ŒéªŒè¯è„šæœ¬æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `APPLY_TAIL_DEDUP_README.md` - å¿«é€Ÿå…¥é—¨
- `TAIL_DEDUP_APPLICATION_GUIDE.md` - è¯¦ç»†æŒ‡å—
- `TAIL_DEDUP_DETAILED_EXPLANATION.md` - æŠ€æœ¯ç»†èŠ‚
- `visualize_tail_dedup_process.py` - å¯è§†åŒ–æ¼”ç¤º

## ğŸ¯ æ€»ç»“

ä½ çš„æ•°æ®æ ¼å¼å·²ç»å…¼å®¹ï¼Œåªéœ€è¦ï¼š

1. âœ… ç¡®ä¿æœ‰ `graph.json`ï¼ˆYoutu-GraphRAGè¾“å‡ºï¼‰
2. âœ… ç¡®ä¿æœ‰ `dedup_results.json`ï¼ˆä½ çš„å»é‡ç»“æœï¼‰
3. âœ… è¿è¡Œ `apply_tail_dedup_results.py`
4. âœ… æ£€æŸ¥è¾“å‡ºå’Œç»Ÿè®¡ä¿¡æ¯

å°±è¿™ä¹ˆç®€å•ï¼ğŸ‰
