# HeadèŠ‚ç‚¹å»é‡æ–¹æ¡ˆè®¾è®¡æ–‡æ¡£

**æ—¥æœŸ**: 2025-10-27  
**ç‰ˆæœ¬**: v1.0  
**ä½œè€…**: çŸ¥è¯†å›¾è°±æ¶æ„å¸ˆ

---

## ğŸ“‹ ç›®å½•

1. [èƒŒæ™¯ä¸ç°çŠ¶åˆ†æ](#èƒŒæ™¯ä¸ç°çŠ¶åˆ†æ)
2. [é—®é¢˜å®šä¹‰](#é—®é¢˜å®šä¹‰)
3. [æ–¹æ¡ˆè®¾è®¡åŸåˆ™](#æ–¹æ¡ˆè®¾è®¡åŸåˆ™)
4. [æŠ€æœ¯æ–¹æ¡ˆ](#æŠ€æœ¯æ–¹æ¡ˆ)
5. [å®ç°ç»†èŠ‚](#å®ç°ç»†èŠ‚)
6. [æ•°æ®ä¸€è‡´æ€§ä¿éšœ](#æ•°æ®ä¸€è‡´æ€§ä¿éšœ)
7. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
8. [é£é™©è¯„ä¼°ä¸ç¼“è§£](#é£é™©è¯„ä¼°ä¸ç¼“è§£)

---

## èƒŒæ™¯ä¸ç°çŠ¶åˆ†æ

### å½“å‰Tailå»é‡å®ç°

å½“å‰ç³»ç»Ÿå·²å®ç°å¯¹**å…±äº«headå’Œrelationçš„tailåˆ—è¡¨**çš„å»é‡ï¼Œæ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š

```python
# æŒ‰ (head, relation) åˆ†ç»„
for (head, relation), edges in grouped_edges.items():
    # 1. ç²¾ç¡®å»é‡
    exact_unique = self._deduplicate_exact(edges)
    
    # 2. è¯­ä¹‰å»é‡ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰å¤šä¸ªtailï¼‰
    if self._semantic_dedup_enabled() and len(exact_unique) > 1:
        semantic_deduplicated = self._semantic_deduplicate_group(head, relation, exact_unique)
```

**å…³é”®ç‰¹ç‚¹**ï¼š
- âœ… åˆ†ç»„ç²’åº¦ï¼š`(head, relation)`
- âœ… å»é‡å¯¹è±¡ï¼štailèŠ‚ç‚¹
- âœ… ä¸¤é˜¶æ®µå¤„ç†ï¼šç²¾ç¡®å»é‡ + è¯­ä¹‰å»é‡
- âœ… å…ƒæ•°æ®ä¿ç•™ï¼šå®Œæ•´æº¯æºä¿¡æ¯

### å½“å‰å®ä½“èŠ‚ç‚¹ç®¡ç†

```python
def _find_or_create_entity(self, entity_name: str, chunk_id: int, ...):
    # é€šè¿‡åç§°æŸ¥æ‰¾ç°æœ‰å®ä½“
    entity_node_id = next(
        (n for n, d in self.graph.nodes(data=True)
         if d.get("label") == "entity" and d["properties"]["name"] == entity_name),
        None,
    )
    
    if not entity_node_id:
        # åˆ›å»ºæ–°å®ä½“èŠ‚ç‚¹
        entity_node_id = f"entity_{self.node_counter}"
        # ...
```

**é—®é¢˜**ï¼šä»…åŸºäº**ç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…**è¿›è¡ŒèŠ‚ç‚¹å¤ç”¨ï¼Œæ— æ³•è¯†åˆ«è¯­ä¹‰ç­‰ä»·çš„å®ä½“ã€‚

---

## é—®é¢˜å®šä¹‰

### æ ¸å¿ƒé—®é¢˜

åœ¨çŸ¥è¯†å›¾è°±æ„å»ºè¿‡ç¨‹ä¸­ï¼ŒåŒä¸€å®ä½“å¯èƒ½è¢«ä»¥ä¸åŒå½¢å¼æåŠï¼š

| åœºæ™¯ | ç¤ºä¾‹ | é—®é¢˜ |
|------|------|------|
| åˆ«å/ç®€ç§° | "åŒ—äº¬" vs "åŒ—äº¬å¸‚" | åˆ›å»ºé‡å¤èŠ‚ç‚¹ |
| å…¨ç§°/ç®€ç§° | "ä¸­å›½äººæ°‘å¤§å­¦" vs "äººå¤§" | è¯­ä¹‰ç›¸åŒï¼Œå½¢å¼ä¸åŒ |
| ç¼©å†™ | "United Nations" vs "UN" | åŒä¸€ç»„ç»‡ä¸åŒè¡¨è¾¾ |
| ç¿»è¯‘å·®å¼‚ | "New York" vs "çº½çº¦" | è·¨è¯­è¨€ç­‰ä»· |
| æ­§ä¹‰æŒ‡ä»£ | "ä»–"ï¼Œ"å¼ ä¸‰" åœ¨ä¸åŒchunkä¸­ | ä¸Šä¸‹æ–‡ä¾èµ–çš„æŒ‡ä»£ |

**åæœ**ï¼š
- ğŸ”´ å›¾ç»“æ„å†—ä½™ï¼šé‡å¤èŠ‚ç‚¹å¢åŠ å­˜å‚¨å¼€é”€
- ğŸ”´ æŸ¥è¯¢æ•ˆç‡ä½ï¼šåŒä¸€å®ä½“çš„å…³ç³»åˆ†æ•£åœ¨å¤šä¸ªèŠ‚ç‚¹
- ğŸ”´ æ¨ç†å®Œæ•´æ€§å·®ï¼šæ— æ³•æ•´åˆåŒä¸€å®ä½“çš„æ‰€æœ‰çŸ¥è¯†

### ç›®æ ‡

å®ç°**headèŠ‚ç‚¹çš„è¯­ä¹‰å»é‡**ï¼Œä½¿å¾—ï¼š
1. è¯†åˆ«å¹¶åˆå¹¶è¯­ä¹‰ç­‰ä»·çš„headèŠ‚ç‚¹
2. æ•´åˆæ‰€æœ‰å…³è”å…³ç³»åˆ°ä»£è¡¨æ€§èŠ‚ç‚¹
3. ä¿ç•™å®Œæ•´æº¯æºä¿¡æ¯
4. ä¿æŒå›¾ç»“æ„ä¸€è‡´æ€§

---

## æ–¹æ¡ˆè®¾è®¡åŸåˆ™

### 1. ä¸“ä¸šåŸåˆ™

#### 1.1 å®ä½“ç­‰ä»·æ€§åˆ¤å®šåŸåˆ™ï¼ˆEntity Coreference Resolutionï¼‰

**æ ¸å¿ƒå®šä¹‰**ï¼šä¸¤ä¸ªheadèŠ‚ç‚¹ç­‰ä»· âŸº å®ƒä»¬æŒ‡ä»£ç°å®ä¸–ç•Œä¸­çš„**åŒä¸€å®ä½“**

**åˆ¤å®šæ ‡å‡†**ï¼ˆå‚è€ƒNLPä¸­çš„å®ä½“é“¾æ¥/Entity Linkingï¼‰ï¼š

```
ç­‰ä»·æ¡ä»¶ï¼ˆALLå¿…é¡»æ»¡è¶³ï¼‰ï¼š
â”œâ”€ æŒ‡ç§°ä¸€è‡´æ€§ (Referential Identity)
â”‚  â””â”€ æ˜¯å¦æŒ‡å‘åŒä¸€çœŸå®ä¸–ç•Œå¯¹è±¡ï¼Ÿ
â”œâ”€ æ›¿æ¢æµ‹è¯• (Substitutability Test)  
â”‚  â””â”€ åœ¨æ‰€æœ‰ä¸Šä¸‹æ–‡ä¸­äº’æ¢ä¸æ”¹å˜è¯­ä¹‰ï¼Ÿ
â””â”€ å±æ€§ä¸€è‡´æ€§ (Property Consistency)
   â””â”€ å…³é”®å±æ€§ï¼ˆç±»å‹/ç±»åˆ«ï¼‰æ˜¯å¦å…¼å®¹ï¼Ÿ
```

**é”™è¯¯ç¤ºä¾‹**ï¼ˆä¸åº”åˆå¹¶ï¼‰ï¼š
- âŒ "è‹¹æœ(æ°´æœ)" â‰  "è‹¹æœ(å…¬å¸)" â†’ ä¸åŒå®ä½“ç±»å‹
- âŒ "å¼ ä¸‰(æ•™æˆ)" â‰  "å¼ ä¸‰(å­¦ç”Ÿ)" â†’ åŒåä¸åŒäºº
- âŒ "åŒ—äº¬(å¤ä»£ç‡•äº¬)" â‰  "åŒ—äº¬(ç°ä»£é¦–éƒ½)" â†’ æ—¶é—´è¯­å¢ƒä¸åŒ

#### 1.2 ä¿å®ˆæ€§åŸåˆ™ï¼ˆConservative Principleï¼‰

```
é”™è¯¯æˆæœ¬ä¸å¯¹ç­‰ï¼š
  False Merge (é”™è¯¯åˆå¹¶) >> False Split (é”™è¯¯åˆ†ç¦»)
  
ç­–ç•¥ï¼š
  ä¸ç¡®å®šæ—¶ â†’ ä¿æŒåˆ†ç¦»
  ç½®ä¿¡åº¦é˜ˆå€¼ â†’ ä¸¥æ ¼è®¾å®šï¼ˆå»ºè®® â‰¥ 0.85ï¼‰
```

**ç†ç”±**ï¼š
- é”™è¯¯åˆå¹¶ä¼šå¯¼è‡´**ä¿¡æ¯æ±¡æŸ“**ï¼ˆä¸åŒå®ä½“çš„çŸ¥è¯†æ··æ·†ï¼‰
- é”™è¯¯åˆ†ç¦»ä»…å¯¼è‡´**ä¿¡æ¯åˆ†æ•£**ï¼ˆå¯é€šè¿‡æŸ¥è¯¢èšåˆè¡¥æ•‘ï¼‰

#### 1.3 å¯è§£é‡Šæ€§åŸåˆ™

æ¯æ¬¡åˆå¹¶å¿…é¡»æä¾›ï¼š
- **åˆå¹¶ä¾æ®**ï¼šä¸ºä»€ä¹ˆè¿™ä¸¤ä¸ªèŠ‚ç‚¹æ˜¯åŒä¸€å®ä½“ï¼Ÿ
- **ç½®ä¿¡åº¦**ï¼šå†³ç­–å¯é æ€§é‡åŒ–
- **æº¯æºè·¯å¾„**ï¼šä»åŸå§‹èŠ‚ç‚¹åˆ°åˆå¹¶èŠ‚ç‚¹çš„å®Œæ•´è·¯å¾„

---

### 2. æŠ€æœ¯åŸåˆ™

#### 2.1 ä¸¤é˜¶æ®µå¤„ç†ï¼ˆä¸tailå»é‡ä¿æŒä¸€è‡´ï¼‰

```
Phase 1: ç²¾ç¡®åŒ¹é…å»é‡
  â”œâ”€ å®Œå…¨ç›¸åŒçš„å­—ç¬¦ä¸² â†’ ç›´æ¥åˆå¹¶
  â””â”€ æ€§èƒ½ï¼šO(n)ï¼Œå¿«é€Ÿè¿‡æ»¤

Phase 2: è¯­ä¹‰ç›¸ä¼¼åº¦å»é‡
  â”œâ”€ åµŒå…¥å‘é‡èšç±»
  â”œâ”€ LLMåˆ¤æ–­ï¼ˆå¯é€‰ï¼‰
  â””â”€ æ€§èƒ½ï¼šO(nÂ²) æˆ– O(n log n)ï¼Œç²¾ç»†å¤„ç†
```

#### 2.2 å›¾ç»“æ„ä¸€è‡´æ€§

**èŠ‚ç‚¹åˆå¹¶æ“ä½œ**å¿…é¡»ä¿è¯ï¼š
1. **è¾¹çš„å®Œæ•´æ€§**ï¼šæ‰€æœ‰å…¥è¾¹å’Œå‡ºè¾¹æ­£ç¡®è½¬ç§»
2. **å…ƒæ•°æ®å®Œæ•´æ€§**ï¼šæ‰€æœ‰å±æ€§ä¿¡æ¯åˆå¹¶
3. **æº¯æºå®Œæ•´æ€§**ï¼šè®°å½•åˆå¹¶å‰çš„æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ID

#### 2.3 æ‰¹é‡å¹¶å‘å¤„ç†

ä¸ç°æœ‰tailå»é‡é€»è¾‘ä¿æŒä¸€è‡´ï¼š
- æ‰¹é‡æ”¶é›†æ‰€æœ‰å¾…åˆ¤æ–­çš„èŠ‚ç‚¹å¯¹
- å¹¶å‘è°ƒç”¨LLM/embeddingæ¨¡å‹
- ç»Ÿä¸€è§£æç»“æœåæ‰¹é‡åº”ç”¨

---

## æŠ€æœ¯æ–¹æ¡ˆ

### æ–¹æ¡ˆæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Head Deduplication Pipeline               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1:     â”‚              â”‚ Phase 2:         â”‚
â”‚ Exact Match  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Semantic Dedup   â”‚
â”‚ Deduplicationâ”‚              â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                               â”‚
        â”œâ”€ String normalization         â”œâ”€ Candidate Generation
        â”œâ”€ Hash-based grouping          â”œâ”€ Embedding Clustering
        â””â”€ Direct merge                 â”œâ”€ LLM Validation (optional)
                                        â””â”€ Merge with metadata
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Phase 3:                      â”‚
        â”‚ Graph Structure Update        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”œâ”€ Edge reassignment (in/out)
        â”œâ”€ Metadata consolidation
        â”œâ”€ Node removal
        â””â”€ Provenance recording
```

### æ ¸å¿ƒæµç¨‹

#### 1. å€™é€‰èŠ‚ç‚¹æ”¶é›†

```python
def _collect_head_candidates(self) -> List[str]:
    """æ”¶é›†æ‰€æœ‰éœ€è¦å»é‡çš„headèŠ‚ç‚¹ï¼ˆentityèŠ‚ç‚¹ï¼‰"""
    candidates = [
        node_id
        for node_id, data in self.graph.nodes(data=True)
        if data.get("label") == "entity"  # åªå¤„ç†å®ä½“èŠ‚ç‚¹
    ]
    return candidates
```

**å…³é”®å†³ç­–**ï¼š
- ä»…å¯¹ `label == "entity"` çš„èŠ‚ç‚¹å»é‡
- ä¸å¤„ç† `attribute`ã€`keyword`ã€`community` èŠ‚ç‚¹
- ç†ç”±ï¼šè¿™äº›èŠ‚ç‚¹æœ‰ä¸åŒçš„è¯­ä¹‰å’Œç”Ÿå‘½å‘¨æœŸ

#### 2. Phase 1: ç²¾ç¡®åŒ¹é…å»é‡

```python
def _deduplicate_heads_exact(self, candidates: List[str]) -> Dict[str, str]:
    """
    ç²¾ç¡®åŒ¹é…å»é‡
    
    Returns:
        Dict[old_id, canonical_id]: æ˜ å°„è¡¨
    """
    # æŒ‰æ ‡å‡†åŒ–åç§°åˆ†ç»„
    name_groups = defaultdict(list)
    
    for node_id in candidates:
        node_data = self.graph.nodes[node_id]
        name = node_data.get("properties", {}).get("name", "")
        
        # æ ‡å‡†åŒ–å¤„ç†
        normalized_name = self._normalize_entity_name(name)
        name_groups[normalized_name].append(node_id)
    
    # æ„å»ºåˆå¹¶æ˜ å°„
    merge_mapping = {}
    for normalized_name, node_ids in name_groups.items():
        if len(node_ids) > 1:
            # é€‰æ‹©ä»£è¡¨æ€§èŠ‚ç‚¹ï¼ˆå¯å‘å¼ï¼šæœ€æ—©åˆ›å»ºçš„èŠ‚ç‚¹ï¼‰
            canonical_id = min(node_ids, key=lambda x: int(x.split('_')[1]))
            for node_id in node_ids:
                if node_id != canonical_id:
                    merge_mapping[node_id] = canonical_id
    
    return merge_mapping

def _normalize_entity_name(self, name: str) -> str:
    """å®ä½“åç§°æ ‡å‡†åŒ–"""
    # 1. è½¬å°å†™
    normalized = name.lower().strip()
    # 2. å»é™¤å¤šä½™ç©ºæ ¼
    normalized = ' '.join(normalized.split())
    # 3. å»é™¤å¸¸è§æ ‡ç‚¹ç¬¦å·
    normalized = normalized.replace('.', '').replace(',', '').replace('!', '')
    # 4. ç»Ÿä¸€å…¨è§’/åŠè§’ï¼ˆä¸­æ–‡ç¯å¢ƒï¼‰
    # ... å¯æ‰©å±•æ›´å¤šè§„åˆ™
    return normalized
```

**æ€§èƒ½**ï¼šO(n)ï¼Œé€‚åˆå¤§è§„æ¨¡å›¾

#### 3. Phase 2: è¯­ä¹‰å»é‡

##### 3.1 å€™é€‰å¯¹ç”Ÿæˆï¼ˆCandidate Pair Generationï¼‰

```python
def _generate_semantic_candidates(
    self, 
    remaining_nodes: List[str],
    max_candidates: int = 1000
) -> List[Tuple[str, str, float]]:
    """
    ç”Ÿæˆéœ€è¦è¯­ä¹‰åˆ¤æ–­çš„å€™é€‰èŠ‚ç‚¹å¯¹
    
    ç­–ç•¥ï¼šåŸºäºembeddingç›¸ä¼¼åº¦é¢„ç­›é€‰ï¼Œé¿å…O(nÂ²)æš´åŠ›æ¯”è¾ƒ
    
    Returns:
        List[(node_id_1, node_id_2, similarity_score)]
    """
    # 1. æ‰¹é‡è·å–èŠ‚ç‚¹æè¿°
    node_descriptions = {
        node_id: self._describe_node_for_clustering(node_id)
        for node_id in remaining_nodes
    }
    
    # 2. æ‰¹é‡è·å–embedding
    embeddings = self._batch_get_embeddings(list(node_descriptions.values()))
    node_to_embedding = dict(zip(node_descriptions.keys(), embeddings))
    
    # 3. åŸºäºä½™å¼¦ç›¸ä¼¼åº¦èšç±»ï¼ˆå¿«é€Ÿé¢„ç­›é€‰ï¼‰
    from sklearn.metrics.pairwise import cosine_similarity
    similarity_matrix = cosine_similarity(list(node_to_embedding.values()))
    
    # 4. æå–é«˜ç›¸ä¼¼åº¦å€™é€‰å¯¹
    candidates = []
    nodes = list(node_to_embedding.keys())
    threshold = 0.75  # é¢„ç­›é€‰é˜ˆå€¼ï¼ˆè¾ƒå®½æ¾ï¼‰
    
    for i in range(len(nodes)):
        for j in range(i + 1, len(nodes)):
            sim = similarity_matrix[i][j]
            if sim >= threshold:
                candidates.append((nodes[i], nodes[j], float(sim)))
    
    # 5. æŒ‰ç›¸ä¼¼åº¦é™åºæ’åºï¼Œå–top-K
    candidates.sort(key=lambda x: x[2], reverse=True)
    return candidates[:max_candidates]
```

**å…³é”®æŠ€æœ¯**ï¼š
- **Blocking/Indexing**ï¼šé¿å…O(nÂ²)å¤æ‚åº¦
- **ä¸¤é˜¶æ®µè¿‡æ»¤**ï¼šembeddingé¢„ç­›é€‰ + LLMç²¾ç¡®åˆ¤æ–­
- **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒLSHã€FAISSç­‰è¿‘ä¼¼æœ€è¿‘é‚»ç®—æ³•

##### 3.2 LLMè¯­ä¹‰åˆ¤æ–­ï¼ˆå¯é€‰å¢å¼ºï¼‰

```python
def _validate_head_coreference_with_llm(
    self,
    node_id_1: str,
    node_id_2: str
) -> Tuple[bool, str, float]:
    """
    ä½¿ç”¨LLMåˆ¤æ–­ä¸¤ä¸ªheadèŠ‚ç‚¹æ˜¯å¦æŒ‡ä»£åŒä¸€å®ä½“
    
    Returns:
        (is_coreferent, rationale, confidence)
    """
    desc_1 = self._describe_node(node_id_1)
    desc_2 = self._describe_node(node_id_2)
    
    # æ”¶é›†ä¸Šä¸‹æ–‡ï¼ˆå…³è”çš„å…³ç³»å’Œå±æ€§ï¼‰
    context_1 = self._collect_node_context(node_id_1)
    context_2 = self._collect_node_context(node_id_2)
    
    prompt = self._build_head_dedup_prompt(
        entity_1=desc_1,
        entity_2=desc_2,
        context_1=context_1,
        context_2=context_2
    )
    
    response = self.extract_with_llm(prompt)
    parsed = self._parse_coreference_response(response)
    
    return (
        parsed.get("is_coreferent", False),
        parsed.get("rationale", ""),
        parsed.get("confidence", 0.0)
    )

def _collect_node_context(self, node_id: str, max_relations: int = 10) -> str:
    """æ”¶é›†èŠ‚ç‚¹çš„å…³ç³»ä¸Šä¸‹æ–‡ï¼Œç”¨äºLLMåˆ¤æ–­"""
    contexts = []
    
    # å‡ºè¾¹ï¼ˆè¯¥å®ä½“ä½œä¸ºheadï¼‰
    out_edges = list(self.graph.out_edges(node_id, data=True))[:max_relations]
    for _, tail_id, data in out_edges:
        relation = data.get("relation", "related_to")
        tail_desc = self._describe_node(tail_id)
        contexts.append(f"  - {relation}: {tail_desc}")
    
    # å…¥è¾¹ï¼ˆè¯¥å®ä½“ä½œä¸ºtailï¼‰
    in_edges = list(self.graph.in_edges(node_id, data=True))[:max_relations]
    for head_id, _, data in in_edges:
        relation = data.get("relation", "related_to")
        head_desc = self._describe_node(head_id)
        contexts.append(f"  - (reverse) {head_desc} â†’ {relation}")
    
    return "\n".join(contexts) if contexts else "No relations found"
```

##### 3.3 Promptè®¾è®¡

```python
HEAD_DEDUP_PROMPT_TEMPLATE = """
You are an expert in knowledge graph entity resolution.

TASK: Determine if the following two entities refer to the SAME real-world object.

Entity 1: {entity_1}
Related knowledge about Entity 1:
{context_1}

Entity 2: {entity_2}
Related knowledge about Entity 2:
{context_2}

CRITICAL RULES:
1. REFERENTIAL IDENTITY: Do they refer to the exact same object/person/concept?
   - Same entity with different names â†’ YES (e.g., "NYC" = "New York City")
   - Different but related entities â†’ NO (e.g., "Apple Inc." â‰  "Apple Store")

2. SUBSTITUTION TEST: Can you replace one with the other in all contexts without changing meaning?
   - If substitution changes information â†’ NO
   - If substitution preserves meaning â†’ YES

3. TYPE CONSISTENCY: Check entity types/categories
   - "Beijing (city)" â‰  "Beijing (ancient capital)" if contexts differ significantly
   - Same name, different types â†’ carefully verify with context

4. CONSERVATIVE PRINCIPLE:
   - When uncertain â†’ answer NO
   - False merge is worse than false split

PROHIBITED MERGE REASONS (NOT valid):
âœ— Similar names (e.g., "John Smith" vs "John Smith Jr.")
âœ— Related entities (e.g., "Apple" company vs "Apple" product)
âœ— Same category (e.g., both are cities, but different cities)

OUTPUT FORMAT (strict JSON):
{{
  "is_coreferent": true/false,
  "confidence": 0.0-1.0,
  "rationale": "Clear explanation based on referential identity test"
}}

EXAMPLES:
âœ“ "åŒ—äº¬" and "åŒ—äº¬å¸‚" â†’ is_coreferent: true (same city, different name format)
âœ“ "UN" and "United Nations" â†’ is_coreferent: true (abbreviation of same organization)
âœ— "å¼ ä¸‰(æ•™æˆ)" and "å¼ ä¸‰(å­¦ç”Ÿ)" â†’ is_coreferent: false (same name, different persons based on context)
âœ— "Apple Inc." and "Apple Store" â†’ is_coreferent: false (company vs retail location)

Now, analyze Entity 1 and Entity 2:
"""
```

**è®¾è®¡è¦ç‚¹**ï¼š
- æ˜ç¡®**æŒ‡ä»£ä¸€è‡´æ€§**ï¼ˆreferential identityï¼‰åŸåˆ™
- æä¾›**æ­£ä¾‹å’Œåä¾‹**ï¼Œfew-shot learning
- è¦æ±‚**ç»“æ„åŒ–è¾“å‡º**ï¼ˆJSONï¼‰ä¾¿äºè§£æ
- **ä¿å®ˆæ€§æç¤º**ï¼šä¸ç¡®å®šæ—¶é€‰æ‹©ä¸åˆå¹¶

#### 4. Phase 3: å›¾ç»“æ„æ›´æ–°

```python
def _merge_head_nodes(
    self,
    merge_mapping: Dict[str, str],
    merge_metadata: Dict[str, dict]
) -> int:
    """
    æ‰§è¡ŒheadèŠ‚ç‚¹åˆå¹¶ï¼Œæ›´æ–°å›¾ç»“æ„
    
    Args:
        merge_mapping: {duplicate_id: canonical_id}
        merge_metadata: {duplicate_id: {"rationale": ..., "confidence": ...}}
    
    Returns:
        åˆå¹¶çš„èŠ‚ç‚¹æ•°é‡
    """
    merged_count = 0
    
    for duplicate_id, canonical_id in merge_mapping.items():
        if duplicate_id not in self.graph:
            continue  # å·²è¢«åˆ é™¤
        
        if canonical_id not in self.graph:
            logger.warning(f"Canonical node {canonical_id} not found, skipping")
            continue
        
        # 1. è½¬ç§»æ‰€æœ‰å‡ºè¾¹ (duplicate_idä½œä¸ºheadçš„è¾¹)
        self._reassign_outgoing_edges(duplicate_id, canonical_id)
        
        # 2. è½¬ç§»æ‰€æœ‰å…¥è¾¹ (duplicate_idä½œä¸ºtailçš„è¾¹)
        self._reassign_incoming_edges(duplicate_id, canonical_id)
        
        # 3. åˆå¹¶èŠ‚ç‚¹å±æ€§
        self._merge_node_properties(duplicate_id, canonical_id, merge_metadata.get(duplicate_id, {}))
        
        # 4. åˆ é™¤é‡å¤èŠ‚ç‚¹
        self.graph.remove_node(duplicate_id)
        merged_count += 1
        
        logger.debug(f"Merged {duplicate_id} into {canonical_id}")
    
    return merged_count

def _reassign_outgoing_edges(self, source_id: str, target_id: str):
    """è½¬ç§»å‡ºè¾¹ï¼ˆsource_idä½œä¸ºheadçš„æ‰€æœ‰å…³ç³»ï¼‰"""
    outgoing = list(self.graph.out_edges(source_id, keys=True, data=True))
    
    for _, tail_id, key, data in outgoing:
        # é¿å…è‡ªç¯ (å¦‚æœtailå°±æ˜¯target_id)
        if tail_id == target_id:
            continue
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„è¾¹
        if not self._edge_exists(target_id, tail_id, data):
            self.graph.add_edge(target_id, tail_id, **copy.deepcopy(data))
        else:
            # å¦‚æœè¾¹å·²å­˜åœ¨ï¼Œåˆå¹¶chunkä¿¡æ¯
            self._merge_edge_chunks(target_id, tail_id, data)

def _reassign_incoming_edges(self, source_id: str, target_id: str):
    """è½¬ç§»å…¥è¾¹ï¼ˆsource_idä½œä¸ºtailçš„æ‰€æœ‰å…³ç³»ï¼‰"""
    incoming = list(self.graph.in_edges(source_id, keys=True, data=True))
    
    for head_id, _, key, data in incoming:
        # é¿å…è‡ªç¯
        if head_id == target_id:
            continue
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„è¾¹
        if not self._edge_exists(head_id, target_id, data):
            self.graph.add_edge(head_id, target_id, **copy.deepcopy(data))
        else:
            self._merge_edge_chunks(head_id, target_id, data)

def _merge_node_properties(self, duplicate_id: str, canonical_id: str, metadata: dict):
    """åˆå¹¶èŠ‚ç‚¹å±æ€§ï¼Œè®°å½•æº¯æºä¿¡æ¯"""
    canonical_data = self.graph.nodes[canonical_id]
    duplicate_data = self.graph.nodes[duplicate_id]
    
    # åˆå§‹åŒ–head_dedupå…ƒæ•°æ®
    if "head_dedup" not in canonical_data.get("properties", {}):
        canonical_data.setdefault("properties", {})["head_dedup"] = {
            "merged_nodes": [],
            "merge_history": []
        }
    
    # è®°å½•åˆå¹¶ä¿¡æ¯
    canonical_data["properties"]["head_dedup"]["merged_nodes"].append(duplicate_id)
    canonical_data["properties"]["head_dedup"]["merge_history"].append({
        "merged_node_id": duplicate_id,
        "merged_node_name": duplicate_data.get("properties", {}).get("name", ""),
        "rationale": metadata.get("rationale", "Exact match or semantic similarity"),
        "confidence": metadata.get("confidence", 1.0),
        "timestamp": time.time()
    })
    
    # åˆå¹¶chunkä¿¡æ¯
    canonical_chunks = set(canonical_data.get("properties", {}).get("chunk_ids", []))
    duplicate_chunks = set(duplicate_data.get("properties", {}).get("chunk_ids", []))
    merged_chunks = list(canonical_chunks | duplicate_chunks)
    if merged_chunks:
        canonical_data["properties"]["chunk_ids"] = merged_chunks

def _edge_exists(self, u: str, v: str, new_data: dict) -> bool:
    """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„è¾¹ï¼ˆåŸºäºrelationï¼‰"""
    new_relation = new_data.get("relation")
    for _, _, data in self.graph.edges(u, v, data=True):
        if data.get("relation") == new_relation:
            return True
    return False

def _merge_edge_chunks(self, u: str, v: str, new_data: dict):
    """åˆå¹¶è¾¹çš„chunkä¿¡æ¯åˆ°å·²å­˜åœ¨çš„è¾¹"""
    new_relation = new_data.get("relation")
    new_chunks = set(new_data.get("source_chunks", []))
    
    for key, data in self.graph[u][v].items():
        if data.get("relation") == new_relation:
            existing_chunks = set(data.get("source_chunks", []))
            merged_chunks = list(existing_chunks | new_chunks)
            if merged_chunks:
                data["source_chunks"] = merged_chunks
            break
```

---

## å®ç°ç»†èŠ‚

### å®Œæ•´Pipelineæ¥å£

```python
def deduplicate_heads(
    self,
    enable_semantic: bool = True,
    similarity_threshold: float = 0.85,
    use_llm_validation: bool = False,
    max_candidates: int = 1000
) -> Dict[str, Any]:
    """
    ä¸»å…¥å£ï¼šæ‰§è¡ŒheadèŠ‚ç‚¹å»é‡
    
    Args:
        enable_semantic: æ˜¯å¦å¯ç”¨è¯­ä¹‰å»é‡
        similarity_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
        use_llm_validation: æ˜¯å¦ä½¿ç”¨LLMéªŒè¯ï¼ˆæé«˜å‡†ç¡®ç‡ä½†æ›´æ…¢ï¼‰
        max_candidates: æœ€å¤§å¤„ç†å€™é€‰å¯¹æ•°é‡
    
    Returns:
        å»é‡ç»Ÿè®¡ä¿¡æ¯
    """
    logger.info("=" * 60)
    logger.info("Starting Head Node Deduplication")
    logger.info("=" * 60)
    
    start_time = time.time()
    
    # Phase 1: æ”¶é›†å€™é€‰èŠ‚ç‚¹
    logger.info("Phase 1: Collecting head candidates...")
    candidates = self._collect_head_candidates()
    logger.info(f"  Found {len(candidates)} entity nodes")
    
    # Phase 2: ç²¾ç¡®åŒ¹é…å»é‡
    logger.info("Phase 2: Exact match deduplication...")
    exact_merge_mapping = self._deduplicate_heads_exact(candidates)
    logger.info(f"  Exact matches: {len(exact_merge_mapping)} merges")
    
    # åº”ç”¨ç²¾ç¡®åŒ¹é…åˆå¹¶
    exact_merged_count = self._merge_head_nodes(exact_merge_mapping, {})
    
    # Phase 3: è¯­ä¹‰å»é‡ï¼ˆå¯é€‰ï¼‰
    semantic_merge_mapping = {}
    semantic_merged_count = 0
    
    if enable_semantic:
        logger.info("Phase 3: Semantic deduplication...")
        
        # 3.1 è·å–å‰©ä½™èŠ‚ç‚¹ï¼ˆæ’é™¤å·²åˆå¹¶çš„ï¼‰
        remaining_nodes = [
            node_id for node_id in candidates
            if node_id not in exact_merge_mapping and node_id in self.graph
        ]
        logger.info(f"  Remaining nodes: {len(remaining_nodes)}")
        
        # 3.2 ç”Ÿæˆå€™é€‰å¯¹
        candidate_pairs = self._generate_semantic_candidates(
            remaining_nodes, 
            max_candidates=max_candidates
        )
        logger.info(f"  Generated {len(candidate_pairs)} candidate pairs")
        
        # 3.3 éªŒè¯å€™é€‰å¯¹
        if use_llm_validation:
            logger.info("  Using LLM validation (slower but more accurate)...")
            semantic_merge_mapping, metadata = self._validate_candidates_with_llm(
                candidate_pairs, 
                similarity_threshold
            )
        else:
            logger.info("  Using embedding-only validation (faster)...")
            semantic_merge_mapping, metadata = self._validate_candidates_with_embedding(
                candidate_pairs, 
                similarity_threshold
            )
        
        logger.info(f"  Semantic matches: {len(semantic_merge_mapping)} merges")
        
        # 3.4 åº”ç”¨è¯­ä¹‰åˆå¹¶
        semantic_merged_count = self._merge_head_nodes(semantic_merge_mapping, metadata)
    
    elapsed_time = time.time() - start_time
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_candidates": len(candidates),
        "exact_merges": exact_merged_count,
        "semantic_merges": semantic_merged_count,
        "total_merges": exact_merged_count + semantic_merged_count,
        "remaining_nodes": len([n for n, d in self.graph.nodes(data=True) if d.get("label") == "entity"]),
        "elapsed_time_seconds": elapsed_time
    }
    
    logger.info("=" * 60)
    logger.info("Head Deduplication Completed")
    logger.info(f"  Total merges: {stats['total_merges']}")
    logger.info(f"  Reduction rate: {stats['total_merges'] / stats['total_candidates'] * 100:.2f}%")
    logger.info(f"  Time elapsed: {elapsed_time:.2f}s")
    logger.info("=" * 60)
    
    return stats

def _validate_candidates_with_embedding(
    self,
    candidate_pairs: List[Tuple[str, str, float]],
    threshold: float
) -> Tuple[Dict[str, str], Dict[str, dict]]:
    """åŸºäºembeddingç›¸ä¼¼åº¦éªŒè¯å€™é€‰å¯¹"""
    merge_mapping = {}
    metadata = {}
    
    for node_id_1, node_id_2, similarity in candidate_pairs:
        if similarity >= threshold:
            # é€‰æ‹©canonicalèŠ‚ç‚¹ï¼ˆå¯å‘å¼ï¼šIDæ›´å°çš„ï¼‰
            canonical = min(node_id_1, node_id_2, key=lambda x: int(x.split('_')[1]))
            duplicate = node_id_2 if canonical == node_id_1 else node_id_1
            
            merge_mapping[duplicate] = canonical
            metadata[duplicate] = {
                "rationale": f"High embedding similarity: {similarity:.3f}",
                "confidence": float(similarity)
            }
    
    return merge_mapping, metadata

def _validate_candidates_with_llm(
    self,
    candidate_pairs: List[Tuple[str, str, float]],
    threshold: float
) -> Tuple[Dict[str, str], Dict[str, dict]]:
    """ä½¿ç”¨LLMéªŒè¯å€™é€‰å¯¹ï¼ˆå¹¶å‘å¤„ç†ï¼‰"""
    # æ„å»ºLLM prompts
    prompts = []
    for node_id_1, node_id_2, embedding_sim in candidate_pairs:
        prompts.append({
            "prompt": self._build_head_dedup_prompt(node_id_1, node_id_2),
            "metadata": {
                "node_id_1": node_id_1,
                "node_id_2": node_id_2,
                "embedding_similarity": embedding_sim
            }
        })
    
    # å¹¶å‘è°ƒç”¨LLM
    logger.info(f"  Processing {len(prompts)} LLM validation calls...")
    llm_results = self._concurrent_llm_calls(prompts)
    
    # è§£æç»“æœ
    merge_mapping = {}
    metadata = {}
    
    for result in llm_results:
        meta = result.get("metadata", {})
        response = result.get("response", "")
        
        parsed = self._parse_coreference_response(response)
        is_coreferent = parsed.get("is_coreferent", False)
        confidence = parsed.get("confidence", 0.0)
        rationale = parsed.get("rationale", "")
        
        # åªåˆå¹¶é«˜ç½®ä¿¡åº¦çš„ç»“æœ
        if is_coreferent and confidence >= threshold:
            node_id_1 = meta["node_id_1"]
            node_id_2 = meta["node_id_2"]
            
            canonical = min(node_id_1, node_id_2, key=lambda x: int(x.split('_')[1]))
            duplicate = node_id_2 if canonical == node_id_1 else node_id_1
            
            merge_mapping[duplicate] = canonical
            metadata[duplicate] = {
                "rationale": rationale,
                "confidence": confidence,
                "embedding_similarity": meta.get("embedding_similarity", 0.0)
            }
    
    return merge_mapping, metadata

def _parse_coreference_response(self, response: str) -> dict:
    """è§£æLLMçš„å…±æŒ‡åˆ¤æ–­å“åº”"""
    try:
        parsed = json_repair.loads(response)
        return {
            "is_coreferent": parsed.get("is_coreferent", False),
            "confidence": float(parsed.get("confidence", 0.0)),
            "rationale": parsed.get("rationale", "")
        }
    except Exception as e:
        logger.warning(f"Failed to parse LLM response: {e}")
        return {"is_coreferent": False, "confidence": 0.0, "rationale": "Parse error"}
```

---

## æ•°æ®ä¸€è‡´æ€§ä¿éšœ

### 1. äº‹åŠ¡æ€§ä¿è¯

```python
def _merge_head_nodes_with_rollback(self, merge_mapping: Dict[str, str], metadata: Dict[str, dict]):
    """å¸¦å›æ»šæœºåˆ¶çš„èŠ‚ç‚¹åˆå¹¶"""
    # 1. ä¿å­˜å›¾å¿«ç…§ï¼ˆä»…å…ƒæ•°æ®ï¼Œä¸æ·±æ‹·è´æ•´ä¸ªå›¾ï¼‰
    snapshot = {
        "removed_nodes": [],
        "added_edges": [],
        "modified_nodes": []
    }
    
    try:
        for duplicate_id, canonical_id in merge_mapping.items():
            # è®°å½•å˜æ›´
            snapshot["removed_nodes"].append((duplicate_id, copy.deepcopy(self.graph.nodes[duplicate_id])))
            
            # æ‰§è¡Œåˆå¹¶
            self._merge_single_head_node(duplicate_id, canonical_id, metadata, snapshot)
        
        return True
    
    except Exception as e:
        logger.error(f"Error during head merge: {e}, rolling back...")
        self._rollback_merge(snapshot)
        return False

def _rollback_merge(self, snapshot: dict):
    """å›æ»šåˆå¹¶æ“ä½œ"""
    # æ¢å¤åˆ é™¤çš„èŠ‚ç‚¹
    for node_id, node_data in snapshot["removed_nodes"]:
        if node_id not in self.graph:
            self.graph.add_node(node_id, **node_data)
    
    # åˆ é™¤æ–°å¢çš„è¾¹
    for u, v, key in snapshot["added_edges"]:
        if self.graph.has_edge(u, v, key):
            self.graph.remove_edge(u, v, key)
    
    # æ¢å¤ä¿®æ”¹çš„èŠ‚ç‚¹
    for node_id, original_data in snapshot["modified_nodes"]:
        if node_id in self.graph:
            self.graph.nodes[node_id].update(original_data)
```

### 2. å›¾å®Œæ•´æ€§éªŒè¯

```python
def validate_graph_integrity_after_head_dedup(self) -> Dict[str, Any]:
    """éªŒè¯å»é‡åå›¾ç»“æ„çš„å®Œæ•´æ€§"""
    issues = {
        "orphan_nodes": [],
        "self_loops": [],
        "dangling_references": [],
        "missing_metadata": []
    }
    
    # 1. æ£€æŸ¥å­¤ç«‹èŠ‚ç‚¹ï¼ˆæ— å…¥è¾¹ä¹Ÿæ— å‡ºè¾¹çš„entityèŠ‚ç‚¹ï¼‰
    for node_id, data in self.graph.nodes(data=True):
        if data.get("label") == "entity":
            in_degree = self.graph.in_degree(node_id)
            out_degree = self.graph.out_degree(node_id)
            if in_degree == 0 and out_degree == 0:
                issues["orphan_nodes"].append(node_id)
    
    # 2. æ£€æŸ¥è‡ªç¯
    for u, v in self.graph.edges():
        if u == v:
            issues["self_loops"].append((u, v))
    
    # 3. æ£€æŸ¥è¾¹å¼•ç”¨çš„èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
    for u, v, data in self.graph.edges(data=True):
        if u not in self.graph.nodes:
            issues["dangling_references"].append(("head", u, v, data))
        if v not in self.graph.nodes:
            issues["dangling_references"].append(("tail", u, v, data))
    
    # 4. æ£€æŸ¥åˆå¹¶å…ƒæ•°æ®çš„å®Œæ•´æ€§
    for node_id, data in self.graph.nodes(data=True):
        if "head_dedup" in data.get("properties", {}):
            dedup_info = data["properties"]["head_dedup"]
            if "merged_nodes" not in dedup_info or "merge_history" not in dedup_info:
                issues["missing_metadata"].append(node_id)
    
    return issues
```

---

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. åˆ†æ‰¹å¤„ç†å¤§è§„æ¨¡å›¾

```python
def deduplicate_heads_batched(self, batch_size: int = 1000):
    """åˆ†æ‰¹å¤„ç†å¤§è§„æ¨¡å›¾çš„headå»é‡"""
    candidates = self._collect_head_candidates()
    
    # åˆ†æ‰¹
    for i in range(0, len(candidates), batch_size):
        batch = candidates[i:i + batch_size]
        logger.info(f"Processing batch {i//batch_size + 1}/{(len(candidates) + batch_size - 1)//batch_size}")
        
        # å¯¹æ¯æ‰¹ç‹¬ç«‹å»é‡
        self.deduplicate_heads_on_subset(batch)
```

### 2. ç¼“å­˜ä¼˜åŒ–

```python
@lru_cache(maxsize=10000)
def _describe_node_cached(self, node_id: str) -> str:
    """ç¼“å­˜èŠ‚ç‚¹æè¿°ï¼Œé¿å…é‡å¤è®¡ç®—"""
    return self._describe_node(node_id)

@lru_cache(maxsize=10000)
def _get_node_embedding_cached(self, node_id: str) -> np.ndarray:
    """ç¼“å­˜èŠ‚ç‚¹embedding"""
    description = self._describe_node_cached(node_id)
    return self._get_embedding(description)
```

### 3. ç´¢å¼•åŠ é€Ÿ

```python
def _build_entity_index(self):
    """æ„å»ºå®ä½“åç§°å€’æ’ç´¢å¼•ï¼ŒåŠ é€ŸæŸ¥æ‰¾"""
    self.entity_name_index = defaultdict(list)
    
    for node_id, data in self.graph.nodes(data=True):
        if data.get("label") == "entity":
            name = data.get("properties", {}).get("name", "")
            normalized = self._normalize_entity_name(name)
            self.entity_name_index[normalized].append(node_id)
```

---

## é£é™©è¯„ä¼°ä¸ç¼“è§£

### é£é™©çŸ©é˜µ

| é£é™© | ä¸¥é‡æ€§ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|--------|------|----------|
| é”™è¯¯åˆå¹¶ä¸åŒå®ä½“ | ğŸ”´ é«˜ | ä¸­ | 1. ä¸¥æ ¼é˜ˆå€¼(â‰¥0.85)<br>2. LLMäºŒæ¬¡éªŒè¯<br>3. äººå·¥å®¡æ ¸æ¥å£ |
| æ€§èƒ½ç“¶é¢ˆï¼ˆå¤§å›¾ï¼‰ | ğŸŸ¡ ä¸­ | é«˜ | 1. åˆ†æ‰¹å¤„ç†<br>2. å¹¶å‘LLMè°ƒç”¨<br>3. embeddingé¢„ç­›é€‰ |
| å›¾ç»“æ„ç ´å | ğŸ”´ é«˜ | ä½ | 1. äº‹åŠ¡æ€§æ“ä½œ<br>2. å®Œæ•´æ€§éªŒè¯<br>3. å›æ»šæœºåˆ¶ |
| å…ƒæ•°æ®ä¸¢å¤± | ğŸŸ¡ ä¸­ | ä½ | 1. å®Œæ•´æº¯æºè®°å½•<br>2. åˆå¹¶å†å²ä¿å­˜<br>3. chunkä¿¡æ¯èšåˆ |

### äººå·¥å®¡æ ¸æ¥å£

```python
def export_head_merge_candidates_for_review(
    self,
    output_path: str,
    min_confidence: float = 0.7,
    max_confidence: float = 0.9
):
    """
    å¯¼å‡ºä¸­ç­‰ç½®ä¿¡åº¦çš„åˆå¹¶å€™é€‰ï¼Œä¾›äººå·¥å®¡æ ¸
    
    ç½®ä¿¡åº¦åŒºé—´ [min_confidence, max_confidence] çš„æ¡ˆä¾‹éœ€è¦äººå·¥ç¡®è®¤
    """
    candidates = []
    
    for node_id, data in self.graph.nodes(data=True):
        dedup_info = data.get("properties", {}).get("head_dedup", {})
        for merge_record in dedup_info.get("merge_history", []):
            confidence = merge_record.get("confidence", 1.0)
            if min_confidence <= confidence <= max_confidence:
                candidates.append({
                    "canonical_node": node_id,
                    "canonical_name": data.get("properties", {}).get("name", ""),
                    "merged_node": merge_record["merged_node_id"],
                    "merged_name": merge_record["merged_node_name"],
                    "confidence": confidence,
                    "rationale": merge_record["rationale"]
                })
    
    # å¯¼å‡ºä¸ºCSV
    import csv
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=candidates[0].keys() if candidates else [])
        writer.writeheader()
        writer.writerows(candidates)
    
    logger.info(f"Exported {len(candidates)} merge candidates to {output_path}")
```

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ç”¨æ³•

```python
# åˆ›å»ºçŸ¥è¯†å›¾è°±æ„å»ºå™¨
builder = KnowledgeTreeGen(dataset_name="demo", ...)

# æ‰§è¡Œæ–‡æ¡£å¤„ç†å’Œåˆæ­¥å»é‡
builder.process_document(doc)

# ã€æ–°å¢ã€‘æ‰§è¡ŒheadèŠ‚ç‚¹å»é‡
stats = builder.deduplicate_heads(
    enable_semantic=True,
    similarity_threshold=0.85,
    use_llm_validation=False,  # å¿«é€Ÿæ¨¡å¼ï¼šä»…ä½¿ç”¨embedding
    max_candidates=1000
)

print(f"Merged {stats['total_merges']} head nodes")
print(f"Reduction rate: {stats['total_merges'] / stats['total_candidates'] * 100:.1f}%")

# éªŒè¯å›¾å®Œæ•´æ€§
issues = builder.validate_graph_integrity_after_head_dedup()
if any(issues.values()):
    print(f"Warning: Found integrity issues: {issues}")
```

### é«˜ç²¾åº¦æ¨¡å¼ï¼ˆä½¿ç”¨LLMéªŒè¯ï¼‰

```python
stats = builder.deduplicate_heads(
    enable_semantic=True,
    similarity_threshold=0.90,  # æ›´ä¸¥æ ¼
    use_llm_validation=True,    # å¯ç”¨LLMäºŒæ¬¡éªŒè¯
    max_candidates=500          # é™åˆ¶LLMè°ƒç”¨æ¬¡æ•°
)
```

### å¯¼å‡ºäººå·¥å®¡æ ¸

```python
builder.export_head_merge_candidates_for_review(
    output_path="output/head_merge_candidates.csv",
    min_confidence=0.70,
    max_confidence=0.90
)
```

---

## åç»­ä¼˜åŒ–æ–¹å‘

### çŸ­æœŸï¼ˆ1-2å‘¨ï¼‰

1. **å®ç°åŸºç¡€ç‰ˆæœ¬**ï¼š
   - ç²¾ç¡®åŒ¹é…å»é‡
   - Embedding-basedè¯­ä¹‰å»é‡
   - å›¾ç»“æ„æ›´æ–°é€»è¾‘

2. **æµ‹è¯•ä¸éªŒè¯**ï¼š
   - å•å…ƒæµ‹è¯•ï¼ˆæ¯ä¸ªç»„ä»¶ï¼‰
   - é›†æˆæµ‹è¯•ï¼ˆå®Œæ•´pipelineï¼‰
   - å°è§„æ¨¡æ•°æ®éªŒè¯

### ä¸­æœŸï¼ˆ1ä¸ªæœˆï¼‰

3. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - å¹¶å‘å¤„ç†ä¼˜åŒ–
   - ç¼“å­˜æœºåˆ¶å®Œå–„
   - å¤§è§„æ¨¡å›¾åˆ†æ‰¹å¤„ç†

4. **å¯è§‚æµ‹æ€§å¢å¼º**ï¼š
   - è¯¦ç»†æ—¥å¿—è®°å½•
   - ä¸­é—´ç»“æœä¿å­˜
   - å¯è§†åŒ–dashboard

### é•¿æœŸï¼ˆ3ä¸ªæœˆ+ï¼‰

5. **é«˜çº§ç‰¹æ€§**ï¼š
   - ä¸»åŠ¨å­¦ä¹ ï¼ˆActive Learningï¼‰ï¼šæ¨¡å‹ä¸ç¡®å®šæ—¶è¯·æ±‚äººå·¥æ ‡æ³¨
   - å¢é‡å»é‡ï¼šæ–°å¢èŠ‚ç‚¹æ—¶å®æ—¶å»é‡
   - è·¨ç¤¾åŒºå»é‡ï¼šè€ƒè™‘communityç»“æ„çš„å…¨å±€å»é‡

6. **é¢†åŸŸé€‚é…**ï¼š
   - é¢†åŸŸç‰¹å®šè§„åˆ™ï¼ˆå¦‚äººåã€åœ°åçš„ç‰¹æ®Šå¤„ç†ï¼‰
   - å¤šè¯­è¨€æ”¯æŒï¼ˆè·¨è¯­è¨€å®ä½“é“¾æ¥ï¼‰
   - æ—¶é—´æ„ŸçŸ¥å»é‡ï¼ˆåŒºåˆ†å†å²å®ä½“vsç°ä»£å®ä½“ï¼‰

---

## æ€»ç»“

æœ¬æ–¹æ¡ˆæä¾›äº†**ä¸“ä¸šã€å®Œæ•´ã€å¯å®æ–½**çš„headèŠ‚ç‚¹å»é‡è§£å†³æ–¹æ¡ˆï¼Œæ ¸å¿ƒç‰¹ç‚¹ï¼š

âœ… **ç†è®ºåŸºç¡€æ‰å®**ï¼šåŸºäºNLPä¸­çš„å®ä½“é“¾æ¥ï¼ˆEntity Linkingï¼‰å’Œå…±æŒ‡æ¶ˆè§£ï¼ˆCoreference Resolutionï¼‰ç†è®º  
âœ… **æ¶æ„è®¾è®¡åˆç†**ï¼šä¸¤é˜¶æ®µå¤„ç†ï¼ˆç²¾ç¡®+è¯­ä¹‰ï¼‰ï¼Œä¸ç°æœ‰tailå»é‡ä¿æŒä¸€è‡´  
âœ… **å®ç°ç»†èŠ‚å®Œæ•´**ï¼šåŒ…å«å®Œæ•´ä»£ç æ¡†æ¶å’Œå…³é”®å‡½æ•°å®ç°  
âœ… **æ•°æ®å®‰å…¨å¯é **ï¼šäº‹åŠ¡æ€§æ“ä½œã€å›æ»šæœºåˆ¶ã€å®Œæ•´æ€§éªŒè¯  
âœ… **æ€§èƒ½å¯æ‰©å±•**ï¼šæ”¯æŒå¤§è§„æ¨¡å›¾çš„æ‰¹å¤„ç†å’Œå¹¶å‘ä¼˜åŒ–  
âœ… **å¯è§‚æµ‹å¯å®¡æ ¸**ï¼šå®Œæ•´æº¯æºä¿¡æ¯ã€äººå·¥å®¡æ ¸æ¥å£  

**å»ºè®®å®æ–½è·¯å¾„**ï¼š
1. å…ˆå®ç°ç²¾ç¡®åŒ¹é…å»é‡ï¼ˆå¿«é€Ÿè§æ•ˆï¼‰
2. å†å®ç°embedding-basedè¯­ä¹‰å»é‡ï¼ˆè¦†ç›–ä¸»è¦åœºæ™¯ï¼‰
3. æœ€åæ ¹æ®éœ€è¦æ·»åŠ LLMéªŒè¯ï¼ˆæå‡ç²¾åº¦ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬å†å²**
- v1.0 (2025-10-27): åˆå§‹ç‰ˆæœ¬ï¼Œå®Œæ•´æ–¹æ¡ˆè®¾è®¡
