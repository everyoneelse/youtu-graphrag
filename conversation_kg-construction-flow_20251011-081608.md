# å¯¹è¯è®°å½•ï¼šçŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹åˆ†æ

**æ—¶é—´**: 2025-10-11 08:16:08  
**ä¸»é¢˜**: Youtu-GraphRAG çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹è§£æ  
**åˆ†æ”¯**: cursor/build-knowledge-graph-with-llm-extraction-585a

---

## ç”¨æˆ·é—®é¢˜

æ‰€ä»¥è¿™ä¸ªrepoæ„å»ºçŸ¥è¯†å›¾è°±çš„æµç¨‹å°±æ˜¯ï¼Œå…ˆåˆ†chunkï¼Ÿç„¶åé€šè¿‡llmæŠ½å–å®ä½“å’Œå…³ç³»ï¼ŸæŠ½å–å®Œæˆä¹‹ååšä»€ä¹ˆï¼Ÿ

---

## å›ç­”æ¦‚è¦

æ ¹æ®ä»£ç åˆ†æï¼Œè¿™ä¸ª repo æ„å»ºçŸ¥è¯†å›¾è°±çš„å®Œæ•´æµç¨‹å¦‚ä¸‹ï¼š

## ğŸ“‹ å®Œæ•´æµç¨‹

### 1ï¸âƒ£ **æ–‡æœ¬åˆ†å—ï¼ˆChunkingï¼‰**

é¦–å…ˆå¯¹è¾“å…¥æ–‡æ¡£è¿›è¡Œåˆ†å—å¤„ç†ï¼š

```python
# models/constructor/kt_gen.py: 43-61
def chunk_text(self, text) -> Tuple[List[str], Dict[str, str]]:
    if self.dataset_name in self.datasets_no_chunk:
        chunks = [f"{text.get('title', '')} {text.get('text', '')}".strip() 
                 if isinstance(text, dict) else str(text)]
    else:
        chunks = [str(text)]

    chunk2id = {}
    for chunk in chunks:
        try:
            chunk_id = nanoid.generate(size=8)
            chunk2id[chunk_id] = chunk
```

æ¯ä¸ª chunk ä¼šè¢«åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ 8 ä½ IDï¼Œç”¨äºåç»­è¿½è¸ªã€‚

---

### 2ï¸âƒ£ **LLM æŠ½å–å®ä½“å’Œå…³ç³»**

é€šè¿‡ LLM API ä»æ¯ä¸ª chunk ä¸­æŠ½å–ä¸‰ç±»ä¿¡æ¯ï¼š

- **å±æ€§ (attributes)**: å®ä½“çš„å±æ€§ä¿¡æ¯
- **ä¸‰å…ƒç»„ (triples)**: `[ä¸»ä½“, å…³ç³», å®¢ä½“]` æ ¼å¼çš„å…³ç³»
- **å®ä½“ç±»å‹ (entity_types)**: æ¯ä¸ªå®ä½“å¯¹åº”çš„ schema ç±»å‹

```python
# models/constructor/kt_gen.py: 245-271
def process_level1_level2(self, chunk: str, id: int):
    """Process attributes (level 1) and triples (level 2) with optimized structure."""
    prompt = self._get_construction_prompt(chunk)
    llm_response = self.extract_with_llm(prompt)
    
    # Validate and parse response
    parsed_response = self._validate_and_parse_llm_response(prompt, llm_response)
    if not parsed_response:
        return
    
    extracted_attr = parsed_response.get("attributes", {})
    extracted_triples = parsed_response.get("triples", [])
    entity_types = parsed_response.get("entity_types", {})
```

**æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š**
- **NoAgent æ¨¡å¼**: æ ‡å‡†æŠ½å–æµç¨‹
- **Agent æ¨¡å¼**: æ”¯æŒåŠ¨æ€ Schema æ¼”åŒ–ï¼ŒLLM å¯ä»¥å»ºè®®æ–°çš„å®ä½“ç±»å‹ã€å…³ç³»ç±»å‹å’Œå±æ€§ç±»å‹

---

### 3ï¸âƒ£ **æŠ½å–å®Œæˆåï¼šæ„å»ºå››å±‚çŸ¥è¯†æ ‘**

è¿™æ˜¯è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°ï¼æŠ½å–å®Œæˆåä¼šæ„å»ºä¸€ä¸ª**å››å±‚å±‚æ¬¡åŒ–çŸ¥è¯†æ ‘ç»“æ„**ï¼š

#### **Level 1 - å±æ€§å±‚ (Attribute Layer)**
- å­˜å‚¨å®ä½“çš„å±æ€§èŠ‚ç‚¹
- é€šè¿‡ `has_attribute` å…³ç³»è¿æ¥åˆ°å¯¹åº”å®ä½“
- ä¾‹å¦‚: äººç‰© â†’ å¹´é¾„ã€èŒä¸šç­‰å±æ€§

#### **Level 2 - å…³ç³»å±‚ (Entity & Relation Layer)**
- æ„å»ºå®ä½“é—´çš„ä¸‰å…ƒç»„å…³ç³»ç½‘ç»œ
- å½¢æˆçŸ¥è¯†å›¾è°±çš„æ ¸å¿ƒç»“æ„
- ä¾‹å¦‚: `[å¼ ä¸‰, å·¥ä½œäº, è…¾è®¯]`

#### **Level 3 & 4 - å…³é”®è¯å±‚å’Œç¤¾åŒºå±‚ (Community Layer)**
ä½¿ç”¨ **Tree-Comm ç®—æ³•**è¿›è¡Œç¤¾åŒºæ£€æµ‹å’Œå±‚æ¬¡åŒ–ç»„ç»‡ï¼š

```python
# models/constructor/kt_gen.py: 423-440
def process_level4(self):
    """Process communities using Tree-Comm algorithm"""
    level2_nodes = [n for n, d in self.graph.nodes(data=True) if d['level'] == 2]
    start_comm = time.time()
    _tree_comm = tree_comm.FastTreeComm(
        self.graph, 
        embedding_model=self.config.tree_comm.embedding_model,
        struct_weight=self.config.tree_comm.struct_weight,
    )
    comm_to_nodes = _tree_comm.detect_communities(level2_nodes)

    # create super nodes (level 4 communities)
    _tree_comm.create_super_nodes_with_keywords(comm_to_nodes, level=4)
```

**Tree-Comm ç®—æ³•ç‰¹ç‚¹**ï¼š
- ç»“åˆ**ç»“æ„æ‹“æ‰‘ç‰¹å¾**å’Œ**å­å›¾è¯­ä¹‰ä¿¡æ¯**
- æ¯”ä¼ ç»Ÿ Leiden å’Œ Louvain ç®—æ³•æ•ˆæœæ›´å¥½
- ç”Ÿæˆå±‚æ¬¡åŒ–ç¤¾åŒºæ‘˜è¦ï¼Œæ”¯æŒå¤šç²’åº¦æ£€ç´¢

---

### 4ï¸âƒ£ **ä¸‰å…ƒç»„å»é‡**

å¯¹ Level 1 å’Œ Level 2 çš„ä¸‰å…ƒç»„è¿›è¡Œå»é‡å¤„ç†ï¼š

```python
# models/constructor/kt_gen.py: 537-550
def triple_deduplicate(self):
    """deduplicate triples in lv1 and lv2"""
    new_graph = nx.MultiDiGraph()

    for node, node_data in self.graph.nodes(data=True):
        new_graph.add_node(node, **node_data)

    seen_triples = set()
    for u, v, key, data in self.graph.edges(keys=True, data=True):
        relation = data.get('relation') 
        if (u, v, relation) not in seen_triples:
            seen_triples.add((u, v, relation))
            new_graph.add_edge(u, v, **data)
```

ç¡®ä¿å›¾ä¸­ä¸ä¼šæœ‰é‡å¤çš„å…³ç³»ä¸‰å…ƒç»„ã€‚

---

### 5ï¸âƒ£ **ä¿å­˜è¾“å‡º**

æ„å»ºå®Œæˆåä¿å­˜ä¸¤ç±»æ–‡ä»¶ï¼š

- **Chunks æ–‡ä»¶**: `output/chunks/{dataset}.txt`
  - æ ¼å¼: `id: {chunk_id}\tChunk: {chunk_text}`
  - ç”¨äºåç»­æ£€ç´¢æ—¶å…³è”åŸå§‹æ–‡æœ¬

- **Graph æ–‡ä»¶**: `output/graphs/{dataset}_new.json`
  - JSON æ ¼å¼å­˜å‚¨å››å±‚çŸ¥è¯†æ ‘ç»“æ„
  - å¯ç›´æ¥å¯¼å…¥ Neo4j è¿›è¡Œå¯è§†åŒ–

```python
# models/constructor/kt_gen.py: 578-599
def build_knowledge_graph(self, corpus):
    logger.info(f"========{'Start Building':^20}========")
    
    with open(corpus, 'r', encoding='utf-8') as f:
        documents = json_repair.load(f)
    
    self.process_all_documents(documents)
    
    self.save_chunks_to_file()
    
    output = self.format_output()
    
    json_output_path = f"output/graphs/{self.dataset_name}_new.json"
    os.makedirs("output/graphs", exist_ok=True)
    with open(json_output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
```

---

### 6ï¸âƒ£ **æ„å»º FAISS ç´¢å¼•**ï¼ˆæ£€ç´¢é˜¶æ®µï¼‰

å›¾è°±æ„å»ºå®Œæˆåï¼Œè¿›å…¥æ£€ç´¢é˜¶æ®µï¼Œé¦–å…ˆæ„å»ºå‘é‡ç´¢å¼•ï¼š

```python
# main.py: 185-189
logger.info("ğŸš€ Building FAISS index ğŸš€")
logger.info("-"*30)
start_time = time.time()
kt_retriever.build_indices()
logger.info(f"Time taken to build FAISS index: {time.time() - start_time} seconds")
```

**ç´¢å¼•å†…å®¹åŒ…æ‹¬**ï¼š
- **èŠ‚ç‚¹ï¼ˆå®ä½“ï¼‰**å‘é‡
- **å…³ç³»ï¼ˆä¸‰å…ƒç»„ï¼‰**å‘é‡
- **Chunks** å‘é‡
- **ç¤¾åŒºæ‘˜è¦**å‘é‡

è¿™äº›ç´¢å¼•è¢«ç¼“å­˜åˆ° `retriever/faiss_cache_new/{dataset}/` ç›®å½•ã€‚

---

### 7ï¸âƒ£ **æ£€ç´¢å’Œæ¨ç†**

æ¡†æ¶æ”¯æŒä¸¤ç§æ£€ç´¢æ¨ç†æ¨¡å¼ï¼š

#### **NoAgent æ¨¡å¼**
åŸºç¡€æ£€ç´¢æ¨¡å¼ï¼Œæµç¨‹å¦‚ä¸‹ï¼š
1. é—®é¢˜åˆ†è§£ â†’ å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜
2. å¹¶è¡Œæ£€ç´¢ â†’ åŒæ—¶å¤„ç†æ‰€æœ‰å­é—®é¢˜çš„æ£€ç´¢
3. èšåˆç»“æœ â†’ åˆå¹¶æ‰€æœ‰æ£€ç´¢åˆ°çš„ä¸‰å…ƒç»„å’Œ chunks
4. ç”Ÿæˆç­”æ¡ˆ â†’ åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

#### **Agent æ¨¡å¼ (IRCoT - Iterative Retrieval Chain of Thought)**
è¿­ä»£æ¨ç†æ¨¡å¼ï¼Œæ›´åŠ æ™ºèƒ½ï¼š

```python
# main.py: 406-469
logger.info(f"ğŸš€ Starting IRCoT for question: {current_query}")

while step <= max_steps:
    logger.info(f"ğŸ“ IRCoT Step {step}/{max_steps}")
    
    # æ„å»ºå½“å‰ä¸Šä¸‹æ–‡
    context = "=== Triples ===\n" + "\n".join(dedup_triples)
    context += "\n=== Chunks ===\n" + "\n".join(dedup_chunk_contents)
    
    # ç”Ÿæˆæ¨ç†å’Œæ–°æŸ¥è¯¢
    response = kt_retriever.generate_answer(ircot_prompt)
    thoughts.append(response)
    
    # åˆ¤æ–­æ˜¯å¦å·²æ‰¾åˆ°ç­”æ¡ˆ
    if "So the answer is:" in response:
        logger.info("âœ… Final answer found, stopping IRCoT")
        break

    # æå–æ–°çš„æŸ¥è¯¢å¹¶ç»§ç»­æ£€ç´¢
    if "The new query is:" in response:
        new_query = response.split("The new query is:")[1].strip()
        current_query = new_query
        
        # æ‰§è¡Œæ–°ä¸€è½®æ£€ç´¢
        retrieval_results, time_taken = kt_retriever.process_retrieval_results(
            current_query, top_k=config.retrieval.top_k_filter
        )
        
        # æ›´æ–°çŸ¥è¯†åº“
        all_triples.update(new_triples)
        all_chunk_ids.update(new_chunk_ids)
    
    step += 1
```

**Agent æ¨¡å¼ç‰¹ç‚¹**ï¼š
- å…ˆè¿è¡Œ NoAgent æ¨¡å¼è·å¾—åˆå§‹åˆ†æ
- åŸºäºåˆå§‹ç»“æœè¿›è¡Œè¿­ä»£æ¨ç†
- æ¯æ­¥å¯ä»¥ç”Ÿæˆæ–°çš„æŸ¥è¯¢è¿›è¡Œè¡¥å……æ£€ç´¢
- æä¾›å®Œæ•´çš„æ¨ç†é“¾è·¯ï¼ˆæ€ç»´è¿‡ç¨‹å¯è§ï¼‰

---

## ğŸ¯ æ ¸å¿ƒåˆ›æ–°ç‚¹æ€»ç»“

1. **å››å±‚çŸ¥è¯†æ ‘**: å±æ€§â†’å®ä½“å…³ç³»â†’å…³é”®è¯â†’ç¤¾åŒºï¼Œæ”¯æŒå¤šç²’åº¦æ£€ç´¢
   - Level 1: å±æ€§å±‚
   - Level 2: å®ä½“å…³ç³»å±‚
   - Level 3: å…³é”®è¯å±‚
   - Level 4: ç¤¾åŒºå±‚

2. **Tree-Comm ç®—æ³•**: ç»“æ„+è¯­ä¹‰åŒé‡æ„ŸçŸ¥çš„ç¤¾åŒºæ£€æµ‹ï¼Œä¼˜äºä¼ ç»Ÿç®—æ³•

3. **Schema æ¼”åŒ–**: Agent æ¨¡å¼æ”¯æŒåŠ¨æ€æ‰©å±• schemaï¼Œæ— éœ€äººå·¥å¹²é¢„å³å¯é€‚åº”æ–°é¢†åŸŸ

4. **è¿­ä»£æ£€ç´¢ (IRCoT)**: é€šè¿‡è¿­ä»£åæ€æœºåˆ¶å®ç°æ·±åº¦æ¨ç†ï¼Œæ˜¾è‘—æå‡å¤æ‚é—®é¢˜çš„å›ç­”ç²¾åº¦

5. **å¹¶è¡Œå¤„ç†**: æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ–‡æ¡£å’Œå­é—®é¢˜ï¼Œæé«˜æ•ˆç‡

6. **å‘é‡ç´¢å¼•**: å¯¹æ‰€æœ‰å±‚çº§çš„å†…å®¹å»ºç«‹ FAISS ç´¢å¼•ï¼Œå®ç°é«˜æ•ˆæ£€ç´¢

---

## ğŸ“‚ å…³é”®æ–‡ä»¶è¯´æ˜

- **`models/constructor/kt_gen.py`**: KTBuilder - çŸ¥è¯†å›¾è°±æ„å»ºæ ¸å¿ƒç±»
- **`models/retriever/enhanced_kt_retriever.py`**: KTRetriever - æ£€ç´¢æ ¸å¿ƒç±»
- **`models/retriever/agentic_decomposer.py`**: GraphQ - é—®é¢˜åˆ†è§£æ¨¡å—
- **`models/retriever/faiss_filter.py`**: DualFAISSRetriever - FAISS æ£€ç´¢å™¨
- **`utils/tree_comm.py`**: FastTreeComm - ç¤¾åŒºæ£€æµ‹ç®—æ³•å®ç°
- **`utils/graph_processor.py`**: å›¾çš„åŠ è½½ã€ä¿å­˜å’Œå¤„ç†å·¥å…·
- **`main.py`**: ä¸»ç¨‹åºå…¥å£ï¼ŒåŒ…å«å®Œæ•´çš„æ„å»ºå’Œæ£€ç´¢æµç¨‹

---

## ğŸ”„ å®Œæ•´æµç¨‹å›¾

```
åŸå§‹æ–‡æ¡£
   â†“
[1] æ–‡æœ¬åˆ†å— (Chunking)
   â†“
[2] LLM æŠ½å– (å±æ€§ + ä¸‰å…ƒç»„ + å®ä½“ç±»å‹)
   â†“
[3] æ„å»ºå››å±‚çŸ¥è¯†æ ‘
   â”œâ”€â”€ Level 1: å±æ€§å±‚
   â”œâ”€â”€ Level 2: å®ä½“å…³ç³»å±‚
   â”œâ”€â”€ Level 3: å…³é”®è¯å±‚
   â””â”€â”€ Level 4: ç¤¾åŒºå±‚ (Tree-Comm)
   â†“
[4] ä¸‰å…ƒç»„å»é‡
   â†“
[5] ä¿å­˜è¾“å‡º
   â”œâ”€â”€ output/chunks/{dataset}.txt
   â””â”€â”€ output/graphs/{dataset}_new.json
   â†“
[6] æ„å»º FAISS ç´¢å¼•
   â”œâ”€â”€ èŠ‚ç‚¹å‘é‡ç´¢å¼•
   â”œâ”€â”€ å…³ç³»å‘é‡ç´¢å¼•
   â”œâ”€â”€ Chunk å‘é‡ç´¢å¼•
   â””â”€â”€ ç¤¾åŒºå‘é‡ç´¢å¼•
   â†“
[7] æ£€ç´¢æ¨ç†
   â”œâ”€â”€ NoAgent: é—®é¢˜åˆ†è§£ â†’ å¹¶è¡Œæ£€ç´¢ â†’ ç”Ÿæˆç­”æ¡ˆ
   â””â”€â”€ Agent: NoAgent + IRCoT è¿­ä»£æ¨ç†
```

---

## ğŸ’¡ æ€§èƒ½ä¼˜åŠ¿

æ ¹æ®è®ºæ–‡å®éªŒç»“æœï¼š
- **Token æˆæœ¬èŠ‚çº¦**: æœ€é«˜è¾¾ 90.71%
- **ç²¾åº¦æå‡**: æœ€é«˜è¾¾ 16.62%
- **å¸•ç´¯æ‰˜å‰æ²¿**: åœ¨æˆæœ¬å’Œç²¾åº¦ä¹‹é—´å®ç°æœ€ä¼˜å¹³è¡¡
- **è·¨åŸŸè¿ç§»**: åœ¨ Schema æœ€å°äººä¸ºå¹²é¢„ä¸‹å®ç°è·¨é¢†åŸŸåº”ç”¨

---

**å¯¹è¯ç»“æŸ**
