#!/usr/bin/env python3
"""
æ£€æŸ¥æ•°æ®é›†é…ç½®å·¥å…·
ç”¨äºæ£€æŸ¥æ‰‹åŠ¨æ·»åŠ çš„graphæ–‡ä»¶é…ç½®æ˜¯å¦æ­£ç¡®
"""

import os
import json
import sys

def check_dataset_files(dataset_name):
    """æ£€æŸ¥æ•°æ®é›†ç›¸å…³æ–‡ä»¶"""
    
    print(f"\n{'='*60}")
    print(f"æ£€æŸ¥æ•°æ®é›†: {dataset_name}")
    print(f"{'='*60}\n")
    
    files_status = {
        'graph': None,
        'chunks': None,
        'corpus': None,
        'cache': None
    }
    
    # æ£€æŸ¥ graph æ–‡ä»¶
    graph_path = f"output/graphs/{dataset_name}_new.json"
    if os.path.exists(graph_path):
        size = os.path.getsize(graph_path)
        files_status['graph'] = {
            'path': graph_path,
            'size': size,
            'size_mb': f"{size / 1024 / 1024:.2f} MB"
        }
        try:
            with open(graph_path, 'r', encoding='utf-8') as f:
                graph_data = json.load(f)
                if isinstance(graph_data, list):
                    files_status['graph']['nodes'] = 'æœªçŸ¥ï¼ˆå…³ç³»åˆ—è¡¨æ ¼å¼ï¼‰'
                    files_status['graph']['edges'] = len(graph_data)
                elif isinstance(graph_data, dict):
                    files_status['graph']['nodes'] = len(graph_data.get('nodes', []))
                    files_status['graph']['edges'] = len(graph_data.get('edges', []))
        except Exception as e:
            files_status['graph']['error'] = str(e)
    
    # æ£€æŸ¥ chunks æ–‡ä»¶
    chunks_path = f"output/chunks/{dataset_name}.txt"
    if os.path.exists(chunks_path):
        size = os.path.getsize(chunks_path)
        files_status['chunks'] = {
            'path': chunks_path,
            'size': size,
            'size_mb': f"{size / 1024 / 1024:.2f} MB"
        }
        try:
            with open(chunks_path, 'r', encoding='utf-8') as f:
                chunk_count = sum(1 for line in f if line.strip() and line.startswith('id:'))
            files_status['chunks']['count'] = chunk_count
        except Exception as e:
            files_status['chunks']['error'] = str(e)
    
    # æ£€æŸ¥ corpus æ–‡ä»¶
    corpus_path = f"data/uploaded/{dataset_name}/corpus.json"
    if os.path.exists(corpus_path):
        size = os.path.getsize(corpus_path)
        files_status['corpus'] = {
            'path': corpus_path,
            'size': size,
            'size_mb': f"{size / 1024 / 1024:.2f} MB"
        }
        try:
            with open(corpus_path, 'r', encoding='utf-8') as f:
                corpus_data = json.load(f)
                if isinstance(corpus_data, list):
                    files_status['corpus']['count'] = len(corpus_data)
        except Exception as e:
            files_status['corpus']['error'] = str(e)
    
    # æ£€æŸ¥ cache ç›®å½•
    cache_dir = f"retriever/faiss_cache_new/{dataset_name}"
    if os.path.exists(cache_dir):
        cache_files = os.listdir(cache_dir)
        files_status['cache'] = {
            'path': cache_dir,
            'files': len(cache_files),
            'file_list': cache_files
        }
    
    # æ‰“å°ç»“æœ
    print("ğŸ“Š æ–‡ä»¶æ£€æŸ¥ç»“æœ:")
    print("-" * 60)
    
    # Graph æ–‡ä»¶
    if files_status['graph']:
        print(f"âœ… Graph æ–‡ä»¶: {files_status['graph']['path']}")
        print(f"   å¤§å°: {files_status['graph']['size_mb']}")
        if 'nodes' in files_status['graph']:
            print(f"   èŠ‚ç‚¹æ•°: {files_status['graph']['nodes']}")
            print(f"   è¾¹æ•°: {files_status['graph']['edges']}")
        if 'error' in files_status['graph']:
            print(f"   âš ï¸  è§£æé”™è¯¯: {files_status['graph']['error']}")
    else:
        print(f"âŒ Graph æ–‡ä»¶: ä¸å­˜åœ¨")
        print(f"   é¢„æœŸè·¯å¾„: output/graphs/{dataset_name}_new.json")
    
    print()
    
    # Chunks æ–‡ä»¶
    if files_status['chunks']:
        print(f"âœ… Chunks æ–‡ä»¶: {files_status['chunks']['path']}")
        print(f"   å¤§å°: {files_status['chunks']['size_mb']}")
        if 'count' in files_status['chunks']:
            print(f"   ç‰‡æ®µæ•°: {files_status['chunks']['count']}")
        if 'error' in files_status['chunks']:
            print(f"   âš ï¸  è§£æé”™è¯¯: {files_status['chunks']['error']}")
    else:
        print(f"âš ï¸  Chunks æ–‡ä»¶: ä¸å­˜åœ¨ï¼ˆå¯é€‰ï¼‰")
        print(f"   é¢„æœŸè·¯å¾„: output/chunks/{dataset_name}.txt")
        print(f"   å½±å“: æ— æ³•æ£€ç´¢åŸå§‹æ–‡æ¡£ç‰‡æ®µ")
    
    print()
    
    # Corpus æ–‡ä»¶
    if files_status['corpus']:
        print(f"âœ… Corpus æ–‡ä»¶: {files_status['corpus']['path']}")
        print(f"   å¤§å°: {files_status['corpus']['size_mb']}")
        if 'count' in files_status['corpus']:
            print(f"   æ–‡æ¡£æ•°: {files_status['corpus']['count']}")
        if 'error' in files_status['corpus']:
            print(f"   âš ï¸  è§£æé”™è¯¯: {files_status['corpus']['error']}")
    else:
        print(f"â„¹ï¸  Corpus æ–‡ä»¶: ä¸å­˜åœ¨ï¼ˆå¯é€‰ï¼‰")
        print(f"   é¢„æœŸè·¯å¾„: data/uploaded/{dataset_name}/corpus.json")
        print(f"   å½±å“: æ— æ³•é‡æ–°æ„å»ºå›¾è°±")
    
    print()
    
    # Cache ç›®å½•
    if files_status['cache']:
        print(f"âœ… Cache ç›®å½•: {files_status['cache']['path']}")
        print(f"   æ–‡ä»¶æ•°: {files_status['cache']['files']}")
        print(f"   æ–‡ä»¶åˆ—è¡¨: {', '.join(files_status['cache']['file_list'][:5])}")
        if len(files_status['cache']['file_list']) > 5:
            print(f"             ...ç­‰{len(files_status['cache']['file_list']) - 5}ä¸ªæ–‡ä»¶")
    else:
        print(f"â„¹ï¸  Cache ç›®å½•: ä¸å­˜åœ¨ï¼ˆé¦–æ¬¡æŸ¥è¯¢æ—¶è‡ªåŠ¨ç”Ÿæˆï¼‰")
        print(f"   é¢„æœŸè·¯å¾„: retriever/faiss_cache_new/{dataset_name}/")
    
    print()
    print("-" * 60)
    
    # åŠŸèƒ½æ”¯æŒæƒ…å†µ
    print("\nğŸ”§ åŠŸèƒ½æ”¯æŒæƒ…å†µ:")
    print("-" * 60)
    
    support_status = {
        'å›¾è°±å¯è§†åŒ–': files_status['graph'] is not None,
        'åŸºç¡€é—®ç­”': files_status['graph'] is not None,
        'å®Œæ•´é—®ç­”ï¼ˆå«åŸæ–‡ç‰‡æ®µï¼‰': files_status['graph'] is not None and files_status['chunks'] is not None,
        'é‡æ–°æ„å»ºå›¾è°±': files_status['corpus'] is not None,
        'åˆ é™¤æ•°æ®é›†': True  # æ€»æ˜¯å¯ä»¥åˆ é™¤
    }
    
    for feature, supported in support_status.items():
        status = "âœ… æ”¯æŒ" if supported else "âŒ ä¸æ”¯æŒ"
        print(f"{status} - {feature}")
    
    # æ•°æ®é›†ç±»å‹åˆ¤æ–­
    print("\nğŸ“ æ•°æ®é›†ç±»å‹:")
    print("-" * 60)
    
    if dataset_name == "demo":
        dataset_type = "demoï¼ˆç¤ºä¾‹æ•°æ®é›†ï¼‰"
    elif files_status['corpus']:
        dataset_type = "uploadedï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰"
    else:
        dataset_type = "customï¼ˆè‡ªå®šä¹‰graphï¼‰"
    
    print(f"ç±»å‹: {dataset_type}")
    
    # å»ºè®®
    print("\nğŸ’¡ å»ºè®®:")
    print("-" * 60)
    
    if not files_status['graph']:
        print("âŒ ç¼ºå°‘graphæ–‡ä»¶ï¼Œæ•°æ®é›†æ— æ³•ä½¿ç”¨ï¼")
        print(f"   è¯·å°†graphæ–‡ä»¶æ‹·è´åˆ°: output/graphs/{dataset_name}_new.json")
    elif not files_status['chunks']:
        print("âš ï¸  å»ºè®®æ·»åŠ chunksæ–‡ä»¶ä»¥è·å¾—æ›´å¥½çš„é—®ç­”ä½“éªŒ")
        print(f"   chunksæ–‡ä»¶è·¯å¾„: output/chunks/{dataset_name}.txt")
        print("   æ ¼å¼: id: chunk_id\\tChunk: chunk_text")
    elif not files_status['corpus']:
        print("â„¹ï¸  å¦‚éœ€æ”¯æŒé‡æ–°æ„å»ºåŠŸèƒ½ï¼Œå¯æ·»åŠ corpusæ–‡ä»¶")
        print(f"   corpusæ–‡ä»¶è·¯å¾„: data/uploaded/{dataset_name}/corpus.json")
    else:
        print("âœ… æ•°æ®é›†é…ç½®å®Œæ•´ï¼Œæ‰€æœ‰åŠŸèƒ½å‡å¯ä½¿ç”¨ï¼")
    
    if files_status['cache']:
        print("\nâ„¹ï¸  æ£€æµ‹åˆ°ç´¢å¼•ç¼“å­˜ï¼ŒæŸ¥è¯¢é€Ÿåº¦ä¼šå¾ˆå¿«")
    else:
        print("\nâ„¹ï¸  é¦–æ¬¡æŸ¥è¯¢æ—¶ä¼šè‡ªåŠ¨æ„å»ºç´¢å¼•ï¼ˆçº¦éœ€10-60ç§’ï¼‰")
    
    print("\n" + "="*60 + "\n")
    
    return files_status

def list_all_datasets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
    print("\n" + "="*60)
    print("æ‰«ææ‰€æœ‰æ•°æ®é›†")
    print("="*60 + "\n")
    
    datasets = set()
    
    # æ‰«æ graphs ç›®å½•
    graphs_dir = "output/graphs"
    if os.path.exists(graphs_dir):
        for filename in os.listdir(graphs_dir):
            if filename.endswith("_new.json"):
                dataset_name = filename[:-9]
                datasets.add(dataset_name)
    
    # æ‰«æ uploaded ç›®å½•
    uploaded_dir = "data/uploaded"
    if os.path.exists(uploaded_dir):
        for item in os.listdir(uploaded_dir):
            item_path = os.path.join(uploaded_dir, item)
            if os.path.isdir(item_path):
                corpus_path = os.path.join(item_path, "corpus.json")
                if os.path.exists(corpus_path):
                    datasets.add(item)
    
    if not datasets:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†")
        print("\nè¯·å‚è€ƒæ–‡æ¡£æ·»åŠ æ•°æ®é›†ï¼š")
        print("  - å°†graphæ–‡ä»¶æ‹·è´åˆ°: output/graphs/{æ•°æ®é›†åç§°}_new.json")
        return
    
    print(f"æ‰¾åˆ° {len(datasets)} ä¸ªæ•°æ®é›†:\n")
    
    for i, dataset in enumerate(sorted(datasets), 1):
        print(f"{i}. {dataset}")
    
    print("\nä½¿ç”¨æ–¹æ³•:")
    print(f"  python3 {sys.argv[0]} <æ•°æ®é›†åç§°>")
    print(f"\nç¤ºä¾‹:")
    print(f"  python3 {sys.argv[0]} demo")
    print(f"  python3 {sys.argv[0]} my_custom_graph")
    
    print("\n" + "="*60 + "\n")

def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•:")
        print(f"  python3 {sys.argv[0]} <æ•°æ®é›†åç§°>     # æ£€æŸ¥æŒ‡å®šæ•°æ®é›†")
        print(f"  python3 {sys.argv[0]} --list          # åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†")
        print(f"\nç¤ºä¾‹:")
        print(f"  python3 {sys.argv[0]} demo")
        print(f"  python3 {sys.argv[0]} my_custom_graph")
        print(f"  python3 {sys.argv[0]} --list")
        sys.exit(1)
    
    if sys.argv[1] in ['--list', '-l', 'list']:
        list_all_datasets()
    else:
        dataset_name = sys.argv[1]
        check_dataset_files(dataset_name)

if __name__ == "__main__":
    main()
