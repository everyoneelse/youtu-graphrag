#!/usr/bin/env python3
"""
ç¤ºä¾‹ï¼šå¦‚ä½•ä½¿ç”¨è¿˜åŸçš„semantic_resultsè·³è¿‡LLMè°ƒç”¨

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä¿®æ”¹triple_deduplicate_semanticä»¥ä½¿ç”¨ç¼“å­˜çš„ç»“æœ
"""

import pickle
import json
from typing import List, Dict, Any


class ExampleUsage:
    """å±•ç¤ºå¦‚ä½•ä½¿ç”¨è¿˜åŸçš„semantic_results"""
    
    def __init__(self, restored_results_file: str = None):
        """
        Args:
            restored_results_file: è¿˜åŸçš„semantic_results pickleæ–‡ä»¶è·¯å¾„
        """
        self.restored_results_file = restored_results_file
        self.cached_semantic_results = None
        
        if restored_results_file:
            print(f"Loading cached semantic_results from: {restored_results_file}")
            with open(restored_results_file, 'rb') as f:
                self.cached_semantic_results = pickle.load(f)
            print(f"âœ… Loaded {len(self.cached_semantic_results)} cached results")
    
    def triple_deduplicate_semantic_modified(self):
        """
        ä¿®æ”¹åçš„triple_deduplicate_semanticï¼Œæ”¯æŒä½¿ç”¨ç¼“å­˜çš„semantic_results
        
        è¿™æ˜¯åŸå§‹æ–¹æ³•çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå±•ç¤ºå…³é”®ä¿®æ”¹ç‚¹
        """
        # ... å‰é¢çš„ä»£ç ï¼ˆPHASE 1, PHASE 2ï¼‰ä¿æŒä¸å˜ ...
        
        # ================================================================
        # PHASE 3: Batch collect and process semantic dedup prompts
        # ================================================================
        print("Collecting all semantic dedup prompts...")
        semantic_prompts = []
        
        # æ”¶é›†æ‰€æœ‰promptsï¼ˆå³ä½¿ä½¿ç”¨ç¼“å­˜ä¹Ÿéœ€è¦è¿™ä¸€æ­¥æ¥å»ºç«‹ç´¢å¼•ï¼‰
        for group_idx, group_data in enumerate(dedup_groups):
            prompts = self._collect_semantic_dedup_prompts(group_data)
            for prompt_data in prompts:
                prompt_data['metadata']['group_idx'] = group_idx
                semantic_prompts.append(prompt_data)
        
        print(f"Collected {len(semantic_prompts)} semantic dedup prompts")
        
        # ============================================================
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ç¼“å­˜çš„semantic_resultsæˆ–è°ƒç”¨LLM
        # ============================================================
        if self.cached_semantic_results is not None:
            print("ğŸš€ Using cached semantic_results, skipping LLM calls!")
            print(f"ğŸ’° Saving tokens for {len(semantic_prompts)} prompts")
            semantic_results = self.cached_semantic_results
        else:
            print("Calling LLM for semantic deduplication...")
            semantic_results = self._concurrent_llm_calls(semantic_prompts)
        
        # Parse semantic dedup results and update group_data
        print("Parsing semantic dedup results...")
        self._parse_semantic_dedup_results(dedup_groups, semantic_results)
        
        # ================================================================
        # PHASE 4: Build final deduplicated edges
        # ================================================================
        # ... åé¢çš„ä»£ç ä¿æŒä¸å˜ ...


def demo_usage():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨"""
    
    print("=" * 70)
    print("æ–¹æ¡ˆ1ï¼šåœ¨ä»£ç ä¸­ç›´æ¥åŠ è½½ç¼“å­˜çš„ç»“æœ")
    print("=" * 70)
    print("""
# åœ¨kt_gen.pyçš„triple_deduplicate_semanticæ–¹æ³•ä¸­ï¼š

def triple_deduplicate_semantic(self, use_cached_results: str = None):
    '''
    Args:
        use_cached_results: ç¼“å­˜çš„semantic_results pickleæ–‡ä»¶è·¯å¾„
    '''
    config = self._get_semantic_dedup_config()
    save_intermediate = config and getattr(config, "save_intermediate_results", False)
    if save_intermediate:
        self._edge_dedup_results = []
    
    # ... PHASE 1 & 2 ä»£ç  ...
    
    # PHASE 3: æ”¶é›†prompts
    semantic_prompts = []
    for group_idx, group_data in enumerate(dedup_groups):
        prompts = self._collect_semantic_dedup_prompts(group_data)
        for prompt_data in prompts:
            prompt_data['metadata']['group_idx'] = group_idx
            semantic_prompts.append(prompt_data)
    
    # ğŸ”¥ ä½¿ç”¨ç¼“å­˜æˆ–è°ƒç”¨LLM
    if use_cached_results:
        logger.info(f"Using cached results from: {use_cached_results}")
        import pickle
        with open(use_cached_results, 'rb') as f:
            semantic_results = pickle.load(f)
        logger.info(f"Loaded {len(semantic_results)} cached results")
    else:
        logger.info(f"Collected {len(semantic_prompts)} semantic dedup prompts, processing concurrently...")
        semantic_results = self._concurrent_llm_calls(semantic_prompts)
    
    # Parseç»“æœ
    self._parse_semantic_dedup_results(dedup_groups, semantic_results)
    
    # ... PHASE 4 ä»£ç  ...
    """)
    
    print("\n" + "=" * 70)
    print("æ–¹æ¡ˆ2ï¼šé€šè¿‡é…ç½®æ–‡ä»¶æŒ‡å®šç¼“å­˜")
    print("=" * 70)
    print("""
# åœ¨config/semantic_dedup.yamlä¸­æ·»åŠ ï¼š

semantic_dedup:
  # ... å…¶ä»–é…ç½® ...
  
  # ä½¿ç”¨ç¼“å­˜çš„semantic_resultsï¼ˆå¦‚æœæŒ‡å®šï¼Œå°†è·³è¿‡LLMè°ƒç”¨ï¼‰
  cached_results_path: "output/dedup_intermediate/demo_semantic_results_20241023_123456.pkl"
    """)
    
    print("\n" + "=" * 70)
    print("æ–¹æ¡ˆ3ï¼šå‘½ä»¤è¡Œå‚æ•°")
    print("=" * 70)
    print("""
# ä¿®æ”¹main.pyæ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼š

parser.add_argument(
    '--use-cached-semantic-results',
    type=str,
    default=None,
    help='Path to cached semantic_results pickle file to skip LLM calls'
)

# ç„¶ååœ¨æ„å»ºKnowledgeGraphæ—¶ä¼ å…¥ï¼š
kg = KnowledgeGraph(
    dataset_name=args.dataset,
    cached_semantic_results=args.use_cached_semantic_results
)
    """)
    
    print("\n" + "=" * 70)
    print("å®Œæ•´å·¥ä½œæµç¨‹")
    print("=" * 70)
    print("""
æ­¥éª¤1ï¼šé¦–æ¬¡è¿è¡Œï¼Œä¿å­˜ä¸­é—´ç»“æœ
---------------------------------
# è®¾ç½®é…ç½®æ–‡ä»¶ä¸­çš„ save_intermediate_results: true
python main.py --dataset demo --config config/semantic_dedup.yaml

# è¾“å‡ºï¼šoutput/dedup_intermediate/demo_edge_dedup_20241023_123456.json


æ­¥éª¤2ï¼šä»ä¸­é—´ç»“æœè¿˜åŸsemantic_results
------------------------------------
python restore_semantic_results.py \\
    output/dedup_intermediate/demo_edge_dedup_20241023_123456.json

# è¾“å‡ºï¼š
#   output/dedup_intermediate/demo_semantic_results_20241023_123456.pkl
#   output/dedup_intermediate/demo_semantic_results_20241023_123456.json


æ­¥éª¤3ï¼šä½¿ç”¨ç¼“å­˜çš„ç»“æœé‡æ–°è¿è¡Œï¼ˆæ— éœ€è°ƒç”¨LLMï¼‰
------------------------------------------
python main.py --dataset demo \\
    --use-cached-semantic-results \\
    output/dedup_intermediate/demo_semantic_results_20241023_123456.pkl

# ğŸš€ è¿™æ¬¡è¿è¡Œä¼šè·³è¿‡æ‰€æœ‰semantic dedupçš„LLMè°ƒç”¨ï¼
# ğŸ’° èŠ‚çœå¤§é‡tokensæˆæœ¬ï¼
    """)


def validate_restored_results(intermediate_file: str, restored_file: str):
    """éªŒè¯è¿˜åŸçš„ç»“æœæ˜¯å¦æ­£ç¡®"""
    print("\n" + "=" * 70)
    print("éªŒè¯è¿˜åŸçš„ç»“æœ")
    print("=" * 70)
    
    # åŠ è½½ä¸­é—´ç»“æœ
    with open(intermediate_file, 'r', encoding='utf-8') as f:
        intermediate = json.load(f)
    
    # åŠ è½½è¿˜åŸçš„ç»“æœ
    with open(restored_file, 'rb') as f:
        restored = pickle.load(f)
    
    print(f"\nä¸­é—´ç»“æœæ–‡ä»¶ï¼š{intermediate_file}")
    print(f"è¿˜åŸç»“æœæ–‡ä»¶ï¼š{restored_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    if intermediate.get('dedup_type') == 'edge_deduplication':
        total_llm_groups = sum(len(t.get('llm_groups', [])) for t in intermediate['triples'])
        print(f"\nä¸­é—´ç»“æœä¸­çš„LLMè°ƒç”¨æ¬¡æ•°ï¼š{total_llm_groups}")
        print(f"è¿˜åŸçš„semantic_resultsæ¡ç›®æ•°ï¼š{len(restored)}")
        
        if total_llm_groups == len(restored):
            print("âœ… æ•°é‡åŒ¹é…ï¼")
        else:
            print("âš ï¸  æ•°é‡ä¸åŒ¹é…ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªç»“æœçš„æ ¼å¼
    if restored:
        print("\nç¬¬ä¸€ä¸ªè¿˜åŸç»“æœçš„æ ¼å¼ï¼š")
        first = restored[0]
        print(f"  - type: {first.get('type')}")
        print(f"  - metadata keys: {list(first.get('metadata', {}).keys())}")
        print(f"  - has response: {'response' in first}")
        print(f"  - has error: {'error' in first}")
        
        # å°è¯•è§£æresponse
        try:
            response_data = json.loads(first['response'])
            print(f"  - response is valid JSON: âœ…")
            print(f"  - response has groups: {'groups' in response_data}")
            if 'groups' in response_data:
                print(f"  - number of groups: {len(response_data['groups'])}")
        except Exception as e:
            print(f"  - response parse error: {e}")


if __name__ == "__main__":
    demo_usage()
    
    print("\n" + "=" * 70)
    print("å¦‚éœ€éªŒè¯è¿˜åŸç»“æœï¼Œè¯·æä¾›æ–‡ä»¶è·¯å¾„ï¼š")
    print("=" * 70)
    print("python example_use_restored_results.py \\")
    print("  --validate \\")
    print("  --intermediate output/dedup_intermediate/demo_edge_dedup_xxx.json \\")
    print("  --restored output/dedup_intermediate/demo_semantic_results_xxx.pkl")
