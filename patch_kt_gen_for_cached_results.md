# è¡¥ä¸ï¼šæ”¯æŒä½¿ç”¨ç¼“å­˜çš„semantic_results

è¿™ä¸ªæ–‡æ¡£å±•ç¤ºå¦‚ä½•ä¿®æ”¹`models/constructor/kt_gen.py`æ¥æ”¯æŒä½¿ç”¨ç¼“å­˜çš„semantic_resultsï¼Œä»è€Œè·³è¿‡æ˜‚è´µçš„LLMè°ƒç”¨ã€‚

## ä¿®æ”¹1ï¼šæ·»åŠ cached_resultså‚æ•°åˆ°æ„é€ å‡½æ•°

**ä½ç½®ï¼š** `KnowledgeTree.__init__` æ–¹æ³•

```python
def __init__(
    self,
    dataset_name: str,
    chunk_func: Optional[Callable] = None,
    cached_semantic_results: str = None,  # ğŸ”¥ æ–°å¢å‚æ•°
):
    """
    Args:
        dataset_name: dataset name
        chunk_func: function to chunk text
        cached_semantic_results: Path to cached semantic_results pickle file.
                                If provided, will skip LLM calls in semantic deduplication.
    """
    self.dataset_name = dataset_name
    self.chunk_func = chunk_func
    self.cached_semantic_results = cached_semantic_results  # ğŸ”¥ ä¿å­˜å‚æ•°
    
    # ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...
```

## ä¿®æ”¹2ï¼šä¿®æ”¹triple_deduplicate_semanticæ–¹æ³•

**ä½ç½®ï¼š** `triple_deduplicate_semantic` æ–¹æ³•ä¸­çš„PHASE 3

**åŸå§‹ä»£ç ï¼ˆLine 4370-4375ï¼‰ï¼š**
```python
logger.info(f"Collected {len(semantic_prompts)} semantic dedup prompts, processing concurrently...")
semantic_results = self._concurrent_llm_calls(semantic_prompts)

# Parse semantic dedup results and update group_data
logger.info("Parsing semantic dedup results...")
self._parse_semantic_dedup_results(dedup_groups, semantic_results)
```

**ä¿®æ”¹åçš„ä»£ç ï¼š**
```python
logger.info(f"Collected {len(semantic_prompts)} semantic dedup prompts")

# ğŸ”¥ ä½¿ç”¨ç¼“å­˜çš„ç»“æœæˆ–è°ƒç”¨LLM
if self.cached_semantic_results:
    logger.info(f"ğŸš€ Using cached semantic_results from: {self.cached_semantic_results}")
    logger.info(f"ğŸ’° Skipping {len(semantic_prompts)} LLM calls to save tokens!")
    
    import pickle
    try:
        with open(self.cached_semantic_results, 'rb') as f:
            semantic_results = pickle.load(f)
        logger.info(f"âœ… Loaded {len(semantic_results)} cached results")
        
        # éªŒè¯ç¼“å­˜ç»“æœæ•°é‡
        if len(semantic_results) != len(semantic_prompts):
            logger.warning(
                f"âš ï¸  Cached results count ({len(semantic_results)}) "
                f"doesn't match prompts count ({len(semantic_prompts)}). "
                f"Falling back to LLM calls."
            )
            semantic_results = self._concurrent_llm_calls(semantic_prompts)
    except Exception as e:
        logger.error(f"âŒ Failed to load cached results: {e}")
        logger.info("Falling back to LLM calls...")
        semantic_results = self._concurrent_llm_calls(semantic_prompts)
else:
    logger.info("Processing concurrently...")
    semantic_results = self._concurrent_llm_calls(semantic_prompts)

# Parse semantic dedup results and update group_data
logger.info("Parsing semantic dedup results...")
self._parse_semantic_dedup_results(dedup_groups, semantic_results)
```

## ä¿®æ”¹3ï¼ˆå¯é€‰ï¼‰ï¼šä»é…ç½®æ–‡ä»¶è¯»å–cached_resultsè·¯å¾„

**ä½ç½®ï¼š** `triple_deduplicate_semantic` æ–¹æ³•å¼€å§‹å¤„

**åœ¨æ–¹æ³•å¼€å§‹å¤„æ·»åŠ ï¼š**
```python
def triple_deduplicate_semantic(self):
    """Perform semantic deduplication on all edges"""
    config = self._get_semantic_dedup_config()
    save_intermediate = config and getattr(config, "save_intermediate_results", False)
    if save_intermediate:
        self._edge_dedup_results = []
    
    # ğŸ”¥ å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†cached_results_pathï¼Œä½¿ç”¨å®ƒ
    if config and not self.cached_semantic_results:
        cached_path = getattr(config, "cached_results_path", None)
        if cached_path:
            logger.info(f"Using cached_results_path from config: {cached_path}")
            self.cached_semantic_results = cached_path
    
    # ... ç»§ç»­åŸæœ‰ä»£ç  ...
```

**é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼ˆconfig/semantic_dedup.yamlï¼‰ï¼š**
```yaml
semantic_dedup:
  save_intermediate_results: true
  intermediate_results_path: "output/dedup_intermediate/"
  
  # ä½¿ç”¨ç¼“å­˜çš„semantic_resultsï¼ˆå¦‚æœä¸éœ€è¦ï¼Œæ³¨é‡Šæ‰æˆ–åˆ é™¤ï¼‰
  cached_results_path: "output/dedup_intermediate/demo_semantic_results_20241023_123456.pkl"
  
  threshold: 0.85
  max_batch_size: 8
```

## ä¿®æ”¹4ï¼ˆå¯é€‰ï¼‰ï¼šåœ¨_deduplicate_keyword_nodesä¸­ä¹Ÿæ”¯æŒç¼“å­˜

**ä½ç½®ï¼š** `_deduplicate_keyword_nodes` æ–¹æ³•ä¸­çš„PHASE 3

ç±»ä¼¼çš„ä¿®æ”¹åº”ç”¨åˆ°keyword deduplicationï¼š

```python
logger.info(f"Collected {len(semantic_prompts)} keyword semantic dedup prompts")

# ğŸ”¥ ä½¿ç”¨ç¼“å­˜çš„ç»“æœæˆ–è°ƒç”¨LLM
if self.cached_semantic_results:
    logger.info(f"ğŸš€ Using cached semantic_results for keyword dedup")
    # ... åŠ è½½ç¼“å­˜é€»è¾‘ ...
else:
    logger.info("Processing concurrently...")
    semantic_results = self._concurrent_llm_calls(semantic_prompts)
```

## ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1ï¼šé€šè¿‡æ„é€ å‡½æ•°å‚æ•°

```python
from models.constructor.kt_gen import KnowledgeTree

# é¦–æ¬¡è¿è¡Œï¼Œä¿å­˜ä¸­é—´ç»“æœ
kg = KnowledgeTree(
    dataset_name="demo",
    cached_semantic_results=None  # ä¸ä½¿ç”¨ç¼“å­˜
)
kg.build()  # save_intermediate_resultséœ€è¦åœ¨é…ç½®ä¸­å¼€å¯

# ç¬¬äºŒæ¬¡è¿è¡Œï¼Œä½¿ç”¨ç¼“å­˜
kg = KnowledgeTree(
    dataset_name="demo",
    cached_semantic_results="output/dedup_intermediate/demo_semantic_results_20241023.pkl"
)
kg.build()  # è¿™æ¬¡ä¼šè·³è¿‡LLMè°ƒç”¨
```

### æ–¹æ³•2ï¼šé€šè¿‡é…ç½®æ–‡ä»¶

**config/semantic_dedup.yaml:**
```yaml
semantic_dedup:
  cached_results_path: "output/dedup_intermediate/demo_semantic_results_xxx.pkl"
```

ç„¶åæ­£å¸¸è¿è¡Œå³å¯ã€‚

### æ–¹æ³•3ï¼šé€šè¿‡å‘½ä»¤è¡Œå‚æ•°

**ä¿®æ”¹main.pyæ·»åŠ å‚æ•°ï¼š**
```python
parser.add_argument(
    '--cached-semantic-results',
    type=str,
    default=None,
    help='Path to cached semantic_results pickle file'
)

# åœ¨åˆ›å»ºKnowledgeTreeæ—¶ä¼ å…¥
kg = KnowledgeTree(
    dataset_name=args.dataset,
    cached_semantic_results=args.cached_semantic_results
)
```

**è¿è¡Œå‘½ä»¤ï¼š**
```bash
python main.py --dataset demo \
    --cached-semantic-results output/dedup_intermediate/demo_semantic_results_xxx.pkl
```

## å®Œæ•´å·¥ä½œæµç¨‹

```bash
# Step 1: é¦–æ¬¡è¿è¡Œï¼Œä¿å­˜ä¸­é—´ç»“æœ
python main.py --dataset demo

# è¿™ä¼šç”Ÿæˆï¼šoutput/dedup_intermediate/demo_edge_dedup_20241023_123456.json

# Step 2: è¿˜åŸsemantic_results
python restore_semantic_results.py \
    output/dedup_intermediate/demo_edge_dedup_20241023_123456.json

# è¿™ä¼šç”Ÿæˆï¼šoutput/dedup_intermediate/demo_semantic_results_20241023_123456.pkl

# Step 3: ä½¿ç”¨ç¼“å­˜é‡æ–°è¿è¡Œï¼ˆä¸è°ƒç”¨LLMï¼‰
python main.py --dataset demo \
    --cached-semantic-results \
    output/dedup_intermediate/demo_semantic_results_20241023_123456.pkl
```

## æ³¨æ„äº‹é¡¹

1. **ç¼“å­˜æœ‰æ•ˆæ€§ï¼š** ç¼“å­˜çš„semantic_resultsåªåœ¨ä»¥ä¸‹æƒ…å†µä¸‹æœ‰æ•ˆï¼š
   - è¾“å…¥æ•°æ®æ²¡æœ‰å˜åŒ–
   - èšç±»ç»“æœæ²¡æœ‰å˜åŒ–ï¼ˆclusteringé…ç½®ç›¸åŒï¼‰
   - promptsæ•°é‡å’Œé¡ºåºä¸€è‡´

2. **éªŒè¯ç¼“å­˜ï¼š** ä»£ç ä¼šè‡ªåŠ¨éªŒè¯ç¼“å­˜çš„ç»“æœæ•°é‡æ˜¯å¦ä¸å½“å‰promptsæ•°é‡åŒ¹é…ï¼Œä¸åŒ¹é…ä¼šè‡ªåŠ¨fallbackåˆ°LLMè°ƒç”¨

3. **è°ƒè¯•ï¼š** å¦‚æœé‡åˆ°é—®é¢˜ï¼Œæ£€æŸ¥æ—¥å¿—ä¸­çš„ç¼“å­˜åŠ è½½ä¿¡æ¯

4. **æ€§èƒ½æå‡ï¼š** ä½¿ç”¨ç¼“å­˜å¯ä»¥èŠ‚çœï¼š
   - 100% çš„LLM APIè°ƒç”¨æˆæœ¬
   - 90%+ çš„å¤„ç†æ—¶é—´
   - é€‚åˆè°ƒè¯•ã€æµ‹è¯•å’Œé‡å¤è¿è¡Œåœºæ™¯

## æµ‹è¯•

ä¿®æ”¹åå¯ä»¥ç”¨ä»¥ä¸‹ä»£ç æµ‹è¯•ï¼š

```python
# test_cached_results.py
from models.constructor.kt_gen import KnowledgeTree

# æµ‹è¯•1ï¼šæ­£å¸¸è¿è¡Œï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
kg1 = KnowledgeTree(dataset_name="demo")
print("Test 1: Normal run (no cache)")
# kg1.build()

# æµ‹è¯•2ï¼šä½¿ç”¨ç¼“å­˜
kg2 = KnowledgeTree(
    dataset_name="demo",
    cached_semantic_results="output/dedup_intermediate/demo_semantic_results_xxx.pkl"
)
print("Test 2: Using cached results")
# kg2.build()

# æµ‹è¯•3ï¼šç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆåº”è¯¥fallbackåˆ°LLMè°ƒç”¨ï¼‰
kg3 = KnowledgeTree(
    dataset_name="demo",
    cached_semantic_results="nonexistent_file.pkl"
)
print("Test 3: Invalid cache file (should fallback)")
# kg3.build()
```
