# åˆ«å/å®ä½“å»é‡ä¸šç•Œè§£å†³æ–¹æ¡ˆè°ƒç ”

**æ—¥æœŸ**: 2025-10-31  
**èƒŒæ™¯**: é’ˆå¯¹è¯­ä¹‰ç›¸ä¼¼åº¦è¾ƒä½çš„åˆ«åï¼ˆå¦‚"å‰å¸ƒæ–¯ä¼ªå½±/æˆªæ–­ä¼ªå½±"ï¼‰çš„å»é‡é—®é¢˜  
**è°ƒç ”äºº**: Knowledge Graph Team

---

## ğŸ“‹ é—®é¢˜å®šä¹‰

### å½“å‰æŒ‘æˆ˜

åœ¨HeadèŠ‚ç‚¹å»é‡è¿‡ç¨‹ä¸­ï¼Œä»…ä¾èµ–**å®ä½“åç§°çš„è¯­ä¹‰ç›¸ä¼¼åº¦**å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

**å…¸å‹æ¡ˆä¾‹**ï¼š
```
å®ä½“A: "å‰å¸ƒæ–¯ä¼ªå½±" (Gibbs Artifact)
å®ä½“B: "æˆªæ–­ä¼ªå½±" (Truncation Artifact)

é—®é¢˜ï¼š
- åç§°è¯­ä¹‰ç›¸ä¼¼åº¦: ~0.65 (ä½äºé˜ˆå€¼0.85)
- å®é™…å…³ç³»: Bæ˜¯Açš„ä¸€ç§/åˆ«å
- å­å›¾å¯èƒ½ä¸åŒ:
  * Açš„å­å›¾: å®šä¹‰ã€è¡¨ç°å½¢å¼ã€å‘ç”Ÿæœºåˆ¶
  * Bçš„å­å›¾: è§£å†³æ–¹æ¡ˆ
- å›¾ä¸­å­˜åœ¨: A --[åˆ«ååŒ…æ‹¬]--> B
```

### æ ¸å¿ƒçŸ›ç›¾

1. **åç§°ç›¸ä¼¼åº¦ä½** â†’ æ— æ³•é€šè¿‡embeddingåŒ¹é…
2. **å­å›¾å¯èƒ½ä¸åŒ** â†’ æè¿°ç›¸ä¼¼åº¦ä¹Ÿä½
3. **æœ‰æ˜¾å¼å…³ç³»** â†’ ä½†ä¸åœ¨ç›¸ä¼¼åº¦è®¡ç®—èŒƒå›´å†…

---

## ğŸŒ ä¸šç•Œè§£å†³æ–¹æ¡ˆæ¦‚è§ˆ

| æ–¹æ¡ˆç±»å‹ | æ ¸å¿ƒæ€æƒ³ | é€‚ç”¨åœºæ™¯ | ä»£è¡¨å·¥ä½œ |
|---------|---------|---------|---------|
| **1. å¤šç‰¹å¾èåˆ** | ç»“åˆåç§°+å±æ€§+ç»“æ„å¤šç§ä¿¡å· | é€šç”¨å®ä½“å»é‡ | DeepMatcher, Magellan |
| **2. åŸºäºå›¾ç»“æ„** | åˆ©ç”¨é‚»å±…èŠ‚ç‚¹ã€è·¯å¾„ä¿¡æ¯ | æœ‰ä¸°å¯Œå…³ç³»çš„KG | PSL, PARIS |
| **3. æ˜¾å¼å…³ç³»åˆ©ç”¨** | æŒ–æ˜å·²æœ‰åˆ«å/ç­‰ä»·å…³ç³» | æœ‰éƒ¨åˆ†æ ‡æ³¨çš„KG | Relation-aware ER |
| **4. ä¸Šä¸‹æ–‡å¢å¼º** | ç”¨å­å›¾/æ–‡æœ¬æ‰©å±•å®ä½“è¡¨ç¤º | ä¸Šä¸‹æ–‡ä¸°å¯Œçš„åœºæ™¯ | GNN-based methods |
| **5. æ··åˆæ¨ç†** | è§„åˆ™+ML+KGæ¨ç†ç»“åˆ | éœ€è¦é«˜å‡†ç¡®ç‡ | Hybrid ER systems |
| **6. å¼±ç›‘ç£å­¦ä¹ ** | åˆ©ç”¨å°‘é‡å…³ç³»ä½œä¸ºè¿œç¨‹ç›‘ç£ | æ ‡æ³¨æ•°æ®å°‘ | Distant Supervision |
| **7. å…³ç³»ä¼ æ’­** | ä»å·²çŸ¥åˆ«åæ¨æ–­æœªçŸ¥åˆ«å | æœ‰åˆ«åé—­åŒ…éœ€æ±‚ | Transitive Closure |

---

## ğŸ“Š æ–¹æ¡ˆè¯¦ç»†åˆ†æ

### æ–¹æ¡ˆ1: å¤šç‰¹å¾èåˆæ–¹æ³•

#### æ ¸å¿ƒæ€æƒ³
**ä¸ä»…ä»…ä¾èµ–åç§°ç›¸ä¼¼åº¦ï¼Œè€Œæ˜¯ç»¼åˆå¤šç§ç‰¹å¾è®¡ç®—åŒ¹é…åˆ†æ•°**

#### å…·ä½“æ–¹æ³•

**1.1 ç‰¹å¾ç±»å‹**
```python
# ä¼ªä»£ç ç¤ºä¾‹
features = {
    # åç§°ç‰¹å¾ (30%)
    "name_similarity": cosine_sim(emb_A, emb_B),
    "name_edit_distance": edit_distance(name_A, name_B),
    "name_token_overlap": jaccard(tokens_A, tokens_B),
    
    # å±æ€§ç‰¹å¾ (30%)
    "attribute_overlap": jaccard(attrs_A, attrs_B),
    "description_similarity": cosine_sim(desc_A, desc_B),
    
    # ç»“æ„ç‰¹å¾ (40%)
    "neighbor_similarity": jaccard(neighbors_A, neighbors_B),
    "relation_type_overlap": jaccard(rel_types_A, rel_types_B),
    "common_paths": count_common_paths(A, B, max_length=3)
}

# åŠ æƒèåˆ
score = sum(weight[k] * features[k] for k in features)
is_match = score > threshold
```

**1.2 æƒé‡å­¦ä¹ **
```python
# æ–¹å¼1: æ‰‹åŠ¨è°ƒå‚
weights = {
    "name_similarity": 0.3,
    "attribute_overlap": 0.2,
    "neighbor_similarity": 0.5
}

# æ–¹å¼2: æœºå™¨å­¦ä¹ ï¼ˆéœ€è¦æ ‡æ³¨æ•°æ®ï¼‰
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(training_features, training_labels)
```

#### é€‚ç”¨æ€§åˆ†æ

**ä¼˜ç‚¹**ï¼š
- âœ… ä¸ä¾èµ–å•ä¸€ä¿¡å·ï¼Œé²æ£’æ€§å¼º
- âœ… å¯ä»¥æ•è·åç§°ç›¸ä¼¼åº¦ä½ä½†ç»“æ„ç›¸ä¼¼çš„æƒ…å†µ
- âœ… çµæ´»å¯è°ƒï¼šå¯æ ¹æ®é¢†åŸŸè°ƒæ•´æƒé‡

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦ç²¾å¿ƒè®¾è®¡ç‰¹å¾
- âŒ æƒé‡è°ƒå‚éœ€è¦æ ‡æ³¨æ•°æ®
- âŒ è®¡ç®—å¤æ‚åº¦è¾ƒé«˜

**é’ˆå¯¹æœ¬æ¡ˆä¾‹**ï¼š
```
å‰å¸ƒæ–¯ä¼ªå½± vs æˆªæ–­ä¼ªå½±:
- name_similarity: 0.65 (ä½)
- neighbor_similarity: 0.80 (é«˜ - å¦‚æœæœ‰å…±åŒé‚»å±…)
- relation_type_overlap: 0.85 (é«˜ - éƒ½æœ‰"æ˜¯ä¸€ç§"ã€"è¡¨ç°ä¸º"ç­‰)
â†’ åŠ æƒå: 0.30*0.65 + 0.20*0.80 + 0.50*0.85 = 0.78 (å¯èƒ½é€šè¿‡)
```

#### ä»£è¡¨æ€§å·¥ä½œ

1. **DeepMatcher** (SIGMOD 2018)
   - æ·±åº¦å­¦ä¹  + å¤šç‰¹å¾èåˆ
   - è‡ªåŠ¨å­¦ä¹ ç‰¹å¾æƒé‡
   
2. **Magellan** (VLDB 2016)
   - å£°æ˜å¼å®ä½“åŒ¹é…ç³»ç»Ÿ
   - æ”¯æŒè‡ªå®šä¹‰ç‰¹å¾å‡½æ•°

---

### æ–¹æ¡ˆ2: åŸºäºå›¾ç»“æ„çš„æ–¹æ³•

#### æ ¸å¿ƒæ€æƒ³
**åˆ©ç”¨èŠ‚ç‚¹çš„é‚»åŸŸä¿¡æ¯å’Œå›¾æ‹“æ‰‘ç»“æ„æ¥å¢å¼ºå®ä½“è¡¨ç¤º**

#### 2.1 é‚»å±…ç›¸ä¼¼åº¦ (Neighbor Similarity)

```python
def neighbor_based_similarity(node_a, node_b, graph):
    """
    åŸºäºå…±åŒé‚»å±…åˆ¤æ–­ç›¸ä¼¼åº¦
    å‡è®¾: å¦‚æœä¸¤ä¸ªèŠ‚ç‚¹çš„é‚»å±…é«˜åº¦é‡å ï¼Œå®ƒä»¬å¯èƒ½æ˜¯åŒä¸€å®ä½“
    """
    neighbors_a = set(graph.neighbors(node_a))
    neighbors_b = set(graph.neighbors(node_b))
    
    # Jaccardç›¸ä¼¼åº¦
    jaccard = len(neighbors_a & neighbors_b) / len(neighbors_a | neighbors_b)
    
    # Adamic-AdaræŒ‡æ•° (è€ƒè™‘é‚»å±…çš„ç¨€æœ‰åº¦)
    common = neighbors_a & neighbors_b
    aa_index = sum(1 / math.log(graph.degree(n)) for n in common if graph.degree(n) > 1)
    
    return (jaccard, aa_index)
```

**æ¡ˆä¾‹åº”ç”¨**ï¼š
```
å‰å¸ƒæ–¯ä¼ªå½±çš„é‚»å±…: [MRI, é¢‘åŸŸ, Kç©ºé—´, ä¿¡å·å¤„ç†, ...]
æˆªæ–­ä¼ªå½±çš„é‚»å±…: [MRI, é¢‘åŸŸ, Kç©ºé—´, é‡‡æ ·, ...]
å…±åŒé‚»å±…: [MRI, é¢‘åŸŸ, Kç©ºé—´]  â†’ é«˜é‡å  â†’ å¯èƒ½æ˜¯ç›¸å…³å®ä½“
```

#### 2.2 è·¯å¾„ç‰¹å¾ (Path Features)

```python
def extract_path_features(node_a, node_b, graph, max_length=3):
    """
    æå–ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„è·¯å¾„ç‰¹å¾
    """
    features = {}
    
    # 1. æœ€çŸ­è·¯å¾„é•¿åº¦
    try:
        shortest = nx.shortest_path_length(graph, node_a, node_b)
        features['shortest_path'] = shortest
    except nx.NetworkXNoPath:
        features['shortest_path'] = float('inf')
    
    # 2. è·¯å¾„ä¸Šçš„å…³ç³»ç±»å‹
    if features['shortest_path'] < max_length:
        paths = nx.all_simple_paths(graph, node_a, node_b, cutoff=max_length)
        relation_types = []
        for path in paths:
            for i in range(len(path)-1):
                edge = graph[path[i]][path[i+1]]
                relation_types.append(edge['relation'])
        features['path_relations'] = Counter(relation_types)
    
    # 3. æ˜¯å¦å­˜åœ¨"åˆ«ååŒ…æ‹¬"ç­‰æ˜¾å¼å…³ç³»è·¯å¾„
    features['has_alias_path'] = check_alias_path(graph, node_a, node_b)
    
    return features
```

**æ¡ˆä¾‹åº”ç”¨**ï¼š
```
å‰å¸ƒæ–¯ä¼ªå½± --[åˆ«ååŒ…æ‹¬]--> æˆªæ–­ä¼ªå½±
â†’ has_alias_path = True
â†’ å³ä½¿åç§°ç›¸ä¼¼åº¦ä½ï¼Œä¹Ÿåº”è¯¥è€ƒè™‘åˆå¹¶
```

#### 2.3 GNN-basedæ–¹æ³•

```python
# ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œå­¦ä¹ èŠ‚ç‚¹è¡¨ç¤º
import torch
from torch_geometric.nn import GCNConv

class EntityEncoder(torch.nn.Module):
    def __init__(self, feature_dim, hidden_dim):
        super().__init__()
        self.conv1 = GCNConv(feature_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
    
    def forward(self, x, edge_index):
        # x: èŠ‚ç‚¹ç‰¹å¾ (åŒ…æ‹¬åç§°embedding)
        # edge_index: å›¾ç»“æ„
        h = self.conv1(x, edge_index).relu()
        h = self.conv2(h, edge_index)
        return h  # èåˆäº†é‚»å±…ä¿¡æ¯çš„èŠ‚ç‚¹è¡¨ç¤º

# è®­ç»ƒåï¼Œä½¿ç”¨èåˆåçš„è¡¨ç¤ºè®¡ç®—ç›¸ä¼¼åº¦
emb_a = model(features, graph_structure)[node_a]
emb_b = model(features, graph_structure)[node_b]
similarity = cosine_similarity(emb_a, emb_b)
```

**ä¼˜ç‚¹**ï¼š
- è‡ªåŠ¨èåˆå¤šè·³é‚»å±…ä¿¡æ¯
- å¯ä»¥æ•è·å¤æ‚çš„å›¾æ¨¡å¼

**ç¼ºç‚¹**ï¼š
- éœ€è¦è®­ç»ƒæ•°æ®
- è®¡ç®—æˆæœ¬é«˜

#### é€‚ç”¨æ€§åˆ†æ

**ä¼˜ç‚¹**ï¼š
- âœ… ä¸ä¾èµ–åç§°ï¼Œåˆ©ç”¨å›¾çš„ä¸°å¯Œä¿¡æ¯
- âœ… å¯ä»¥å‘ç°éšå«çš„å…³è”
- âœ… ç‰¹åˆ«é€‚åˆå…³ç³»ä¸°å¯Œçš„KG

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦å›¾å·²ç»æ„å»ºè¾ƒå®Œæ•´
- âŒ å¯¹å­¤ç«‹èŠ‚ç‚¹æ•ˆæœå·®
- âŒ è®¡ç®—å¤æ‚åº¦é«˜ (O(nÂ²) é‚»å±…æ¯”è¾ƒ)

**ä»£è¡¨æ€§å·¥ä½œ**ï¼š
1. **PARIS** (VLDB 2012) - æ¦‚ç‡æ€§å…³ç³»ç›¸ä¼¼åº¦
2. **PSL** (Probabilistic Soft Logic) - åŸºäºé€»è¾‘è§„åˆ™çš„å›¾æ¨ç†
3. **Graph Matching Networks** (ICML 2019) - ç«¯åˆ°ç«¯å›¾åŒ¹é…

---

### æ–¹æ¡ˆ3: æ˜¾å¼å…³ç³»åˆ©ç”¨æ–¹æ³• â­ **ç›´æ¥è§£å†³ä½ çš„é—®é¢˜**

#### æ ¸å¿ƒæ€æƒ³
**æŒ–æ˜å’Œåˆ©ç”¨å›¾ä¸­å·²å­˜åœ¨çš„åˆ«å/ç­‰ä»·å…³ç³»ï¼Œä½œä¸ºå»é‡çš„å¼ºä¿¡å·**

#### 3.1 å…³ç³»ç±»å‹è¯†åˆ«

é¦–å…ˆè¯†åˆ«å“ªäº›å…³ç³»ç±»å‹æš—ç¤º"åˆ«å"æˆ–"ç­‰ä»·"ï¼š

```python
# åˆ«åå…³ç³»çš„å…¸å‹è¡¨è¾¾
ALIAS_RELATIONS = {
    "åˆ«ååŒ…æ‹¬",
    "also_known_as", 
    "alias_of",
    "åŒä¹‰è¯",
    "synonym",
    "also_called",
    "åˆç§°",
    "ç®€ç§°",
    "å…¨ç§°"
}

# å¼±ç­‰ä»·å…³ç³» (éœ€è¦è¿›ä¸€æ­¥éªŒè¯)
WEAK_EQUIV_RELATIONS = {
    "æ˜¯ä¸€ç§",  # ä½†éœ€è¦æ£€æŸ¥æ˜¯å¦æ˜¯"å®Œå…¨æ˜¯"
    "ç­‰åŒäº",
    "equivalent_to",
    "same_as"
}
```

#### 3.2 åŸºäºå…³ç³»çš„å€™é€‰ç”Ÿæˆ

**æ–¹æ³•A: ç›´æ¥æå–åˆ«åå¯¹**

```python
def extract_alias_pairs_from_relations(graph):
    """
    ä»å›¾ä¸­ç›´æ¥æå–æœ‰åˆ«åå…³ç³»çš„å®ä½“å¯¹
    è¿™äº›å¯¹ä¸éœ€è¦è®¡ç®—åç§°ç›¸ä¼¼åº¦ï¼Œç›´æ¥ä½œä¸ºå€™é€‰
    """
    alias_pairs = []
    
    for u, v, data in graph.edges(data=True):
        relation = data.get('relation', '')
        
        # å¼ºåˆ«åå…³ç³» â†’ ç›´æ¥åŠ å…¥
        if relation in ALIAS_RELATIONS:
            alias_pairs.append((u, v, 1.0, 'explicit_alias'))
        
        # å¼±ç­‰ä»·å…³ç³» â†’ éœ€è¦é¢å¤–éªŒè¯
        elif relation in WEAK_EQUIV_RELATIONS:
            # ä½¿ç”¨LLMæˆ–å…¶ä»–æ–¹æ³•éªŒè¯æ˜¯å¦çœŸçš„ç­‰ä»·
            alias_pairs.append((u, v, 0.8, 'weak_equiv'))
    
    return alias_pairs
```

**æ¡ˆä¾‹åº”ç”¨**ï¼š
```python
# å›¾ä¸­å·²æœ‰:
graph.add_edge("å‰å¸ƒæ–¯ä¼ªå½±", "æˆªæ–­ä¼ªå½±", relation="åˆ«ååŒ…æ‹¬")

# å»é‡æ—¶:
alias_pairs = extract_alias_pairs_from_relations(graph)
# â†’ [("å‰å¸ƒæ–¯ä¼ªå½±", "æˆªæ–­ä¼ªå½±", 1.0, 'explicit_alias')]

# è¿™ä¸€å¯¹ç›´æ¥è¿›å…¥åˆå¹¶æµç¨‹ï¼Œæ— éœ€è®¡ç®—åç§°ç›¸ä¼¼åº¦ï¼
```

**æ–¹æ³•B: å…³ç³»ä¼ æ’­é—­åŒ…**

```python
def transitive_closure_aliases(graph):
    """
    å¦‚æœ Aâ†’B ä¸” Bâ†’C éƒ½æ˜¯åˆ«åå…³ç³»ï¼Œåˆ™ A å’Œ C ä¹Ÿåº”è¯¥å»é‡
    """
    alias_graph = nx.DiGraph()
    
    # æå–æ‰€æœ‰åˆ«åè¾¹
    for u, v, data in graph.edges(data=True):
        if data.get('relation') in ALIAS_RELATIONS:
            alias_graph.add_edge(u, v)
    
    # è®¡ç®—ä¼ é€’é—­åŒ…
    closure = nx.transitive_closure(alias_graph)
    
    # å°†æ‰€æœ‰è¿é€šåˆ†é‡ä½œä¸ºç­‰ä»·ç±»
    components = list(nx.weakly_connected_components(closure))
    
    # æ¯ä¸ªè¿é€šåˆ†é‡ä¸­é€‰ä¸€ä¸ªä»£è¡¨ï¼Œå…¶ä»–éƒ½åˆå¹¶åˆ°å®ƒ
    merge_mapping = {}
    for component in components:
        representative = choose_representative(component)
        for node in component:
            if node != representative:
                merge_mapping[node] = representative
    
    return merge_mapping
```

**æ¡ˆä¾‹åº”ç”¨**ï¼š
```
åŸå§‹å…³ç³»:
  A --[åˆ«ååŒ…æ‹¬]--> B
  B --[åˆç§°]--> C
  D --[synonym]--> A

ä¼ é€’é—­åŒ…å:
  {A, B, C, D} å½¢æˆä¸€ä¸ªç­‰ä»·ç±»
  é€‰æ‹© A ä½œä¸ºä»£è¡¨
  â†’ Bâ†’A, Câ†’A, Dâ†’A
```

#### 3.3 å…³ç³»æƒé‡ä¸ä¿¡ä»»åº¦

ä¸æ˜¯æ‰€æœ‰å…³ç³»éƒ½100%å¯ä¿¡ï¼š

```python
RELATION_TRUST_SCORES = {
    "åˆ«ååŒ…æ‹¬": 0.95,        # é«˜å¯ä¿¡
    "also_known_as": 0.95,
    "synonym": 0.90,
    "åˆç§°": 0.90,
    "æ˜¯ä¸€ç§": 0.30,          # ä½å¯ä¿¡ - éœ€è¦é¢å¤–éªŒè¯
    "ç±»ä¼¼äº": 0.20,          # å¾ˆä½ - å¯èƒ½ä¸æ˜¯åˆ«å
}

def score_by_relation(u, v, graph):
    """
    æ ¹æ®å…³ç³»ç±»å‹ç»™å‡ºåŒ¹é…åˆ†æ•°
    """
    if not graph.has_edge(u, v):
        return 0.0
    
    relation = graph[u][v].get('relation', '')
    base_score = RELATION_TRUST_SCORES.get(relation, 0.0)
    
    # å¯ä»¥ç»“åˆå…¶ä»–ä¿¡å·è°ƒæ•´
    # ä¾‹å¦‚ï¼šå¦‚æœåç§°ä¹Ÿç›¸ä¼¼ï¼Œæé«˜å¯ä¿¡åº¦
    name_sim = compute_name_similarity(u, v)
    if name_sim > 0.7:
        base_score = min(1.0, base_score + 0.1)
    
    return base_score
```

#### 3.4 æ··åˆç­–ç•¥ï¼šå…³ç³» + ç›¸ä¼¼åº¦

```python
def hybrid_candidate_generation(graph, similarity_threshold=0.85):
    """
    æ–¹å¼1: åç§°ç›¸ä¼¼åº¦é«˜ â†’ å€™é€‰å¯¹
    æ–¹å¼2: æœ‰åˆ«åå…³ç³» â†’ å€™é€‰å¯¹
    æ–¹å¼3: é‚»å±…ç›¸ä¼¼ + å¼±åˆ«åå…³ç³» â†’ å€™é€‰å¯¹
    """
    candidates = []
    
    # ç­–ç•¥1: åŸºäºembeddingçš„ä¼ ç»Ÿæ–¹æ³•
    similarity_pairs = generate_semantic_candidates(
        graph, threshold=similarity_threshold
    )
    for u, v, score in similarity_pairs:
        candidates.append((u, v, score, 'embedding'))
    
    # ç­–ç•¥2: åŸºäºæ˜¾å¼åˆ«åå…³ç³»
    alias_pairs = extract_alias_pairs_from_relations(graph)
    for u, v, score, source in alias_pairs:
        candidates.append((u, v, score, 'relation'))
    
    # ç­–ç•¥3: ä½ç›¸ä¼¼åº¦ + æœ‰å…³ç³»è·¯å¾„ â†’ ä¹Ÿè€ƒè™‘
    for u, v in graph.edges():
        name_sim = compute_name_similarity(u, v)
        if 0.5 < name_sim < similarity_threshold:  # åœ¨é˜ˆå€¼ä¸‹ä½†ä¸å¤ªä½
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ«åè·¯å¾„
            if has_alias_path(graph, u, v, max_length=2):
                score = (name_sim + 1.0) / 2  # å¹³å‡
                candidates.append((u, v, score, 'hybrid'))
    
    # å»é‡
    candidates = deduplicate_candidates(candidates)
    return candidates
```

#### é€‚ç”¨æ€§åˆ†æ

**ä¼˜ç‚¹**ï¼š
- âœ… **ç›´æ¥åˆ©ç”¨å·²æœ‰çŸ¥è¯†**ï¼Œæ— éœ€çŒœæµ‹
- âœ… å¯ä»¥æ•è·åç§°å®Œå…¨ä¸åŒçš„åˆ«åï¼ˆå¦‚ç¼©å†™ã€ç¿»è¯‘ï¼‰
- âœ… å¯è§£é‡Šæ€§å¼º
- âœ… **å®Œç¾è§£å†³ä½ çš„æ¡ˆä¾‹**ï¼š"å‰å¸ƒæ–¯ä¼ªå½±/æˆªæ–­ä¼ªå½±"æœ‰æ˜¾å¼å…³ç³»

**ç¼ºç‚¹**ï¼š
- âŒ ä¾èµ–å›¾ä¸­å·²æœ‰åˆ«åå…³ç³»çš„è´¨é‡
- âŒ å¯¹äºå®Œå…¨æ²¡æœ‰å…³ç³»çš„å®ä½“å¯¹æ— æ•ˆ
- âŒ éœ€è¦äººå·¥å®šä¹‰å“ªäº›å…³ç³»è¡¨ç¤ºåˆ«å

**ä»£è¡¨æ€§å·¥ä½œ**ï¼š
1. **LinkageRules** - åŸºäºå…³ç³»æ¨¡å¼çš„å®ä½“é“¾æ¥
2. **ER with Relation Signals** - åˆ©ç”¨å…³ç³»ä½œä¸ºå¼±ç›‘ç£ä¿¡å·

---

### æ–¹æ¡ˆ4: ä¸Šä¸‹æ–‡å¢å¼ºæ–¹æ³•

#### æ ¸å¿ƒæ€æƒ³
**ç”¨å®ä½“çš„ä¸Šä¸‹æ–‡ï¼ˆå­å›¾ã€æè¿°æ–‡æœ¬ï¼‰æ‰©å±•å®ä½“è¡¨ç¤ºï¼Œè€Œéä»…ç”¨åç§°**

#### 4.1 å­å›¾æè¿°ç›¸ä¼¼åº¦

**ä½ æåˆ°çš„"æ–¹æ¡ˆ1"çš„å®Œæ•´å®ç°**ï¼š

```python
def compute_subgraph_similarity(node_a, node_b, graph):
    """
    ç”¨å­å›¾çš„æè¿°æ¥è®¡ç®—ç›¸ä¼¼åº¦ï¼Œè€Œéåç§°
    """
    # 1. æ”¶é›†å­å›¾ä¿¡æ¯
    subgraph_a = extract_subgraph_description(node_a, graph)
    subgraph_b = extract_subgraph_description(node_b, graph)
    
    # 2. è½¬ä¸ºæ–‡æœ¬æè¿°
    desc_a = subgraph_to_text(subgraph_a)
    desc_b = subgraph_to_text(subgraph_b)
    
    # 3. è®¡ç®—æè¿°ç›¸ä¼¼åº¦
    emb_a = get_embedding(desc_a)
    emb_b = get_embedding(desc_b)
    similarity = cosine_similarity(emb_a, emb_b)
    
    return similarity

def subgraph_to_text(subgraph):
    """
    å°†å­å›¾è½¬ä¸ºè‡ªç„¶è¯­è¨€æè¿°
    """
    lines = []
    
    # èŠ‚ç‚¹æœ¬èº«
    lines.append(f"Entity: {subgraph['name']}")
    
    # å‡ºè¾¹å…³ç³»
    if subgraph['outgoing']:
        lines.append("Relations:")
        for rel, target in subgraph['outgoing']:
            lines.append(f"  - {rel}: {target}")
    
    # å…¥è¾¹å…³ç³»
    if subgraph['incoming']:
        lines.append("Mentioned in:")
        for rel, source in subgraph['incoming']:
            lines.append(f"  - {source} {rel} this entity")
    
    return "\n".join(lines)
```

**æ¡ˆä¾‹åº”ç”¨**ï¼š

```
å‰å¸ƒæ–¯ä¼ªå½±çš„å­å›¾æè¿°:
  Entity: å‰å¸ƒæ–¯ä¼ªå½±
  Relations:
    - å®šä¹‰: Kç©ºé—´æˆªæ–­å¯¼è‡´çš„æŒ¯è¡ä¼ªå½±
    - è¡¨ç°å½¢å¼: å›¾åƒè¾¹ç¼˜å‡ºç°æŒ¯é“ƒ
    - å‘ç”Ÿæœºåˆ¶: å‚…é‡Œå¶å˜æ¢çš„Gibbsç°è±¡
    - åˆ«ååŒ…æ‹¬: æˆªæ–­ä¼ªå½±
  Mentioned in:
    - MRIä¼ªå½± is_a å‰å¸ƒæ–¯ä¼ªå½±

æˆªæ–­ä¼ªå½±çš„å­å›¾æè¿°:
  Entity: æˆªæ–­ä¼ªå½±
  Relations:
    - è§£å†³æ–¹æ¡ˆ: å¢åŠ é‡‡æ ·ç‚¹
    - è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨çª—å‡½æ•°
  Mentioned in:
    - å‰å¸ƒæ–¯ä¼ªå½± åˆ«ååŒ…æ‹¬ æˆªæ–­ä¼ªå½±
    
é—®é¢˜: å­å›¾å†…å®¹ä¸åŒ â†’ æè¿°ç›¸ä¼¼åº¦å¯èƒ½ä»ç„¶è¾ƒä½ï¼
```

**æ”¹è¿›ï¼šåªç”¨å…³ç³»ç±»å‹ï¼Œå¿½ç•¥å…·ä½“å€¼**

```python
def extract_relation_signature(node, graph):
    """
    åªæå–å…³ç³»ç±»å‹ï¼Œä¸ç®¡å…·ä½“æŒ‡å‘å“ªä¸ªå®ä½“
    è¿™æ ·å³ä½¿å­å›¾å†…å®¹ä¸åŒï¼Œå¦‚æœå…³ç³»æ¨¡å¼ç›¸ä¼¼ï¼Œä¹Ÿèƒ½åŒ¹é…
    """
    out_relations = [data['relation'] for _, _, data in graph.out_edges(node, data=True)]
    in_relations = [data['relation'] for _, _, data in graph.in_edges(node, data=True)]
    
    signature = {
        'out': Counter(out_relations),
        'in': Counter(in_relations)
    }
    return signature

def signature_similarity(sig_a, sig_b):
    """
    æ¯”è¾ƒä¸¤ä¸ªå®ä½“çš„å…³ç³»æ¨¡å¼ç›¸ä¼¼åº¦
    """
    # Jaccard on relation types
    all_out = set(sig_a['out'].keys()) | set(sig_b['out'].keys())
    all_in = set(sig_a['in'].keys()) | set(sig_b['in'].keys())
    
    out_jaccard = len(set(sig_a['out'].keys()) & set(sig_b['out'].keys())) / len(all_out) if all_out else 0
    in_jaccard = len(set(sig_a['in'].keys()) & set(sig_b['in'].keys())) / len(all_in) if all_in else 0
    
    return (out_jaccard + in_jaccard) / 2
```

**æ¡ˆä¾‹åº”ç”¨ï¼ˆæ”¹è¿›åï¼‰**ï¼š
```
å‰å¸ƒæ–¯ä¼ªå½±çš„å…³ç³»ç­¾å:
  out: {å®šä¹‰:1, è¡¨ç°å½¢å¼:1, å‘ç”Ÿæœºåˆ¶:1, åˆ«ååŒ…æ‹¬:1}
  in: {is_a:1}

æˆªæ–­ä¼ªå½±çš„å…³ç³»ç­¾å:
  out: {è§£å†³æ–¹æ¡ˆ:2}
  in: {åˆ«ååŒ…æ‹¬:1}

é—®é¢˜ä¾ç„¶å­˜åœ¨: å…³ç³»ç±»å‹ä¸é‡å  â†’ ç›¸ä¼¼åº¦ä½
```

#### 4.2 åˆ†å±‚åŒ¹é…ç­–ç•¥

æ—¢ç„¶å•ä¸€ç‰¹å¾ä¸å¤Ÿï¼Œé‚£å°±**åˆ†å±‚å†³ç­–**ï¼š

```python
def layered_matching(node_a, node_b, graph):
    """
    åˆ†å±‚å†³ç­–ï¼šä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„åŒ¹é…ä¿¡å·
    """
    # Layer 1: æ˜¾å¼å…³ç³» (æœ€é«˜ä¼˜å…ˆçº§)
    if has_alias_relation(graph, node_a, node_b):
        return True, 0.95, "explicit_alias"
    
    # Layer 2: åç§°é«˜åº¦ç›¸ä¼¼
    name_sim = compute_name_similarity(node_a, node_b)
    if name_sim > 0.85:
        return True, name_sim, "name_similarity"
    
    # Layer 3: åç§°ä¸­åº¦ç›¸ä¼¼ + å…³ç³»æ¨¡å¼ç›¸ä¼¼
    if name_sim > 0.60:
        sig_sim = signature_similarity(
            extract_relation_signature(node_a, graph),
            extract_relation_signature(node_b, graph)
        )
        if sig_sim > 0.70:
            combined = (name_sim + sig_sim) / 2
            return True, combined, "name+signature"
    
    # Layer 4: åç§°ä½ç›¸ä¼¼ + å…±åŒé‚»å±…å¤š
    if name_sim > 0.40:
        neighbor_sim = compute_neighbor_similarity(node_a, node_b, graph)
        if neighbor_sim > 0.80:
            combined = (name_sim * 0.3 + neighbor_sim * 0.7)
            return True, combined, "name+neighbors"
    
    # Layer 5: æœ‰é—´æ¥åˆ«åè·¯å¾„ (åç§°å¯ä»¥å®Œå…¨ä¸åŒ)
    if has_alias_path(graph, node_a, node_b, max_length=2):
        return True, 0.85, "alias_path"
    
    return False, 0.0, "no_match"
```

#### é€‚ç”¨æ€§åˆ†æ

**ä¼˜ç‚¹**ï¼š
- âœ… ä¸ä¾èµ–åç§°ï¼Œä½¿ç”¨æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡
- âœ… å¯ä»¥å¤„ç†æè¿°ä¸°å¯Œçš„å®ä½“

**ç¼ºç‚¹**ï¼š
- âŒ å¦‚ä½ æ‰€è¯´ï¼Œå­å›¾ä¸åŒæ—¶ä»ç„¶å¤±æ•ˆ
- âŒ éœ€è¦å­å›¾ä¿¡æ¯è´¨é‡é«˜
- âŒ è®¡ç®—æˆæœ¬é«˜ï¼ˆæ¯å¯¹éƒ½è¦æå–å­å›¾ï¼‰

---

### æ–¹æ¡ˆ5: æ··åˆæ¨ç†æ–¹æ³•

#### æ ¸å¿ƒæ€æƒ³
**ç»“åˆè§„åˆ™ã€ç»Ÿè®¡ã€LLMï¼Œæ„å»ºå¤šé˜¶æ®µå†³ç­–æµç¨‹**

```python
def hybrid_deduplication_pipeline(graph):
    """
    å¤šé˜¶æ®µæ··åˆå»é‡
    """
    candidates = []
    
    # Stage 1: åŸºäºè§„åˆ™çš„é«˜å¯ä¿¡åŒ¹é…
    rule_matches = apply_dedup_rules(graph)
    candidates.extend(rule_matches)  # å¯ä¿¡åº¦: 0.9-1.0
    
    # Stage 2: åŸºäºç»Ÿè®¡çš„å€™é€‰ç”Ÿæˆ
    statistical_matches = statistical_candidate_generation(graph)
    candidates.extend(statistical_matches)  # å¯ä¿¡åº¦: 0.7-0.9
    
    # Stage 3: LLMéªŒè¯è¾¹ç•Œæƒ…å†µ
    uncertain = [c for c in candidates if 0.6 < c.score < 0.85]
    llm_validated = llm_validation(uncertain, graph)
    candidates.extend(llm_validated)  # å¯ä¿¡åº¦: LLMç»™å‡º
    
    # Stage 4: å›¾æ¨ç†ï¼ˆä¼ é€’é—­åŒ…ã€ä¸€è‡´æ€§æ£€æŸ¥ï¼‰
    refined = graph_reasoning(candidates, graph)
    
    return refined

def apply_dedup_rules(graph):
    """
    è§„åˆ™ç¤ºä¾‹
    """
    rules = []
    
    # Rule 1: æœ‰æ˜¾å¼åˆ«åå…³ç³» â†’ 100%åŒ¹é…
    rules.append(lambda u, v: (
        has_edge_with_relation(graph, u, v, ALIAS_RELATIONS),
        1.0
    ))
    
    # Rule 2: åç§°å®Œå…¨ç›¸åŒ â†’ 100%åŒ¹é…
    rules.append(lambda u, v: (
        normalize(graph.nodes[u]['name']) == normalize(graph.nodes[v]['name']),
        1.0
    ))
    
    # Rule 3: åç§°æ˜¯ç¼©å†™å…³ç³» â†’ 90%åŒ¹é…
    rules.append(lambda u, v: (
        is_abbreviation(graph.nodes[u]['name'], graph.nodes[v]['name']),
        0.90
    ))
    
    # æ‰§è¡Œè§„åˆ™
    matches = []
    for u, v in combinations(graph.nodes(), 2):
        for rule in rules:
            is_match, score = rule(u, v)
            if is_match:
                matches.append((u, v, score, 'rule'))
                break
    
    return matches
```

---

### æ–¹æ¡ˆ6: å¼±ç›‘ç£å­¦ä¹  (Distant Supervision)

#### æ ¸å¿ƒæ€æƒ³
**åˆ©ç”¨å°‘é‡å·²æœ‰çš„åˆ«åå…³ç³»ä½œä¸ºè®­ç»ƒä¿¡å·ï¼Œå­¦ä¹ è¯†åˆ«åˆ«åçš„æ¨¡å¼**

```python
from sklearn.ensemble import RandomForestClassifier

def distant_supervision_training(graph):
    """
    ç”¨ç°æœ‰çš„åˆ«åå…³ç³»ä½œä¸ºæ­£æ ·æœ¬ï¼Œè®­ç»ƒåˆ†ç±»å™¨
    """
    # 1. æ­£æ ·æœ¬: æœ‰åˆ«åå…³ç³»çš„èŠ‚ç‚¹å¯¹
    positive_pairs = []
    for u, v, data in graph.edges(data=True):
        if data.get('relation') in ALIAS_RELATIONS:
            positive_pairs.append((u, v))
    
    # 2. è´Ÿæ ·æœ¬: éšæœºé‡‡æ · (å‡è®¾å¤§å¤šæ•°éƒ½ä¸æ˜¯åˆ«å)
    negative_pairs = random_sample_non_alias_pairs(graph, len(positive_pairs) * 2)
    
    # 3. ç‰¹å¾æå–
    X_train = []
    y_train = []
    
    for u, v in positive_pairs:
        features = extract_features(u, v, graph)
        X_train.append(features)
        y_train.append(1)
    
    for u, v in negative_pairs:
        features = extract_features(u, v, graph)
        X_train.append(features)
        y_train.append(0)
    
    # 4. è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier()
    model.fit(X_train, y_train)
    
    return model

def extract_features(u, v, graph):
    """
    å¤šç»´ç‰¹å¾æå–
    """
    return [
        compute_name_similarity(u, v),
        compute_neighbor_similarity(u, v, graph),
        compute_path_similarity(u, v, graph),
        len(set(graph.neighbors(u)) & set(graph.neighbors(v))),
        graph.degree(u),
        graph.degree(v),
        # ... æ›´å¤šç‰¹å¾
    ]

# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹æ–°çš„å€™é€‰å¯¹
def predict_aliases(model, graph):
    """
    ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹æ‰€æœ‰èŠ‚ç‚¹å¯¹æ˜¯å¦ä¸ºåˆ«å
    """
    predictions = []
    for u, v in combinations(graph.nodes(), 2):
        features = extract_features(u, v, graph)
        prob = model.predict_proba([features])[0][1]
        if prob > 0.7:
            predictions.append((u, v, prob, 'ml_model'))
    return predictions
```

**ä¼˜ç‚¹**ï¼š
- âœ… è‡ªåŠ¨å­¦ä¹ åˆ«åæ¨¡å¼ï¼Œæ— éœ€æ‰‹å·¥è§„åˆ™
- âœ… å¯ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„æƒ…å†µ

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦ä¸€å®šé‡çš„åˆ«åå…³ç³»ä½œä¸ºç§å­
- âŒ è´Ÿé‡‡æ ·å¯èƒ½å¼•å…¥å™ªå£°

---

### æ–¹æ¡ˆ7: åˆ«åä¼ æ’­ä¸é—­åŒ…

#### æ ¸å¿ƒæ€æƒ³
**ä»å·²çŸ¥åˆ«åå…³ç³»æ¨æ–­æœªçŸ¥åˆ«åå…³ç³»**

```python
def alias_propagation(graph, iterations=3):
    """
    åˆ«åå…³ç³»ä¼ æ’­
    
    å‡è®¾:
    - å¦‚æœ A å’Œ B éƒ½ä¸ C æœ‰åˆ«åå…³ç³»ï¼Œé‚£ä¹ˆ A å’Œ B å¯èƒ½ä¹Ÿæ˜¯åˆ«å
    - å¦‚æœ Aâ†’Bâ†’C éƒ½æ˜¯åˆ«åé“¾ï¼Œé‚£ä¹ˆ A å’Œ C ä¹Ÿæ˜¯åˆ«å
    """
    # åˆå§‹åŒ–ï¼šæå–æ‰€æœ‰æ˜¾å¼åˆ«åå¯¹
    alias_pairs = set()
    for u, v, data in graph.edges(data=True):
        if data.get('relation') in ALIAS_RELATIONS:
            alias_pairs.add((u, v))
            alias_pairs.add((v, u))  # åŒå‘
    
    # è¿­ä»£ä¼ æ’­
    for _ in range(iterations):
        new_pairs = set()
        
        # è§„åˆ™1: ä¼ é€’æ€§
        for a, b in alias_pairs:
            for b2, c in alias_pairs:
                if b == b2 and a != c:
                    new_pairs.add((a, c))
        
        # è§„åˆ™2: å¯¹ç§°æ€§ (å·²åœ¨åˆå§‹åŒ–æ—¶å¤„ç†)
        
        # è§„åˆ™3: å…±åŒåˆ«åæ¨æ–­
        #   å¦‚æœ Aâ†’C ä¸” Bâ†’Cï¼Œä¸” name_sim(A,B) > threshold
        #   åˆ™æ¨æ–­ A, B ä¹Ÿå¯èƒ½æ˜¯åˆ«å
        entities_by_target = defaultdict(list)
        for a, c in alias_pairs:
            entities_by_target[c].append(a)
        
        for c, sources in entities_by_target.items():
            for a, b in combinations(sources, 2):
                if compute_name_similarity(a, b) > 0.60:
                    new_pairs.add((a, b))
                    new_pairs.add((b, a))
        
        # æ›´æ–°
        alias_pairs.update(new_pairs)
        if not new_pairs:
            break  # æ”¶æ•›
    
    return alias_pairs
```

---

## ğŸ¯ é’ˆå¯¹ä½ çš„åœºæ™¯çš„æ¨èæ–¹æ¡ˆ

### åœºæ™¯ç‰¹ç‚¹

1. âœ… **å›¾ä¸­å·²æœ‰éƒ¨åˆ†åˆ«åå…³ç³»**ï¼ˆå¦‚"åˆ«ååŒ…æ‹¬"ï¼‰
2. âœ… **åç§°è¯­ä¹‰ç›¸ä¼¼åº¦ä½**ï¼ˆå¦‚"å‰å¸ƒæ–¯ä¼ªå½±/æˆªæ–­ä¼ªå½±"ï¼‰
3. â“ **å­å›¾å¯èƒ½ä¸åŒ**ï¼ˆä¸€ä¸ªæ˜¯å®šä¹‰ï¼Œä¸€ä¸ªæ˜¯è§£å†³æ–¹æ¡ˆï¼‰
4. âœ… **å¸Œæœ›é«˜å‡†ç¡®ç‡**ï¼ˆåŒ»å­¦é¢†åŸŸï¼‰

### æ¨èæ–¹æ¡ˆç»„åˆ ğŸ†

#### **æ–¹æ¡ˆA: å…³ç³»ä¼˜å…ˆ + å¤šç‰¹å¾åå¤‡** (æ¨èåº¦: â­â­â­â­â­)

```python
def recommended_alias_dedup_pipeline(graph, config):
    """
    åˆ†é˜¶æ®µå¤„ç†ï¼Œä¼˜å…ˆåˆ©ç”¨æ˜¾å¼å…³ç³»
    """
    merge_candidates = []
    
    # ========================================
    # Phase 1: æ˜¾å¼åˆ«åå…³ç³» (é«˜ä¼˜å…ˆçº§)
    # ========================================
    explicit_aliases = extract_alias_pairs_from_relations(graph)
    logger.info(f"Phase 1: Found {len(explicit_aliases)} explicit alias pairs")
    merge_candidates.extend(explicit_aliases)
    
    # ä¼ é€’é—­åŒ…
    propagated_aliases = alias_propagation(graph, iterations=2)
    logger.info(f"Phase 1: Propagated to {len(propagated_aliases)} alias pairs")
    merge_candidates.extend(propagated_aliases)
    
    # ========================================
    # Phase 2: åç§°é«˜ç›¸ä¼¼åº¦ (ä¸­ä¼˜å…ˆçº§)
    # ========================================
    remaining_nodes = get_unmatched_nodes(graph, merge_candidates)
    
    name_similar_pairs = generate_semantic_candidates(
        remaining_nodes,
        similarity_threshold=config.get('similarity_threshold', 0.85)
    )
    logger.info(f"Phase 2: Found {len(name_similar_pairs)} name-similar pairs")
    merge_candidates.extend(name_similar_pairs)
    
    # ========================================
    # Phase 3: å¤šç‰¹å¾èåˆ (ä½ä¼˜å…ˆçº§)
    # ========================================
    remaining_nodes = get_unmatched_nodes(graph, merge_candidates)
    
    # åªå¯¹åç§°æœ‰ä¸€å®šç›¸ä¼¼åº¦çš„èŠ‚ç‚¹å¯¹è¿›è¡Œå¤šç‰¹å¾è®¡ç®—
    medium_similar_pairs = generate_semantic_candidates(
        remaining_nodes,
        similarity_threshold=0.60  # é™ä½é˜ˆå€¼
    )
    
    multi_feature_matches = []
    for u, v, name_sim in medium_similar_pairs:
        # è®¡ç®—å…¶ä»–ç‰¹å¾
        neighbor_sim = compute_neighbor_similarity(u, v, graph)
        path_score = compute_path_features(u, v, graph)
        relation_sig_sim = signature_similarity(
            extract_relation_signature(u, graph),
            extract_relation_signature(v, graph)
        )
        
        # åŠ æƒèåˆ
        combined_score = (
            name_sim * 0.3 +
            neighbor_sim * 0.3 +
            relation_sig_sim * 0.4
        )
        
        if combined_score > 0.75:
            multi_feature_matches.append((u, v, combined_score, 'multi_feature'))
    
    logger.info(f"Phase 3: Found {len(multi_feature_matches)} multi-feature matches")
    merge_candidates.extend(multi_feature_matches)
    
    # ========================================
    # Phase 4: LLMéªŒè¯è¾¹ç•Œæƒ…å†µ (å¯é€‰)
    # ========================================
    if config.get('use_llm_validation', False):
        uncertain = [c for c in merge_candidates if 0.65 < c[2] < 0.85]
        llm_validated = llm_batch_validation(uncertain, graph, config)
        # ç”¨LLMç»“æœæ›¿æ¢ä¸ç¡®å®šçš„å€™é€‰
        merge_candidates = [c for c in merge_candidates if c not in uncertain]
        merge_candidates.extend(llm_validated)
    
    return merge_candidates
```

#### **æ–¹æ¡ˆB: åŸºäºå¼±ç›‘ç£å­¦ä¹ ** (æ¨èåº¦: â­â­â­â­)

å¦‚æœä½ çš„å›¾ä¸­æœ‰è¶³å¤Ÿå¤šçš„åˆ«åå…³ç³»ï¼ˆ>100å¯¹ï¼‰ï¼Œå¯ä»¥è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼š

```python
def ml_based_alias_detection(graph, config):
    """
    ä½¿ç”¨æœºå™¨å­¦ä¹ è‡ªåŠ¨å­¦ä¹ åˆ«åæ¨¡å¼
    """
    # 1. ç”¨ç°æœ‰åˆ«åå…³ç³»è®­ç»ƒæ¨¡å‹
    model = distant_supervision_training(graph)
    
    # 2. é¢„æµ‹æ‰€æœ‰å€™é€‰å¯¹
    predictions = predict_aliases(model, graph)
    
    # 3. é«˜ç½®ä¿¡åº¦çš„ç›´æ¥åˆå¹¶ï¼Œä¸­ç­‰ç½®ä¿¡åº¦çš„ç”¨LLMéªŒè¯
    high_conf = [p for p in predictions if p[2] > 0.85]
    medium_conf = [p for p in predictions if 0.65 < p[2] <= 0.85]
    
    if config.get('use_llm_validation', False):
        validated = llm_batch_validation(medium_conf, graph, config)
        return high_conf + validated
    else:
        return high_conf
```

---

## ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“

| æ–¹æ¡ˆ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ | å®æ–½éš¾åº¦ | æ¨èåº¦ |
|-----|---------|-----|-----|---------|--------|
| **æ˜¾å¼å…³ç³»åˆ©ç”¨** | å›¾ä¸­æœ‰åˆ«åå…³ç³» | å‡†ç¡®ã€å¯è§£é‡Š | ä¾èµ–å·²æœ‰å…³ç³» | â­â­ ä½ | â­â­â­â­â­ |
| **å¤šç‰¹å¾èåˆ** | é€šç”¨åœºæ™¯ | é²æ£’ã€å…¨é¢ | éœ€è¦è°ƒå‚ | â­â­â­ ä¸­ | â­â­â­â­ |
| **å›¾ç»“æ„æ–¹æ³•** | å…³ç³»ä¸°å¯Œçš„KG | åˆ©ç”¨å…¨å±€ä¿¡æ¯ | è®¡ç®—å¤æ‚ | â­â­â­â­ é«˜ | â­â­â­ |
| **ä¸Šä¸‹æ–‡å¢å¼º** | æœ‰å­å›¾/æè¿° | ä¸ä¾èµ–åç§° | å­å›¾ä¸åŒæ—¶å¤±æ•ˆ | â­â­â­ ä¸­ | â­â­â­ |
| **å¼±ç›‘ç£å­¦ä¹ ** | æœ‰æ ‡æ³¨æ•°æ® | è‡ªåŠ¨å­¦ä¹ æ¨¡å¼ | éœ€è¦è®­ç»ƒæ•°æ® | â­â­â­â­ é«˜ | â­â­â­â­ |
| **æ··åˆæ¨ç†** | é«˜å‡†ç¡®ç‡éœ€æ±‚ | ç»¼åˆå¤šç§æ–¹æ³• | å¤æ‚åº¦é«˜ | â­â­â­â­â­ å¾ˆé«˜ | â­â­â­â­â­ |

---

## ğŸ’¡ å¯¹ä½ çš„ä¸¤ä¸ªæ–¹æ¡ˆçš„è¯„ä»·

### æ–¹æ¡ˆ1: ä½¿ç”¨å­å›¾æè¿°è®¡ç®—ç›¸ä¼¼åº¦

**è¯„ä»·**: â­â­â­ éƒ¨åˆ†æœ‰æ•ˆï¼Œä½†æœ‰å±€é™

**ä¼˜ç‚¹**:
- âœ… ä¸ä¾èµ–åç§°
- âœ… åˆ©ç”¨äº†æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡

**é—®é¢˜**:
- âŒ å¦‚ä½ æ‰€è¯´ï¼Œå­å›¾å†…å®¹ä¸åŒæ—¶å¤±æ•ˆ
- âŒ "å‰å¸ƒæ–¯ä¼ªå½±"(å®šä¹‰) vs "æˆªæ–­ä¼ªå½±"(è§£å†³æ–¹æ¡ˆ) â†’ æè¿°ä»ç„¶ä¸ç›¸ä¼¼

**æ”¹è¿›å»ºè®®**:
1. ä¸ç”¨å­å›¾å†…å®¹ï¼Œè€Œç”¨**å…³ç³»ç±»å‹æ¨¡å¼**
2. å³ä½¿å…³ç³»ç±»å‹ä¹Ÿä¸åŒï¼Œå¯ä»¥ç»“åˆ**å…±åŒé‚»å±…**
3. ä½œä¸º**å¤šç‰¹å¾èåˆ**ä¸­çš„ä¸€ä¸ªç‰¹å¾ï¼Œè€Œéå”¯ä¸€ä¾æ®

### æ–¹æ¡ˆ2: ä½¿ç”¨"åˆ«ååŒ…æ‹¬"å…³ç³»

**è¯„ä»·**: â­â­â­â­â­ **å¼ºçƒˆæ¨è**

**ä¼˜ç‚¹**:
- âœ… ç›´æ¥åˆ©ç”¨å·²æœ‰çŸ¥è¯†
- âœ… å‡†ç¡®ç‡é«˜
- âœ… å¯è§£é‡Šæ€§å¼º
- âœ… è®¡ç®—æˆæœ¬ä½

**å…³äº"ä¸æ˜¯åŸºäºç›¸ä¼¼åº¦ï¼Œè€Œæ˜¯åŸºäºé€»è¾‘å…³ç³»"**:
- âœ… è¿™æ­£æ˜¯æ­£ç¡®çš„æ–¹å‘ï¼
- âœ… å®ä½“å»é‡æœ¬è´¨ä¸Šæ˜¯**çŸ¥è¯†èåˆ**é—®é¢˜ï¼Œä¸ä»…ä»…æ˜¯ç›¸ä¼¼åº¦è®¡ç®—
- âœ… ä¸šç•Œæœ€ä½³å®è·µå°±æ˜¯ç»“åˆå¤šç§ä¿¡å·ï¼ŒåŒ…æ‹¬æ˜¾å¼å…³ç³»

**æ‰©å±•å»ºè®®**:
1. ä¸ä»…ä½¿ç”¨ç›´æ¥è¾¹ï¼Œè¿˜å¯ä»¥ç”¨**2è·³å…³ç³»ä¼ æ’­**
2. ç»“åˆ**å…³ç³»ä¼ é€’é—­åŒ…**ï¼Œè‡ªåŠ¨æ¨æ–­æ›´å¤šåˆ«åå¯¹
3. å¯¹ä¸åŒç±»å‹çš„å…³ç³»ç»™äºˆ**ä¸åŒæƒé‡**

---

## ğŸš€ å®æ–½å»ºè®®

### çŸ­æœŸ (1å‘¨å†…): å¿«é€Ÿæ”¹è¿›

åœ¨ç°æœ‰ä»£ç ä¸­æ·»åŠ å…³ç³»æ„ŸçŸ¥çš„å€™é€‰ç”Ÿæˆï¼š

```python
def _generate_candidates_with_relations(self, remaining_nodes, config):
    """
    æ”¹è¿›çš„å€™é€‰ç”Ÿæˆï¼šå…³ç³» + ç›¸ä¼¼åº¦
    """
    candidates = []
    
    # æ–°å¢: ä»å…³ç³»ä¸­æå–å€™é€‰
    for u, v, data in self.graph.edges(data=True):
        relation = data.get('relation', '')
        if relation in ['åˆ«ååŒ…æ‹¬', 'also_known_as', 'alias_of', 'åˆç§°', 'synonym']:
            if u in remaining_nodes and v in remaining_nodes:
                candidates.append((u, v, 0.95, 'explicit_relation'))
    
    # åŸæœ‰: åŸºäºembeddingçš„å€™é€‰
    semantic_candidates = self._generate_semantic_candidates(
        remaining_nodes,
        max_candidates=config.max_candidates,
        similarity_threshold=config.candidate_similarity_threshold
    )
    candidates.extend(semantic_candidates)
    
    return candidates
```

### ä¸­æœŸ (1ä¸ªæœˆå†…): å¤šç‰¹å¾èåˆ

å®ç°å®Œæ•´çš„å¤šç‰¹å¾è¯„åˆ†ç³»ç»Ÿï¼š

```python
def _compute_multi_feature_score(self, node_a, node_b):
    """
    å¤šç»´åº¦è¯„åˆ†
    """
    features = {}
    
    # ç‰¹å¾1: åç§°ç›¸ä¼¼åº¦
    features['name_sim'] = self._compute_name_similarity(node_a, node_b)
    
    # ç‰¹å¾2: æ˜¾å¼å…³ç³»
    features['has_alias_rel'] = self._has_alias_relation(node_a, node_b)
    
    # ç‰¹å¾3: é‚»å±…é‡å 
    features['neighbor_sim'] = self._compute_neighbor_similarity(node_a, node_b)
    
    # ç‰¹å¾4: å…³ç³»æ¨¡å¼
    features['relation_sig'] = self._compute_relation_signature_similarity(node_a, node_b)
    
    # åŠ æƒèåˆ
    if features['has_alias_rel']:
        return 0.95  # æ˜¾å¼å…³ç³» â†’ é«˜åˆ†
    else:
        return (
            features['name_sim'] * 0.4 +
            features['neighbor_sim'] * 0.3 +
            features['relation_sig'] * 0.3
        )
```

### é•¿æœŸ (3ä¸ªæœˆå†…): æœºå™¨å­¦ä¹ æ–¹æ³•

å¦‚æœæ•°æ®é‡è¶³å¤Ÿï¼Œè®­ç»ƒä¸€ä¸ªåˆ«åè¯†åˆ«æ¨¡å‹ï¼š

```python
# è§ä¸Šæ–‡"æ–¹æ¡ˆ6: å¼±ç›‘ç£å­¦ä¹ "çš„è¯¦ç»†å®ç°
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Entity Resolutionç»¼è¿°**:
   - Christen, P. (2012). "Data Matching: Concepts and Techniques for Record Linkage"
   - Mudgal et al. (2018). "Deep Learning for Entity Matching: A Design Space Exploration" (SIGMOD)

2. **å›¾ç»“æ„æ–¹æ³•**:
   - Suchanek et al. (2012). "PARIS: Probabilistic Alignment of Relations, Instances, and Schema" (VLDB)
   - Bach et al. (2017). "Probabilistic Soft Logic for KG Construction"

3. **å…³ç³»æ„ŸçŸ¥æ–¹æ³•**:
   - Dong et al. (2014). "Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion" (KDD)
   - Paulheim & Bizer (2014). "Improving the Quality of Linked Data Using Statistical Distributions"

4. **æ·±åº¦å­¦ä¹ æ–¹æ³•**:
   - Li et al. (2019). "Graph Matching Networks for Learning the Similarity of Graph Structured Objects" (ICML)
   - Zhang et al. (2020). "Entity Alignment for Knowledge Graphs with Multi-Graph Attention Networks"

---

## âœ… æ€»ç»“

ä½ çš„è§‚å¯Ÿéå¸¸æ­£ç¡®ï¼**ä»…ä¾èµ–åç§°ç›¸ä¼¼åº¦ç¡®å®ä¸å¤Ÿã€‚**

**æœ€ä½³å®è·µæ˜¯**:
1. â­â­â­â­â­ **ä¼˜å…ˆåˆ©ç”¨æ˜¾å¼å…³ç³»**ï¼ˆä½ çš„æ–¹æ¡ˆ2ï¼‰
2. â­â­â­â­ **å¤šç‰¹å¾èåˆ**ä½œä¸ºåå¤‡
3. â­â­â­ **LLMéªŒè¯**è¾¹ç•Œæƒ…å†µ

**æ ¸å¿ƒinsight**:
> å®ä½“å»é‡ä¸ä»…ä»…æ˜¯ç›¸ä¼¼åº¦è®¡ç®—ï¼Œæ›´æ˜¯**çŸ¥è¯†èåˆå’Œæ¨ç†**é—®é¢˜ã€‚
> æ˜¾å¼å…³ç³»æ˜¯æœ€å¯é çš„ä¿¡å·ï¼Œåº”è¯¥ä½œä¸º**ç¬¬ä¸€ä¼˜å…ˆçº§**ã€‚

**ä¸‹ä¸€æ­¥å»ºè®®**:
1. å…ˆå®ç°å…³ç³»æ„ŸçŸ¥çš„å€™é€‰ç”Ÿæˆï¼ˆæœ€å¿«è§æ•ˆï¼‰
2. å†æ·»åŠ å¤šç‰¹å¾èåˆï¼ˆæå‡å¬å›ç‡ï¼‰
3. é•¿æœŸå¯è€ƒè™‘æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆè‡ªåŠ¨åŒ–ï¼‰

å¸Œæœ›è¿™ä¸ªè°ƒç ”å¯¹ä½ æœ‰å¸®åŠ©ï¼ğŸ‰
